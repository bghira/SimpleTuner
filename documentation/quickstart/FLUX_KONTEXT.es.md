# Gu√≠a r√°pida de Kontext‚ÄØ[dev] Mini

> üìù  Kontext comparte el 90‚ÄØ% del flujo de entrenamiento con Flux, as√≠ que este archivo solo enumera lo que *difiere*. Cuando un paso **no** se menciona aqu√≠, sigue las [instrucciones](../quickstart/FLUX.md) originales.


---

## 1. Resumen del modelo

|                                                  | Flux‚Äëdev               | Kontext‚Äëdev                                 |
| ------------------------------------------------ | -------------------    | ------------------------------------------- |
| Licencia                                         | No comercial           | No comercial                                |
| Guidance                                         | Destilado (CFG‚ÄØ‚âà‚ÄØ1)    | Destilado (CFG‚ÄØ‚âà‚ÄØ1)                         |
| Variantes disponibles                            | *dev*,‚ÄØschnell,¬†[pro]   | *dev*, [pro, max]                          |
| Longitud de secuencia T5                          | 512 dev, 256 schnell   | 512 dev                                     |
| Tiempo t√≠pico de inferencia 1024‚ÄØpx<br>(4090 @‚ÄØCFG‚ÄØ1)  | ‚âà‚ÄØ20‚ÄØs                  | **‚âà‚ÄØ80‚ÄØs**                                  |
| VRAM para LoRA 1024‚ÄØpx @‚ÄØint8‚Äëquanto               | 18‚ÄØG                   | **24‚ÄØG**                                    |

Kontext mantiene el backbone del transformer de Flux pero introduce **condicionamiento con referencia emparejada**.

Hay dos modos `conditioning_type` disponibles para Kontext:

* `conditioning_type=reference_loose`¬†(‚úÖ estable) ‚Äì la referencia puede diferir en relaci√≥n de aspecto/tama√±o de la edici√≥n.
  - Ambos datasets se escanean para metadatos, se agrupan por aspecto y se recortan de forma independiente, lo que puede aumentar sustancialmente el tiempo de arranque.
  - Esto puede ser un problema si deseas garantizar la alineaci√≥n de las im√°genes de edici√≥n y referencia, como en un dataloader que usa una sola imagen por nombre de archivo.
* `conditioning_type=reference_strict`¬†(‚úÖ estable) ‚Äì la referencia se pretransforma exactamente como el recorte de edici√≥n.
  - As√≠ debes configurar tus datasets si necesitas una alineaci√≥n perfecta entre recortes / aspect bucketing entre tus im√°genes de edici√≥n y referencia.
  - Antes requer√≠a `--vae_cache_ondemand` y algo m√°s de uso de VRAM, pero ya no.
  - Duplica los metadatos de recorte / aspect bucket del dataset fuente al inicio, para que no tengas que hacerlo.

Para definiciones de campos, consulta [`conditioning_type`](../DATALOADER.md#conditioning_type) y [`conditioning_data`](../DATALOADER.md#conditioning_data). Para controlar c√≥mo se muestrean m√∫ltiples conjuntos de condicionamiento, usa `conditioning_multidataset_sampling` como se describe en [OPTIONS](../OPTIONS.md#--conditioning_multidataset_sampling).


---

## 2. Requisitos de hardware

* **RAM del sistema**: la cuantizaci√≥n todav√≠a requiere 50‚ÄØGB.
* **GPU**: una 3090 (24‚ÄØG) es el m√≠nimo realista para entrenamiento a 1024‚ÄØpx **con int8‚Äëquanto**.
  * Los sistemas Hopper H100/H200 con Flash Attention 3 pueden habilitar `--fuse_qkv_projections` para acelerar mucho el entrenamiento.
  * Si entrenas a 512‚ÄØpx puedes encajar en una tarjeta de 12‚ÄØG, pero espera batches lentos (la longitud de secuencia sigue siendo grande).


---

## 3. Diferencia r√°pida de configuraci√≥n

A continuaci√≥n est√° el conjunto *m√≠nimo* de cambios que necesitas en `config/config.json` en comparaci√≥n con tu configuraci√≥n t√≠pica de entrenamiento de Flux.

<details>
<summary>Ver ejemplo de config</summary>

```jsonc
{
  "model_family":   "flux",
  "model_flavour": "kontext",                       // <‚Äë‚Äë cambia esto de "dev" a "kontext"
  "base_model_precision": "int8-quanto",            // cabe en 24‚ÄØG a 1024‚ÄØpx
  "gradient_checkpointing": true,
  "fuse_qkv_projections": false,                    // <‚Äë‚Äë √∫salo para acelerar el entrenamiento en sistemas Hopper H100/H200. ADVERTENCIA: requiere flash-attn instalado manualmente.
  "lora_rank": 16,
  "learning_rate": 1e-5,
  "optimizer": "optimi-lion",                       // <‚Äë‚Äë usa Lion para resultados m√°s r√°pidos, y adamw_bf16 para resultados m√°s lentos pero posiblemente m√°s estables.
  "max_train_steps": 10000,
  "validation_guidance": 2.5,                       // <‚Äë‚Äë kontext funciona mejor con guidance 2.5
  "validation_resolution": "1024x1024",
  "conditioning_multidataset_sampling": "random"    // <-- configurar esto en "combined" cuando tienes dos datasets de condicionamiento definidos har√° que se muestren simult√°neamente en lugar de alternar.
}
```
</details>

### Funciones experimentales avanzadas

<details>
<summary>Mostrar detalles experimentales avanzados</summary>


SimpleTuner incluye funciones experimentales que pueden mejorar significativamente la estabilidad y el rendimiento del entrenamiento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduce el sesgo de exposici√≥n y mejora la calidad de la salida al permitir que el modelo genere sus propias entradas durante el entrenamiento.

> ‚ö†Ô∏è Estas funciones aumentan la sobrecarga computacional del entrenamiento.

</details>

### Fragmento de dataloader (multi‚Äëdata‚Äëbackend)

Si has curado manualmente un dataset de pares de im√°genes, puedes configurarlo usando dos directorios separados: uno para las im√°genes de edici√≥n y otro para las de referencia.

El campo `conditioning_data` en el dataset de edici√≥n debe apuntar al `id` del dataset de referencia.

<details>
<summary>Ver ejemplo de config</summary>

```jsonc
[
  {
    "id": "my-edited-images",
    "type": "local",
    "cache_dir_vae": "/cache/vae/flux/kontext/edited-images",   // <-- donde se almacenan las salidas del VAE
    "instance_data_dir": "/datasets/edited-images",             // <-- usa rutas absolutas
    "conditioning_data": [
      "my-reference-images"                                     // <‚Äë‚Äë este debe ser tu "id" del conjunto de referencia
                                                                // puedes especificar un segundo conjunto para alternar o combinar, p. ej. ["reference-images", "reference-images2"]
    ],
    "resolution": 1024,
    "caption_strategy": "textfile"                              // <-- estos captions deben contener las instrucciones de edici√≥n
  },
  {
    "id": "my-reference-images",
    "type": "local",
    "cache_dir_vae": "/cache/vae/flux/kontext/ref-images",      // <-- donde se almacenan las salidas del VAE. debe ser distinto de otras rutas VAE del dataset.
    "instance_data_dir": "/datasets/reference-images",          // <-- usa rutas absolutas
    "conditioning_type": "reference_strict",                    // <‚Äë‚Äë si esto se configura en reference_loose, las im√°genes se recortan de forma independiente de las im√°genes de edici√≥n
    "resolution": 1024,
    "caption_strategy": null,                                   // <‚Äë‚Äë no se necesitan captions para referencias, pero si est√°n disponibles, se usar√°n EN LUGAR de los captions de edici√≥n
                                                                // NOTA: no puedes definir captions de condicionamiento separadas cuando usas conditioning_multidataset_sampling=combined.
                                                                // Solo se usar√°n los captions de los datasets de edici√≥n.
  }
]
```
</details>

> Consulta las opciones y requisitos de caption_strategy en [DATALOADER.md](../DATALOADER.md#caption_strategy).

*Cada imagen de edici√≥n **debe** tener nombres de archivo y extensiones con correspondencia 1‚Äëa‚Äë1 en ambas carpetas de dataset. SimpleTuner unir√° autom√°ticamente el embedding de referencia al condicionamiento de la edici√≥n.

Existe un ejemplo preparado de dataset demo [Kontext Max derived](https://huggingface.co/datasets/terminusresearch/KontextMax-Edit-smol) que contiene im√°genes de referencia y edici√≥n junto con sus archivos de captions para explorar y tener una mejor idea de c√≥mo configurarlo.

### Configurar una divisi√≥n de validaci√≥n dedicada

Aqu√≠ hay un ejemplo de configuraci√≥n que usa un conjunto de entrenamiento con 200,000 muestras y un conjunto de validaci√≥n con solo unas pocas.

En tu `config.json` deber√≠as agregar:

<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "eval_dataset_id": "edited-images",
}
```
</details>

Para tu `multidatabackend.json`, `edited-images` y `reference-images` deber√≠an contener datos de validaci√≥n con el mismo dise√±o que una divisi√≥n de entrenamiento habitual.

<details>
<summary>Ver ejemplo de config</summary>

```json
[
    {
        "id": "edited-images",
        "disabled": false,
        "type": "local",
        "instance_data_dir": "/datasets/edit/edited-images",
        "minimum_image_size": 1024,
        "maximum_image_size": 1536,
        "target_downsample_size": 1024,
        "resolution": 1024,
        "resolution_type": "pixel_area",
        "caption_strategy": "textfile",
        "cache_dir_vae": "cache/vae/flux-edit",
        "vae_cache_clear_each_epoch": false,
        "conditioning_data": ["reference-images"]
    },
    {
        "id": "reference-images",
        "disabled": false,
        "type": "local",
        "instance_data_dir": "/datasets/edit/reference-images",
        "minimum_image_size": 1024,
        "maximum_image_size": 1536,
        "target_downsample_size": 1024,
        "resolution": 1024,
        "resolution_type": "pixel_area",
        "caption_strategy": null,
        "cache_dir_vae": "cache/vae/flux-ref",
        "vae_cache_clear_each_epoch": false,
        "conditioning_type": "reference_strict"
    },
    {
        "id": "subjects200k-left",
        "disabled": false,
        "type": "huggingface",
        "dataset_name": "Yuanshi/Subjects200K",
        "caption_strategy": "huggingface",
        "metadata_backend": "huggingface",
        "resolution": 512,
        "resolution_type": "pixel_area",
        "conditioning_data": ["subjects200k-right"],
        "huggingface": {
            "caption_column": "description.description_0",
            "image_column": "image",
            "composite_image_config": {
                "enabled": true,
                "image_count": 2,
                "select_index": 0
            }
        }
    },
    {
        "id": "subjects200k-right",
        "disabled": false,
        "type": "huggingface",
        "dataset_type": "conditioning",
        "conditioning_type": "reference_strict",
        "source_dataset_id": "subjects200k-left",
        "dataset_name": "Yuanshi/Subjects200K",
        "caption_strategy": "huggingface",
        "metadata_backend": "huggingface",
        "resolution": 512,
        "resolution_type": "pixel_area",
        "huggingface": {
            "caption_column": "description.description_1",
            "image_column": "image",
            "composite_image_config": {
                "enabled": true,
                "image_count": 2,
                "select_index": 1
            }
        }
    },

    {
        "id": "text-embed-cache",
        "dataset_type": "text_embeds",
        "default": true,
        "type": "local",
        "cache_dir": "cache/text/flux"
    }
]
```
</details>

### Generaci√≥n autom√°tica de pares Referencia‚ÄëEdici√≥n

Si no tienes pares referencia-edici√≥n preexistentes, SimpleTuner puede generarlos autom√°ticamente a partir de un solo dataset. Esto es especialmente √∫til para entrenar modelos para:
- Mejora de imagen / super-resoluci√≥n
- Eliminaci√≥n de artefactos JPEG
- Deblurring
- Otras tareas de restauraci√≥n

#### Ejemplo: dataset de entrenamiento de deblurring

<details>
<summary>Ver ejemplo de config</summary>

```jsonc
[
  {
    "id": "high-quality-images",
    "type": "local",
    "instance_data_dir": "/path/to/sharp-images",
    "resolution": 1024,
    "caption_strategy": "textfile",
    "conditioning": [
      {
        "type": "superresolution",
        "blur_radius": 3.0,
        "blur_type": "gaussian",
        "add_noise": true,
        "noise_level": 0.02,
        "captions": ["enhance sharpness", "deblur", "increase clarity", "sharpen image"]
      }
    ]
  },
  {
    "id": "text-embeds",
    "dataset_type": "text_embeds",
    "default": true,
    "type": "local",
    "cache_dir": "cache/text/kontext"
  }
]
```
</details>

Esta configuraci√≥n:
1. Crea versiones borrosas (se convierten en las im√°genes de "referencia") a partir de tus im√°genes n√≠tidas de alta calidad
2. Usa las im√°genes originales de alta calidad como objetivo de p√©rdida de entrenamiento
3. Entrena Kontext para mejorar/deblur la imagen de referencia de baja calidad

> **NOTA**: No puedes definir `captions` en un dataset de condicionamiento cuando usas `conditioning_multidataset_sampling=combined`. Se usar√°n los captions del dataset de edici√≥n.

#### Ejemplo: Eliminaci√≥n de artefactos JPEG

<details>
<summary>Ver ejemplo de config</summary>

```jsonc
[
  {
    "id": "pristine-images",
    "type": "local",
    "instance_data_dir": "/path/to/pristine-images",
    "resolution": 1024,
    "caption_strategy": "textfile",
    "conditioning": [
      {
        "type": "jpeg_artifacts",
        "quality_mode": "range",
        "quality_range": [10, 30],
        "compression_rounds": 2,
        "captions": ["remove compression artifacts", "restore quality", "fix jpeg artifacts"]
      }
    ]
  },
  {
    "id": "text-embeds",
    "dataset_type": "text_embeds",
    "default": true,
    "type": "local",
    "cache_dir": "cache/text/kontext"
  }
]
```
</details>

#### Notas importantes

1. **La generaci√≥n ocurre al inicio**: Las versiones degradadas se crean autom√°ticamente cuando comienza el entrenamiento
2. **Cach√©**: Las im√°genes generadas se guardan, por lo que ejecuciones posteriores no las regenerar√°n
3. **Estrategia de captions**: El campo `captions` en la configuraci√≥n de condicionamiento proporciona prompts espec√≠ficos de tarea que funcionan mejor que descripciones gen√©ricas de imagen
4. **Rendimiento**: Estos generadores basados en CPU (blur, JPEG) son r√°pidos y usan m√∫ltiples procesos
5. **Espacio en disco**: Aseg√∫rate de tener suficiente espacio en disco para las im√°genes generadas, ya que pueden ser grandes. Por desgracia, a√∫n no hay capacidad para crearlas bajo demanda.

Para m√°s tipos de condicionamiento y configuraciones avanzadas, consulta la [documentaci√≥n de ControlNet](../CONTROLNET.md).

---

## 4. Consejos de entrenamiento espec√≠ficos para Kontext

1. **Secuencias m√°s largas ‚Üí pasos m√°s lentos.** Espera
~0.4‚ÄØit/s en una sola 4090 a 1024‚ÄØpx, LoRA de rango 1, bf16 + int8.
2. **Explora para encontrar los ajustes correctos.** No se sabe mucho sobre el ajuste fino de Kontext; por seguridad, mantente en `1e‚Äë5` (Lion) o `5e‚Äë4` (AdamW).
3. **Vigila picos de VRAM durante el cach√© del VAE.** Si haces OOM, agrega `--offload_during_startup=true`, baja tu `resolution`, o posiblemente habilita el tiling del VAE v√≠a tu `config.json`.
4. **Puedes entrenarlo sin im√°genes de referencia, pero no actualmente v√≠a SimpleTuner.** Actualmente, las cosas est√°n algo hardcodeadas para requerir im√°genes condicionales, pero puedes proporcionar datasets normales junto a tus pares de edici√≥n para que aprenda sujetos y semejanzas.
5. **Re-destilaci√≥n de guidance.** Al igual que Flux‚Äëdev, Kontext‚Äëdev est√° destilado con CFG; si necesitas diversidad, reentrena con `validation_guidance_real > 1` y usa un nodo Adaptive‚ÄëGuidance en inferencia, aunque esto tardar√° MUCHO m√°s en converger, y requerir√° un LoRA de alto rango o un Lycoris LoKr para tener √©xito.
6. **El entrenamiento de rango completo probablemente sea una p√©rdida de tiempo.** Kontext est√° dise√±ado para entrenarse con rango bajo, y el entrenamiento de rango completo probablemente no dar√° mejores resultados que un Lycoris LoKr, que normalmente superar√° a un LoRA est√°ndar con menos trabajo buscando los mejores par√°metros. Si quieres probarlo de todas formas, tendr√°s que usar DeepSpeed.
7. **Puedes usar dos o m√°s im√°genes de referencia para entrenar.** Como ejemplo, si tienes im√°genes sujeto-sujeto-escena para insertar los dos sujetos en una sola escena, puedes proporcionar todas las im√°genes relevantes como entradas de referencia. Solo aseg√∫rate de que los nombres de archivo coincidan en todas las carpetas.

---

## 5. Trampas de inferencia

- Haz coincidir la precisi√≥n de entrenamiento e inferencia; el entrenamiento int8 funciona mejor con inferencia int8 y as√≠ sucesivamente.
- Va a ser muy lento debido a que dos im√°genes pasan por el sistema al mismo tiempo. Espera 80‚ÄØs por edici√≥n 1024‚ÄØpx en una 4090.

---

## 6. Hoja de soluci√≥n r√°pida de problemas

| S√≠ntoma                                 | Causa probable              | Soluci√≥n r√°pida                                         |
| --------------------------------------- | ---------------------------- | ------------------------------------------------------ |
| OOM durante la cuantizaci√≥n             | No hay suficiente RAM del sistema | Usa `quantize_via=cpu`                                 |
| Imagen de referencia ignorada / sin edici√≥n aplicada | Emparejamiento incorrecto del dataloader | Asegura nombres de archivo id√©nticos y el campo `conditioning_data` |
| Artefactos de cuadr√≠cula cuadrada       | Ediciones de baja calidad dominan | Crea dataset de mayor calidad, baja el LR, evita Lion |

---

## 7. Lecturas adicionales

Para opciones avanzadas de ajuste (LoKr, cuantizaci√≥n NF4, DeepSpeed, etc.) consulta [la gu√≠a r√°pida original de Flux](../quickstart/FLUX.md) ‚Äì todos los flags funcionan igual a menos que se indique lo contrario arriba.
