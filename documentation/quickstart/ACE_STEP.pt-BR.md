# Guia de In√≠cio R√°pido do ACE-Step

Neste exemplo, vamos treinar o modelo de gera√ß√£o de √°udio ACE-Step v1 3.5B.

## Vis√£o geral

O ACE-Step √© um modelo de flow-matching baseado em transformer com 3.5B par√¢metros, projetado para s√≠ntese de √°udio de alta qualidade. Ele suporta gera√ß√£o de texto para √°udio e pode ser condicionado com letras.

## Requisitos de hardware

O ACE-Step √© um modelo de 3.5B par√¢metros, o que o torna relativamente leve em compara√ß√£o com modelos grandes de gera√ß√£o de imagem como o Flux.

- **M√≠nimo:** GPU NVIDIA com 12GB+ de VRAM (ex.: 3060, 4070).
- **Recomendado:** GPU NVIDIA com 24GB+ de VRAM (ex.: 3090, 4090, A10G) para lotes maiores.
- **Mac:** Suportado via MPS no Apple Silicon (requer ~36GB+ de Mem√≥ria Unificada).

### Requisitos de armazenamento

> ‚ö†Ô∏è **Aviso de uso de disco:** O cache de VAE para modelos de √°udio pode ser substancial. Por exemplo, um √∫nico clipe de √°udio de 60 segundos pode resultar em um arquivo latente em cache de ~89MB. Essa estrat√©gia de cache √© usada para reduzir drasticamente a exig√™ncia de VRAM durante o treinamento. Garanta que voc√™ tenha espa√ßo em disco suficiente para o cache do seu dataset.

> üí° **Dica:** Para datasets maiores, voc√™ pode usar a op√ß√£o `--vae_cache_disable` para desativar a grava√ß√£o de embeddings em disco. Isso habilita implicitamente o cache sob demanda, o que economiza espa√ßo em disco, mas aumenta o tempo de treinamento e o uso de mem√≥ria j√° que as codifica√ß√µes s√£o executadas durante o loop de treinamento.

> üí° **Dica:** Usar quantiza√ß√£o `int8-quanto` permite treinar em GPUs com menos VRAM (ex.: 12GB-16GB) com perda m√≠nima de qualidade.

## Pr√©-requisitos

Garanta que voc√™ tenha um ambiente funcional com Python 3.10+.

```bash
pip install simpletuner
```

## Configura√ß√£o

Recomenda-se manter suas configura√ß√µes organizadas. Vamos criar uma pasta dedicada para esta demonstra√ß√£o.

```bash
mkdir -p config/acestep-training-demo
```

### Configura√ß√µes cr√≠ticas

Crie `config/acestep-training-demo/config.json` com estes valores:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
  "model_family": "ace_step",
  "model_type": "lora",
  "model_flavour": "base",
  "pretrained_model_name_or_path": "ACE-Step/ACE-Step-v1-3.5B",
  "resolution": 0,
  "mixed_precision": "bf16",
  "base_model_precision": "int8-quanto",
  "data_backend_config": "config/acestep-training-demo/multidatabackend.json"
}
```
</details>

### Configura√ß√µes de valida√ß√£o

Adicione estas configura√ß√µes ao seu `config.json` para monitorar o progresso:

- **`validation_prompt`**: Uma descri√ß√£o de texto do √°udio que voc√™ quer gerar (ex.: "Uma m√∫sica pop cativante com bateria animada").
- **`validation_lyrics`**: (Opcional) Letras para o modelo cantar.
- **`validation_audio_duration`**: Dura√ß√£o em segundos para clipes de valida√ß√£o (padr√£o: 30.0).
- **`validation_guidance`**: Escala de guidance (padr√£o: ~3.0 - 5.0).
- **`validation_step_interval`**: Com que frequ√™ncia gerar amostras (ex.: a cada 100 steps).

### Recursos experimentais avan√ßados

<details>
<summary>Mostrar detalhes experimentais avan√ßados</summary>


O SimpleTuner inclui recursos experimentais que podem melhorar significativamente a estabilidade e o desempenho do treinamento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduz o vi√©s de exposi√ß√£o e melhora a qualidade de sa√≠da ao deixar o modelo gerar suas pr√≥prias entradas durante o treinamento.

> ‚ö†Ô∏è Esses recursos aumentam a sobrecarga computacional do treinamento.

</details>

## Configura√ß√£o do dataset

O ACE-Step requer uma configura√ß√£o de dataset **espec√≠fica para √°udio**.

### Op√ß√£o 1: Dataset de demonstra√ß√£o (Hugging Face)

Para um in√≠cio r√°pido, voc√™ pode usar o preset [ACEStep-Songs](../data_presets/preset_audio_dataset_with_lyrics.md).

Crie `config/acestep-training-demo/multidatabackend.json`:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
[
  {
    "id": "acestep-demo-data",
    "type": "huggingface",
    "dataset_type": "audio",
    "dataset_name": "Yi3852/ACEStep-Songs",
    "metadata_backend": "huggingface",
    "caption_strategy": "huggingface",
    "cache_dir_vae": "cache/vae/{model_family}/acestep-demo-data"
  },
  {
    "id": "text-embeds",
    "dataset_type": "text_embeds",
    "default": true,
    "type": "local",
    "cache_dir": "cache/text/{model_family}"
  }
]
```
</details>

> Veja op√ß√µes e requisitos de caption_strategy em [DATALOADER.md](../DATALOADER.md#caption_strategy).

### Op√ß√£o 2: Arquivos de √°udio locais

Crie `config/acestep-training-demo/multidatabackend.json`:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
[
  {
    "id": "my-audio-dataset",
    "type": "local",
    "dataset_type": "audio",
    "instance_data_dir": "datasets/my_audio_files",
    "caption_strategy": "textfile",
    "metadata_backend": "discovery",
    "disabled": false
  },
  {
    "id": "text-embeds",
    "dataset_type": "text_embeds",
    "default": true,
    "type": "local",
    "cache_dir": "cache/text/{model_family}"
  }
]
```
</details>

### Estrutura de dados

Coloque seus arquivos de √°udio em `datasets/my_audio_files`. O SimpleTuner suporta uma ampla variedade de formatos, incluindo:

- **Sem perda:** `.wav`, `.flac`, `.aiff`, `.alac`
- **Com perda:** `.mp3`, `.ogg`, `.m4a`, `.aac`, `.wma`, `.opus`

> ‚ÑπÔ∏è **Nota:** Para suportar formatos como MP3, AAC e WMA, voc√™ deve ter o **FFmpeg** instalado no sistema.

Para captions e letras, coloque os arquivos de texto correspondentes ao lado dos seus arquivos de √°udio:

- **√Åudio:** `track_01.wav`
- **Caption (Prompt):** `track_01.txt` (Cont√©m a descri√ß√£o em texto, ex.: "Uma balada de jazz lenta")
- **Letra (Opcional):** `track_01.lyrics` (Cont√©m o texto da letra)

<details>
<summary>Exemplo de layout do dataset</summary>

```text
datasets/my_audio_files/
‚îú‚îÄ‚îÄ track_01.wav
‚îú‚îÄ‚îÄ track_01.txt
‚îî‚îÄ‚îÄ track_01.lyrics
```
</details>

> üí° **Avan√ßado:** Se seu dataset usa uma conven√ß√£o de nomes diferente (ex.: `_lyrics.txt`), voc√™ pode personalizar isso na configura√ß√£o do dataset.

<details>
<summary>Ver exemplo de nome de arquivo de letras personalizado</summary>

```json
"audio": {
  "lyrics_filename_format": "{filename}_lyrics.txt"
}
```
</details>

> ‚ö†Ô∏è **Nota sobre letras:** Se um arquivo `.lyrics` n√£o for encontrado para uma amostra, os embeddings de letras ser√£o zerados. O ACE-Step espera condicionamento por letras; treinar muito em dados sem letras (instrumentais) pode exigir mais steps de treino para o modelo aprender a gerar √°udio instrumental de alta qualidade com entradas de letras zeradas.

## Treinamento

Inicie o treino especificando seu ambiente:

```bash
simpletuner train env=acestep-training-demo
```

Esse comando diz ao SimpleTuner para procurar `config.json` dentro de `config/acestep-training-demo/`.

> üí° **Dica (Continuar treinamento):** Para continuar o fine-tuning a partir de um LoRA existente (ex.: checkpoints oficiais do ACE-Step ou adaptadores da comunidade), use a op√ß√£o `--init_lora`:
> ```bash
> simpletuner train env=acestep-training-demo --init_lora=/path/to/existing_lora.safetensors
> ```

### Treinando o embedder de letras (estilo upstream)

O treinador upstream do ACE-Step faz fine-tuning do embedder de letras junto com o denoiser. Para espelhar esse comportamento no SimpleTuner (apenas LoRA completo ou standard):

- Habilite: `lyrics_embedder_train: true`
- Substitui√ß√µes opcionais (caso contr√°rio, o otimizador/scheduler principal √© reutilizado):
  - `lyrics_embedder_lr`
  - `lyrics_embedder_optimizer`
  - `lyrics_embedder_lr_scheduler`

Exemplo:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
  "lyrics_embedder_train": true,
  "lyrics_embedder_lr": 5e-5,
  "lyrics_embedder_optimizer": "torch-adamw",
  "lyrics_embedder_lr_scheduler": "cosine_with_restarts"
}
```
</details>
Os pesos do embedder s√£o salvos junto com os checkpoints do LoRA e restaurados ao retomar.

## Solu√ß√£o de problemas

- **Erros de valida√ß√£o:** Garanta que voc√™ n√£o esteja tentando usar recursos de valida√ß√£o centrados em imagem, como `num_validation_images` > 1 (mapeado conceitualmente para tamanho do lote no √°udio) ou m√©tricas baseadas em imagem (CLIP score).
- **Problemas de mem√≥ria:** Se ocorrer OOM, tente reduzir `train_batch_size` ou habilitar `gradient_checkpointing`.

## Migrando do treinador upstream

Se voc√™ vem dos scripts originais de treinamento do ACE-Step, veja como os par√¢metros mapeiam para o `config.json` do SimpleTuner:

| Par√¢metro upstream | `config.json` do SimpleTuner | Padr√£o / Notas |
| :--- | :--- | :--- |
| `--learning_rate` | `learning_rate` | `1e-4` |
| `--num_workers` | `dataloader_num_workers` | `8` |
| `--max_steps` | `max_train_steps` | `2000000` |
| `--every_n_train_steps` | `checkpointing_steps` | `2000` |
| `--precision` | `mixed_precision` | `"fp16"` ou `"bf16"` (use `"no"` para fp32) |
| `--accumulate_grad_batches` | `gradient_accumulation_steps` | `1` |
| `--gradient_clip_val` | `max_grad_norm` | `0.5` |
| `--shift` | `flow_schedule_shift` | `3.0` (Espec√≠fico do ACE-Step) |

### Convertendo dados brutos

Se voc√™ tem arquivos brutos de √°udio/texto/letras e quer usar o formato de dataset do Hugging Face (como usado pela ferramenta upstream `convert2hf_dataset.py`), voc√™ pode usar o dataset resultante diretamente no SimpleTuner.

O conversor upstream produz um dataset com colunas `tags` e `norm_lyrics`. Para usar isso, configure seu backend assim:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
    "type": "huggingface",
    "dataset_type": "audio",
    "dataset_name": "path/to/converted/dataset",
    "config": {
        "audio_caption_fields": ["tags"],
        "lyrics_column": "norm_lyrics"
    }
}
```
</details>
