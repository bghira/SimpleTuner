## Inicio r치pido de PixArt Sigma

En este ejemplo, entrenaremos un modelo PixArt Sigma usando el toolkit SimpleTuner y usaremos el tipo de modelo `full`, ya que al ser un modelo m치s peque침o probablemente quepa en VRAM.

### Requisitos previos

Aseg칰rate de tener Python instalado; SimpleTuner funciona bien con 3.10 a 3.12.

Puedes comprobarlo ejecutando:

```bash
python --version
```

Si no tienes Python 3.12 instalado en Ubuntu, puedes intentar lo siguiente:

```bash
apt -y install python3.12 python3.12-venv
```

#### Dependencias de imagen de contenedor

Para Vast, RunPod y TensorDock (entre otros), lo siguiente funcionar치 en una imagen CUDA 12.2-12.8 para permitir compilar extensiones CUDA:

```bash
apt -y install nvidia-cuda-toolkit
```

### Instalaci칩n

Instala SimpleTuner v칤a pip:

```bash
pip install simpletuner[cuda]
```

Para instalaci칩n manual o setup de desarrollo, consulta la [documentaci칩n de instalaci칩n](../INSTALL.md).

#### Pasos adicionales para AMD ROCm

Lo siguiente debe ejecutarse para que un AMD MI300X sea utilizable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

### Configuraci칩n del entorno

Para ejecutar SimpleTuner, necesitar치s configurar un archivo de configuraci칩n, los directorios de dataset y modelo, y un archivo de configuraci칩n del dataloader.

#### Archivo de configuraci칩n

Un script experimental, `configure.py`, puede permitirte omitir por completo esta secci칩n mediante una configuraci칩n interactiva paso a paso. Incluye funciones de seguridad que ayudan a evitar errores comunes.

**Nota:** Esto no configura tu dataloader. A칰n tendr치s que hacerlo manualmente, m치s adelante.

Para ejecutarlo:

```bash
simpletuner configure
```
> 丘멆잺 Para usuarios ubicados en pa칤ses donde Hugging Face Hub no es f치cilmente accesible, deber칤as a침adir `HF_ENDPOINT=https://hf-mirror.com` a tu `~/.bashrc` o `~/.zshrc` dependiendo de qu칠 `$SHELL` use tu sistema.

Si prefieres configurar manualmente:

Copia `config/config.json.example` a `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

Ah칤, necesitar치s modificar las siguientes variables:

<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "model_type": "full",
  "use_bitfit": false,
  "pretrained_model_name_or_path": "pixart-alpha/pixart-sigma-xl-2-1024-ms",
  "model_family": "pixart_sigma",
  "output_dir": "/home/user/output/models",
  "validation_resolution": "1024x1024,1280x768",
  "validation_guidance": 3.5
}
```
</details>

- `pretrained_model_name_or_path` - Configura esto en `PixArt-alpha/PixArt-Sigma-XL-2-1024-MS`.
- `MODEL_TYPE` - Configura esto en `full`.
- `USE_BITFIT` - Configura esto en `false`.
- `MODEL_FAMILY` - Configura esto en `pixart_sigma`.
- `OUTPUT_DIR` - Configura esto en el directorio donde quieres guardar tus checkpoints e im치genes de validaci칩n. Se recomienda usar una ruta completa.
- `VALIDATION_RESOLUTION` - Como PixArt Sigma viene en formato 1024px o 2048xp, deber칤as configurar esto con cuidado en `1024x1024` para este ejemplo.
  - Adem치s, PixArt fue afinado con buckets multi-aspect, y se pueden especificar otras resoluciones separ치ndolas con comas: `1024x1024,1280x768`
- `VALIDATION_GUIDANCE` - PixArt se beneficia de un valor muy bajo. Configura esto entre `3.6` y `4.4`.

Hay algunas m치s si usas una m치quina Mac M-series:

- `mixed_precision` deber칤a configurarse en `no`.

> 游눠 **Consejo:** Para datasets grandes donde el espacio en disco es una preocupaci칩n, puedes usar `--vae_cache_disable` para realizar la codificaci칩n VAE en l칤nea sin cachear los resultados a disco.

#### Consideraciones de dataset

Es crucial contar con un dataset sustancial para entrenar tu modelo. Hay limitaciones en el tama침o del dataset y debes asegurarte de que sea lo suficientemente grande para entrenar el modelo de manera efectiva. Ten en cuenta que el tama침o m칤nimo del dataset es `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`. El dataset no ser치 detectable por el trainer si es demasiado peque침o.

Dependiendo del dataset que tengas, necesitar치s configurar el directorio de dataset y el archivo de configuraci칩n del dataloader de forma diferente. En este ejemplo, usaremos [pseudo-camera-10k](https://huggingface.co/datasets/bghira/pseudo-camera-10k) como dataset.

En tu directorio `/home/user/simpletuner/config`, crea un multidatabackend.json:

<details>
<summary>Ver ejemplo de config</summary>

```json
[
  {
    "id": "pseudo-camera-10k-pixart",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "random",
    "resolution": 1.0,
    "minimum_image_size": 0.25,
    "maximum_image_size": 1.0,
    "target_downsample_size": 1.0,
    "resolution_type": "area",
    "cache_dir_vae": "cache/vae/pixart/pseudo-camera-10k",
    "instance_data_dir": "/home/user/simpletuner/datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery"
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/pixart/pseudo-camera-10k",
    "disabled": false,
    "write_batch_size": 128
  }
]
```
</details>

> Consulta las opciones y requisitos de caption_strategy en [DATALOADER.md](../DATALOADER.md#caption_strategy).

Luego, crea un directorio `datasets`:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
popd
```

Esto descargar치 alrededor de 10k muestras de fotograf칤as a tu directorio `datasets/pseudo-camera-10k`, que se crear치 autom치ticamente.

#### Inicia sesi칩n en WandB y Huggingface Hub

Querr치s iniciar sesi칩n en WandB y en HF Hub antes de comenzar el entrenamiento, especialmente si est치s usando `push_to_hub: true` y `--report_to=wandb`.

Si vas a subir elementos manualmente a un repositorio Git LFS, tambi칠n deber칤as ejecutar `git config --global credential.helper store`

Ejecuta los siguientes comandos:

```bash
wandb login
```

y

```bash
huggingface-cli login
```

Sigue las instrucciones para iniciar sesi칩n en ambos servicios.

### Ejecutar el entrenamiento

Desde el directorio SimpleTuner, solo tienes que ejecutar:

```bash
bash train.sh
```

Esto iniciar치 el cach칠 en disco de los embeddings de texto y las salidas VAE.

Para m치s informaci칩n, consulta los documentos [dataloader](../DATALOADER.md) y [tutorial](../TUTORIAL.md).

### Seguimiento de puntuaci칩n CLIP

Si deseas habilitar evaluaciones para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/CLIP_SCORES.md) para informaci칩n sobre c칩mo configurar e interpretar las puntuaciones CLIP.

# P칠rdida de evaluaci칩n estable

Si deseas usar p칠rdida MSE estable para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/EVAL_LOSS.md) para informaci칩n sobre c칩mo configurar e interpretar la p칠rdida de evaluaci칩n.

#### Vistas previas de validaci칩n

SimpleTuner soporta streaming de vistas previas intermedias de validaci칩n durante la generaci칩n usando modelos Tiny AutoEncoder. Esto te permite ver im치genes de validaci칩n gener치ndose paso a paso en tiempo real v칤a callbacks de webhook.

Para habilitarlo:
<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "validation_preview": true,
  "validation_preview_steps": 1
}
```
</details>

**Requisitos:**
- Configuraci칩n de webhook
- Validaci칩n habilitada

Configura `validation_preview_steps` a un valor m치s alto (p. ej., 3 o 5) para reducir el overhead del Tiny AutoEncoder. Con `validation_num_inference_steps=20` y `validation_preview_steps=5`, recibir치s vistas previas en los pasos 5, 10, 15 y 20.

### SageAttention

Al usar `--attention_mechanism=sageattention`, la inferencia puede acelerarse en tiempo de validaci칩n.

**Nota**: Esto no es compatible con _todas_ las configuraciones de modelo, pero vale la pena probarlo.

### Funciones experimentales avanzadas

<details>
<summary>Mostrar detalles experimentales avanzados</summary>


SimpleTuner incluye funciones experimentales que pueden mejorar significativamente la estabilidad y el rendimiento del entrenamiento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduce el sesgo de exposici칩n y mejora la calidad de salida dejando que el modelo genere sus propias entradas durante el entrenamiento.
*   **[Diff2Flow](../experimental/DIFF2FLOW.md):** permite entrenar con un objetivo Flow Matching, potencialmente mejorando la rectitud y la calidad de generaci칩n.

> 丘멆잺 Estas funciones aumentan la sobrecarga computacional del entrenamiento.
</details>
