## Gu√≠a r√°pida de Qwen Image

> üÜï ¬øBuscas los checkpoints de edici√≥n? Consulta la [gu√≠a r√°pida de Qwen Image Edit](./QWEN_EDIT.md) para instrucciones de entrenamiento con referencia emparejada.

En este ejemplo, entrenaremos un LoRA para Qwen Image, un modelo visi√≥n‚Äëlenguaje de 20B par√°metros. Debido a su tama√±o, necesitaremos t√©cnicas agresivas de optimizaci√≥n de memoria.

Una GPU de 24GB es el m√≠nimo absoluto, y aun as√≠ necesitar√°s cuantizaci√≥n extensa y configuraci√≥n cuidadosa. Se recomiendan encarecidamente 40GB+ para una experiencia m√°s fluida.

Al entrenar en 24G, las validaciones se quedar√°n sin memoria a menos que uses menor resoluci√≥n o un nivel de cuantizaci√≥n agresivo m√°s all√° de int8.

### Requisitos de hardware

Qwen Image es un modelo de 20B par√°metros con un codificador de texto sofisticado que por s√≠ solo consume ~16GB de VRAM antes de la cuantizaci√≥n. El modelo usa un VAE personalizado con 16 canales latentes.

**Limitaciones importantes:**
- **No est√° soportado en AMD ROCm ni MacOS** por falta de flash attention eficiente
- Tama√±o de lote > 1 no funciona correctamente por ahora; usa acumulaci√≥n de gradiente en su lugar
- TREAD (Text-Representation Enhanced Adversarial Diffusion) a√∫n no est√° soportado

### Requisitos previos

Aseg√∫rate de tener python instalado; SimpleTuner funciona bien con 3.10 a 3.12.

Puedes comprobarlo ejecutando:

```bash
python --version
```

Si no tienes python 3.12 instalado en Ubuntu, puedes intentar lo siguiente:

```bash
apt -y install python3.12 python3.12-venv
```

#### Dependencias de la imagen de contenedor

Para Vast, RunPod y TensorDock (entre otros), lo siguiente funcionar√° en una imagen CUDA 12.2-12.8 para habilitar la compilaci√≥n de extensiones CUDA:

```bash
apt -y install nvidia-cuda-toolkit
```

### Instalaci√≥n

Instala SimpleTuner v√≠a pip:

```bash
pip install 'simpletuner[cuda]'
```

Para instalaci√≥n manual o entorno de desarrollo, consulta la [documentaci√≥n de instalaci√≥n](../INSTALL.md).

### Configuraci√≥n del entorno

Para ejecutar SimpleTuner, necesitas configurar un archivo de configuraci√≥n, los directorios del dataset y del modelo, y un archivo de configuraci√≥n del dataloader.

#### Archivo de configuraci√≥n

Un script experimental, `configure.py`, puede permitirte omitir por completo esta secci√≥n mediante una configuraci√≥n interactiva paso a paso. Incluye funciones de seguridad que ayudan a evitar errores comunes.

**Nota:** Esto no configura tu dataloader. A√∫n tendr√°s que hacerlo manualmente m√°s adelante.

Para ejecutarlo:

```bash
simpletuner configure
```

> ‚ö†Ô∏è Para usuarios ubicados en pa√≠ses donde Hugging Face Hub no es f√°cilmente accesible, debes agregar `HF_ENDPOINT=https://hf-mirror.com` a tu `~/.bashrc` o `~/.zshrc` dependiendo de cu√°l `$SHELL` use tu sistema.

Si prefieres configurarlo manualmente:

Copia `config/config.json.example` a `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

All√≠, probablemente necesitar√°s modificar las siguientes variables:

- `model_type` - Config√∫ralo en `lora`.
- `lora_type` - Config√∫ralo en `standard` para PEFT LoRA o `lycoris` para LoKr.
- `model_family` - Config√∫ralo en `qwen_image`.
- `model_flavour` - Config√∫ralo en `v1.0`.
- `output_dir` - Config√∫ralo al directorio donde quieres guardar tus checkpoints y las im√°genes de validaci√≥n. Se recomienda usar una ruta completa aqu√≠.
- `train_batch_size` - Debe configurarse en 1 (tama√±o de lote > 1 no funciona actualmente).
- `gradient_accumulation_steps` - Config√∫ralo en 2-8 para simular tama√±os de lote mayores.
- `validation_resolution` - Debes configurarlo en `1024x1024` o menor por restricciones de memoria.
  - 24G no puede manejar validaciones 1024x1024 actualmente - tendr√°s que reducir el tama√±o
  - Se pueden especificar otras resoluciones separ√°ndolas con comas: `1024x1024,768x768,512x512`
- `validation_guidance` - Usa un valor alrededor de 3.0-4.0 para buenos resultados.
- `validation_num_inference_steps` - Usa alrededor de 30.
- `use_ema` - Configurar esto en `true` ayudar√° a obtener resultados m√°s suaves pero usa m√°s memoria.

- `optimizer` - Usa `optimi-lion` para buenos resultados, o `adamw-bf16` si tienes memoria de sobra.
- `mixed_precision` - Debe configurarse en `bf16` para Qwen Image.
- `gradient_checkpointing` - **Obligatorio** habilitar (`true`) para un uso razonable de memoria.
- `base_model_precision` - **Muy recomendado** configurar en `int8-quanto` o `nf4-bnb` para tarjetas de 24GB.
- `quantize_via` - Config√∫ralo en `cpu` para evitar OOM durante la cuantizaci√≥n en GPUs m√°s peque√±as.
- `quantize_activations` - Mant√©n esto en `false` para conservar la calidad de entrenamiento.

Ajustes de optimizaci√≥n de memoria para GPUs de 24GB:
- `lora_rank` - Usa 8 o menos.
- `lora_alpha` - Iguala esto a tu valor de lora_rank.
- `flow_schedule_shift` - Configura en 1.73 (o experimenta entre 1.0-3.0).

Tu config.json se ver√° algo as√≠ para un setup m√≠nimo:

<details>
<summary>Ver ejemplo de config</summary>

```json
{
    "model_type": "lora",
    "model_family": "qwen_image",
    "model_flavour": "v1.0",
    "lora_type": "standard",
    "lora_rank": 8,
    "lora_alpha": 8,
    "output_dir": "output/models-qwen_image",
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "validation_resolution": "1024x1024",
    "validation_guidance": 4.0,
    "validation_num_inference_steps": 30,
    "validation_seed": 42,
    "validation_prompt": "A photo-realistic image of a cat",
    "validation_step_interval": 100,
    "vae_batch_size": 1,
    "seed": 42,
    "resume_from_checkpoint": "latest",
    "resolution": 1024,
    "resolution_type": "pixel_area",
    "report_to": "tensorboard",
    "optimizer": "optimi-lion",
    "num_train_epochs": 0,
    "num_eval_images": 1,
    "mixed_precision": "bf16",
    "minimum_image_size": 0,
    "max_train_steps": 1000,
    "max_grad_norm": 0.01,
    "lr_warmup_steps": 100,
    "lr_scheduler": "constant_with_warmup",
    "learning_rate": "1e-4",
    "gradient_checkpointing": "true",
    "base_model_precision": "int2-quanto",
    "quantize_via": "cpu",
    "quantize_activations": false,
    "flow_schedule_shift": 1.73,
    "disable_benchmark": false,
    "data_backend_config": "config/qwen_image/multidatabackend.json",
    "checkpoints_total_limit": 5,
    "checkpoint_step_interval": 500,
    "caption_dropout_probability": 0.0,
    "aspect_bucket_rounding": 2
}
```
</details>

> ‚ÑπÔ∏è Usuarios multi-GPU pueden consultar [este documento](../OPTIONS.md#environment-configuration-variables) para informaci√≥n sobre c√≥mo configurar la cantidad de GPUs a usar.

> ‚ö†Ô∏è **Cr√≠tico para GPUs de 24GB**: El codificador de texto por s√≠ solo usa ~16GB de VRAM. Con cuantizaci√≥n `int2-quanto` o `nf4-bnb`, esto se puede reducir significativamente.

Para una comprobaci√≥n r√°pida con una configuraci√≥n conocida que funciona:

**Opci√≥n 1 (Recomendada - instalaci√≥n con pip):**
```bash
pip install 'simpletuner[cuda]'
simpletuner train example=qwen_image.peft-lora
```

**Opci√≥n 2 (M√©todo de git clone):**
```bash
simpletuner train env=examples/qwen_image.peft-lora
```

**Opci√≥n 3 (M√©todo heredado - a√∫n funciona):**
```bash
ENV=examples/qwen_image.peft-lora ./train.sh
```

### Funciones experimentales avanzadas

<details>
<summary>Mostrar detalles experimentales avanzados</summary>


SimpleTuner incluye funciones experimentales que pueden mejorar significativamente la estabilidad y el rendimiento del entrenamiento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduce el sesgo de exposici√≥n y mejora la calidad de la salida al permitir que el modelo genere sus propias entradas durante el entrenamiento.

> ‚ö†Ô∏è Estas funciones aumentan la sobrecarga computacional del entrenamiento.

#### Prompts de validaci√≥n

Dentro de `config/config.json` est√° el "prompt de validaci√≥n principal", que suele ser el instance_prompt principal en el que est√°s entrenando para tu √∫nico sujeto o estilo. Adem√°s, se puede crear un archivo JSON que contiene prompts adicionales para ejecutar durante las validaciones.

El archivo de ejemplo `config/user_prompt_library.json.example` tiene el siguiente formato:

```json
{
  "nickname": "the prompt goes here",
  "another_nickname": "another prompt goes here"
}
```

Los apodos son el nombre de archivo de la validaci√≥n, as√≠ que mantenlos cortos y compatibles con tu sistema de archivos.

Para indicar al entrenador esta librer√≠a de prompts, a√±√°dela a tu config.json:
```json
  "validation_prompt_library": "config/user_prompt_library.json",
```

Un conjunto de prompts diverso ayudar√° a determinar si el modelo est√° aprendiendo correctamente:

```json
{
    "anime_style": "a breathtaking anime-style portrait with vibrant colors and expressive features",
    "chef_cooking": "a high-quality, detailed photograph of a sous-chef immersed in culinary creation",
    "portrait": "a lifelike and intimate portrait showcasing unique personality and charm",
    "cinematic": "a cinematic, visually stunning photo with dramatic and captivating presence",
    "elegant": "an elegant and timeless portrait exuding grace and sophistication",
    "adventurous": "a dynamic and adventurous photo captured in an exciting moment",
    "mysterious": "a mysterious and enigmatic portrait shrouded in shadows and intrigue",
    "vintage": "a vintage-style portrait evoking the charm and nostalgia of a bygone era",
    "artistic": "an artistic and abstract representation blending creativity with visual storytelling",
    "futuristic": "a futuristic and cutting-edge portrayal set against advanced technology"
}
```

#### Seguimiento de puntuaciones CLIP

Si deseas habilitar evaluaciones para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/CLIP_SCORES.md) para informaci√≥n sobre c√≥mo configurar e interpretar las puntuaciones CLIP.

#### P√©rdida de evaluaci√≥n estable

Si deseas usar p√©rdida MSE estable para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/EVAL_LOSS.md) para informaci√≥n sobre c√≥mo configurar e interpretar la p√©rdida de evaluaci√≥n.

#### Vistas previas de validaci√≥n

SimpleTuner admite la transmisi√≥n de vistas previas de validaci√≥n intermedias durante la generaci√≥n usando modelos Tiny AutoEncoder. Esto te permite ver las im√°genes de validaci√≥n gener√°ndose paso a paso en tiempo real mediante callbacks de webhook.

Para habilitarlo:
```json
{
  "validation_preview": true,
  "validation_preview_steps": 1
}
```

**Requisitos:**
- Configuraci√≥n de webhook
- Validaci√≥n habilitada

Configura `validation_preview_steps` en un valor m√°s alto (p. ej., 3 o 5) para reducir la sobrecarga del Tiny AutoEncoder. Con `validation_num_inference_steps=20` y `validation_preview_steps=5`, recibir√°s im√°genes de vista previa en los pasos 5, 10, 15 y 20.

#### Desplazamiento del calendario de flujo

Qwen Image, como modelo de flow-matching, admite el desplazamiento del calendario de timesteps para controlar qu√© partes del proceso de generaci√≥n se entrenan.

El par√°metro `flow_schedule_shift` controla esto:
- Valores bajos (0.1-1.0): Enfoque en detalles finos
- Valores medios (1.0-3.0): Entrenamiento equilibrado (recomendado)
- Valores altos (3.0-6.0): Enfoque en grandes rasgos compositivos

##### Auto-shift
Puedes habilitar el shift de timesteps dependiente de la resoluci√≥n con `--flow_schedule_auto_shift`, que usa valores de shift m√°s altos para im√°genes grandes y valores m√°s bajos para im√°genes peque√±as. Esto puede dar resultados de entrenamiento estables pero potencialmente mediocres.

##### Especificaci√≥n manual
Se recomienda un valor `--flow_schedule_shift` de 1.73 como punto de partida para Qwen Image, aunque quiz√° necesites experimentar seg√∫n tu dataset y objetivos.

#### Consideraciones del dataset

Es crucial contar con un dataset sustancial para entrenar tu modelo. Hay limitaciones en el tama√±o del dataset, y debes asegurarte de que sea lo suficientemente grande para entrenar tu modelo de forma efectiva.

> ‚ÑπÔ∏è Con pocas im√°genes, podr√≠as ver el mensaje **no images detected in dataset** - aumentar el valor de `repeats` superar√° esta limitaci√≥n.

> ‚ö†Ô∏è **Importante**: Debido a limitaciones actuales, mant√©n `train_batch_size` en 1 y usa `gradient_accumulation_steps` para simular tama√±os de lote mayores.

Crea un documento `--data_backend_config` (`config/multidatabackend.json`) que contenga esto:

```json
[
  {
    "id": "pseudo-camera-10k-qwen",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "center",
    "resolution": 1024,
    "minimum_image_size": 512,
    "maximum_image_size": 1024,
    "target_downsample_size": 1024,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/qwen_image/pseudo-camera-10k",
    "instance_data_dir": "datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery",
    "repeats": 0,
    "is_regularisation_data": true
  },
  {
    "id": "dreambooth-subject",
    "type": "local",
    "crop": false,
    "resolution": 1024,
    "minimum_image_size": 512,
    "maximum_image_size": 1024,
    "target_downsample_size": 1024,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/qwen_image/dreambooth-subject",
    "instance_data_dir": "datasets/dreambooth-subject",
    "caption_strategy": "instanceprompt",
    "instance_prompt": "the name of your subject goes here",
    "metadata_backend": "discovery",
    "repeats": 1000
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/qwen_image",
    "disabled": false,
    "write_batch_size": 16
  }
]
```

> ‚ÑπÔ∏è Usa `caption_strategy=textfile` si tienes archivos `.txt` que contienen captions.
> Consulta las opciones y requisitos de caption_strategy en [DATALOADER.md](../DATALOADER.md#caption_strategy).
> ‚ÑπÔ∏è Nota el `write_batch_size` reducido para text embeds para evitar OOM.

Luego, crea un directorio `datasets`:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
    mkdir dreambooth-subject
    # place your images into dreambooth-subject/ now
popd
```

Esto descargar√° alrededor de 10k muestras de fotograf√≠as a tu directorio `datasets/pseudo-camera-10k`, que se crear√° autom√°ticamente.

Tus im√°genes de Dreambooth deben ir en el directorio `datasets/dreambooth-subject`.

#### Iniciar sesi√≥n en WandB y Huggingface Hub

Querr√°s iniciar sesi√≥n en WandB y HF Hub antes de empezar el entrenamiento, especialmente si usas `--push_to_hub` y `--report_to=wandb`.

Si vas a subir elementos a un repositorio Git LFS manualmente, tambi√©n deber√≠as ejecutar `git config --global credential.helper store`

Ejecuta los siguientes comandos:

```bash
wandb login
```

y

```bash
huggingface-cli login
```

Sigue las instrucciones para iniciar sesi√≥n en ambos servicios.

</details>

### Ejecutar el entrenamiento

Desde el directorio de SimpleTuner, solo hay que ejecutar:

```bash
./train.sh
```

Esto iniciar√° el cach√© a disco de text embeds y salidas del VAE.

Para m√°s informaci√≥n, consulta los documentos de [dataloader](../DATALOADER.md) y [tutorial](../TUTORIAL.md).

### Consejos de optimizaci√≥n de memoria

#### Configuraci√≥n de VRAM m√°s baja (m√≠nimo 24GB)

La configuraci√≥n de VRAM m√°s baja de Qwen Image requiere aproximadamente 24GB:

- OS: Ubuntu Linux 24
- GPU: Un solo dispositivo NVIDIA CUDA (m√≠nimo 24GB)
- Memoria del sistema: Se recomienda 64GB+
- Precisi√≥n del modelo base:
  - Para sistemas NVIDIA: `int2-quanto` o `nf4-bnb` (requerido para tarjetas de 24GB)
  - `int4-quanto` puede funcionar pero con menor calidad
- Optimizador: `optimi-lion` o `bnb-lion8bit-paged` para eficiencia de memoria
- Resoluci√≥n: Comienza con 512px o 768px, sube a 1024px si la memoria lo permite
- Tama√±o de lote: 1 (obligatorio por limitaciones actuales)
- Pasos de acumulaci√≥n de gradiente: 2-8 para simular lotes m√°s grandes
- Habilitar `--gradient_checkpointing` (obligatorio)
- Usa `--quantize_via=cpu` para evitar OOM durante el arranque
- Usa un rango LoRA peque√±o (1-8)
- Configurar la variable de entorno `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` ayuda a minimizar el uso de VRAM

**NOTA**: El pre-cach√© de embeddings del VAE y salidas del codificador de texto usar√° memoria significativa. Habilita `offload_during_startup=true` si encuentras problemas de OOM.

### Ejecutar inferencia en el LoRA despu√©s

Como Qwen Image es un modelo m√°s nuevo, aqu√≠ hay un ejemplo funcional de inferencia:

<details>
<summary>Mostrar ejemplo de inferencia en Python</summary>

```python
import torch
from diffusers import QwenImagePipeline, QwenImageTransformer2DModel
from transformers import Qwen2Tokenizer, Qwen2_5_VLForConditionalGeneration

model_id = 'Qwen/Qwen-Image'
adapter_id = 'your-username/your-lora-name'

# Load the pipeline
pipeline = QwenImagePipeline.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16
)

# Load LoRA weights
pipeline.load_lora_weights(adapter_id)

# Optional: quantize the model to save VRAM
from optimum.quanto import quantize, freeze, qint8
quantize(pipeline.transformer, weights=qint8)
freeze(pipeline.transformer)

# Move to device
pipeline.to('cuda' if torch.cuda.is_available() else 'cpu')

# Generate an image
prompt = "Your test prompt here"
negative_prompt = 'ugly, cropped, blurry, low-quality, mediocre average'

image = pipeline(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=30,
    guidance_scale=4.0,
    generator=torch.Generator(device='cuda').manual_seed(42),
    width=1024,
    height=1024,
).images[0]

image.save("output.png", format="PNG")
```
</details>

### Notas y consejos de soluci√≥n de problemas

#### Limitaciones de tama√±o de lote

Actualmente, Qwen Image tiene problemas con tama√±os de lote > 1 debido al manejo de longitud de secuencia en el codificador de texto. Usa siempre:
- `train_batch_size: 1`
- `gradient_accumulation_steps: 2-8` para simular lotes mayores

#### Cuantizaci√≥n

- `int2-quanto` brinda el mayor ahorro de memoria pero puede afectar la calidad
- `nf4-bnb` ofrece un buen equilibrio entre memoria y calidad
- `int4-quanto` es una opci√≥n intermedia
- Evita `int8` a menos que tengas 40GB+ de VRAM

#### Tasas de aprendizaje

Para entrenamiento LoRA:
- LoRAs peque√±as (rango 1-8): usa tasas alrededor de 1e-4
- LoRAs grandes (rango 16-32): usa tasas alrededor de 5e-5
- Con optimizador Prodigy: empieza con 1.0 y deja que se adapte

#### Artefactos de imagen

Si encuentras artefactos:
- Baja la tasa de aprendizaje
- Aumenta los pasos de acumulaci√≥n de gradiente
- Aseg√∫rate de que tus im√°genes sean de alta calidad y est√©n bien preprocesadas
- Considera usar resoluciones m√°s bajas al inicio

#### Entrenamiento de m√∫ltiples resoluciones

Comienza el entrenamiento en resoluciones m√°s bajas (512px o 768px) para acelerar el aprendizaje inicial, luego ajusta fino en 1024px. Habilita `--flow_schedule_auto_shift` al entrenar en diferentes resoluciones.

### Limitaciones de plataforma

**No soportado en:**
- AMD ROCm (no tiene implementaci√≥n eficiente de flash attention)
- Apple Silicon/MacOS (limitaciones de memoria y atenci√≥n)
- GPUs de consumo con menos de 24GB de VRAM

### Problemas conocidos actuales

1. Tama√±o de lote > 1 no funciona correctamente (usa acumulaci√≥n de gradiente)
2. TREAD a√∫n no est√° soportado
3. Alto uso de memoria del codificador de texto (~16GB antes de cuantizaci√≥n)
4. Problemas de manejo de longitud de secuencia ([issue upstream](https://github.com/huggingface/diffusers/issues/12075))

Para ayuda adicional y soluci√≥n de problemas, consulta la [documentaci√≥n de SimpleTuner](/documentation) o √∫nete al Discord de la comunidad.
