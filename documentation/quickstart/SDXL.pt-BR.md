## Guia de In√≠cio R√°pido do Stable Diffusion XL

Neste exemplo, vamos treinar um modelo Stable Diffusion XL usando o toolkit SimpleTuner e o tipo de modelo `lora`.

Em compara√ß√£o com modelos modernos maiores, o SDXL √© bem modesto em tamanho, ent√£o talvez seja poss√≠vel usar treinamento `full`, mas isso vai exigir VRAM adicional em compara√ß√£o ao LoRA e alguns ajustes de hiperpar√¢metros.

### Pr√©-requisitos

Certifique-se de que voc√™ tenha Python instalado; o SimpleTuner funciona bem com 3.10 at√© 3.12 (m√°quinas AMD ROCm exigem 3.12).

Voc√™ pode verificar executando:

```bash
python --version
```

Se voc√™ n√£o tem Python 3.12 instalado no Ubuntu, pode tentar o seguinte:

```bash
apt -y install python3.13 python3.13-venv
```

#### Depend√™ncias da imagem de cont√™iner

Para Vast, RunPod e TensorDock (entre outros), o seguinte funciona em uma imagem CUDA 12.2-12.8 para habilitar a compila√ß√£o de extens√µes CUDA:

```bash
apt -y install nvidia-cuda-toolkit
```

### Instala√ß√£o

Instale o SimpleTuner via pip:

```bash
pip install 'simpletuner[cuda]'

# CUDA 13 / Blackwell users (NVIDIA B-series GPUs)
pip install 'simpletuner[cuda13]'
```

Para instala√ß√£o manual ou setup de desenvolvimento, veja a [documenta√ß√£o de instala√ß√£o](../INSTALL.md).

### Configurando o ambiente

Para rodar o SimpleTuner, voc√™ precisar√° configurar um arquivo de configura√ß√£o, os diret√≥rios de dataset e modelo, e um arquivo de configura√ß√£o do dataloader.

#### Arquivo de configura√ß√£o

Um script experimental, `configure.py`, pode permitir que voc√™ pule esta se√ß√£o inteiramente por meio de uma configura√ß√£o interativa passo a passo. Ele cont√©m alguns recursos de seguran√ßa que ajudam a evitar armadilhas comuns.

**Nota:** Isso n√£o **configura totalmente** seu dataloader. Voc√™ ainda ter√° que fazer isso manualmente, mais tarde.

Para execut√°-lo:

```bash
simpletuner configure
```
> ‚ö†Ô∏è Para usu√°rios localizados em pa√≠ses onde o Hugging Face Hub n√£o √© facilmente acess√≠vel, voc√™ deve adicionar `HF_ENDPOINT=https://hf-mirror.com` ao seu `~/.bashrc` ou `~/.zshrc` dependendo de qual `$SHELL` seu sistema usa.

Se voc√™ preferir configurar manualmente:

Copie `config/config.json.example` para `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

#### Etapas adicionais para AMD ROCm

O seguinte deve ser executado para um AMD MI300X ser utiliz√°vel:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

L√°, voc√™ precisar√° modificar as seguintes vari√°veis:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
  "model_type": "lora",
  "model_family": "sdxl",
  "model_flavour": "base-1.0",
  "output_dir": "/home/user/output/models",
  "validation_resolution": "1024x1024,1280x768",
  "validation_guidance": 3.4,
  "use_gradient_checkpointing": true,
  "learning_rate": 1e-4
}
```
</details>

- `model_family` - Defina como `sdxl`.
- `model_flavour` - Defina como `base-1.0`, ou use `pretrained_model_name_or_path` para apontar para um modelo diferente.
- `model_type` - Defina como `lora`.
- `use_dora` - Defina como `true` se voc√™ quiser treinar DoRA.
- `output_dir` - Defina o diret√≥rio onde deseja armazenar seus checkpoints e imagens de valida√ß√£o. √â recomendado usar um caminho completo aqui.
- `validation_resolution` - Defina como `1024x1024` para este exemplo.
  - Al√©m disso, o Stable Diffusion XL foi fine-tuned em buckets multi-aspect e outras resolu√ß√µes podem ser especificadas separando por v√≠rgulas: `1024x1024,1280x768`
- `validation_guidance` - Use qualquer valor com o qual voc√™ se sinta confort√°vel para testes em infer√™ncia. Defina entre `4.2` e `6.4`.
- `use_gradient_checkpointing` - Provavelmente deve ser `true`, a menos que voc√™ tenha MUITA VRAM e queira sacrificar um pouco para ficar mais r√°pido.
- `learning_rate` - `1e-4` √© bastante comum para redes de baixo rank, embora `1e-5` possa ser uma escolha mais conservadora se voc√™ notar algum "burning" ou overtraining precoce.

H√° algumas configura√ß√µes adicionais se voc√™ estiver em um Mac M-series:

- `mixed_precision` deve ser definido como `no`.
  - Isso era verdade no pytorch 2.4, mas talvez o bf16 j√° possa ser usado agora a partir do 2.6+
- `attention_mechanism` pode ser definido como `xformers` para aproveitar isso, mas √© meio obsoleto.

#### Treinamento de modelo quantizado

Testado em sistemas Apple e NVIDIA, o Hugging Face Optimum-Quanto pode ser usado para reduzir a precis√£o e os requisitos de VRAM do Unet, mas n√£o funciona t√£o bem quanto em modelos Diffusion Transformer como SD3/Flux, ent√£o n√£o √© recomendado.

Se voc√™ estiver com recursos apertados, ainda √© poss√≠vel usar.

Para `config.json`:
<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
  "base_model_precision": "int8-quanto",
  "text_encoder_1_precision": "no_change",
  "text_encoder_2_precision": "no_change",
  "optimizer": "optimi-lion"
}
```
</details>

#### Recursos experimentais avan√ßados

<details>
<summary>Mostrar detalhes experimentais avan√ßados</summary>


O SimpleTuner inclui recursos experimentais que podem melhorar significativamente a estabilidade e o desempenho do treinamento, especialmente para datasets menores ou arquiteturas mais antigas como o SDXL.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduz o vi√©s de exposi√ß√£o e melhora a qualidade de sa√≠da ao deixar o modelo gerar suas pr√≥prias entradas durante o treinamento.
*   **[Diff2Flow](../experimental/DIFF2FLOW.md):** permite treinar SDXL com um objetivo de Flow Matching, potencialmente melhorando a consist√™ncia e a qualidade da gera√ß√£o.

> ‚ö†Ô∏è Esses recursos aumentam a sobrecarga computacional do treinamento.

</details>

#### Considera√ß√µes sobre o dataset

√â crucial ter um dataset substancial para treinar seu modelo. Existem limita√ß√µes no tamanho do dataset, e voc√™ precisar√° garantir que seu dataset seja grande o suficiente para treinar seu modelo efetivamente. Observe que o tamanho m√≠nimo do dataset √© `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`. O dataset n√£o ser√° descoberto pelo treinador se for muito pequeno.

> üí° **Dica:** Para datasets grandes em que espa√ßo em disco √© uma preocupa√ß√£o, voc√™ pode usar `--vae_cache_disable` para realizar codifica√ß√£o VAE online sem armazenar os resultados no disco. Isso √© implicitamente habilitado se voc√™ usar `--vae_cache_ondemand`, mas adicionar `--vae_cache_disable` garante que nada seja gravado em disco.

Dependendo do dataset que voc√™ tem, ser√° necess√°rio configurar o diret√≥rio do dataset e o arquivo de configura√ß√£o do dataloader de forma diferente. Neste exemplo, usaremos [pseudo-camera-10k](https://huggingface.co/datasets/bghira/pseudo-camera-10k) como dataset.

No seu diret√≥rio `OUTPUT_DIR`, crie um multidatabackend.json:

<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
[
  {
    "id": "pseudo-camera-10k-sdxl",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "random",
    "resolution": 1.0,
    "minimum_image_size": 0.25,
    "maximum_image_size": 1.0,
    "target_downsample_size": 1.0,
    "resolution_type": "area",
    "cache_dir_vae": "cache/vae/sdxl/pseudo-camera-10k",
    "instance_data_dir": "/home/user/simpletuner/datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery"
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/sdxl/pseudo-camera-10k",
    "disabled": false,
    "write_batch_size": 128
  }
]
```
</details>

> Veja op√ß√µes e requisitos de caption_strategy em [DATALOADER.md](../DATALOADER.md#caption_strategy).

Depois, crie um diret√≥rio `datasets`:

```bash
mkdir -p datasets
huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=datasets/pseudo-camera-10k
```

Isso vai baixar cerca de 10k amostras de fotos para o seu diret√≥rio `datasets/pseudo-camera-10k`, que ser√° criado automaticamente.

#### Login no WandB e Huggingface Hub

Voc√™ vai querer fazer login no WandB e no HF Hub antes de iniciar o treinamento, especialmente se estiver usando `push_to_hub: true` e `--report_to=wandb`.

Se voc√™ vai enviar itens para um reposit√≥rio Git LFS manualmente, tamb√©m deve executar `git config --global credential.helper store`

Execute os seguintes comandos:

```bash
wandb login
```

e

```bash
huggingface-cli login
```

Siga as instru√ß√µes para fazer login nos dois servi√ßos.

### Executando o treinamento

No diret√≥rio do SimpleTuner, basta executar:

```bash
bash train.sh
```

Isso vai iniciar o cache de text embeds e sa√≠das VAE em disco.

Para mais informa√ß√µes, veja os documentos [dataloader](../DATALOADER.md) e [tutorial](../TUTORIAL.md).

### Acompanhamento de CLIP score

Se voc√™ quiser habilitar avalia√ß√µes para pontuar o desempenho do modelo, veja [este documento](../evaluation/CLIP_SCORES.md) para informa√ß√µes sobre configura√ß√£o e interpreta√ß√£o de CLIP scores.

# Perda de avalia√ß√£o est√°vel

Se voc√™ quiser usar perda MSE est√°vel para pontuar o desempenho do modelo, veja [este documento](../evaluation/EVAL_LOSS.md) para informa√ß√µes sobre configura√ß√£o e interpreta√ß√£o de perda de avalia√ß√£o.

#### Pr√©vias de valida√ß√£o

O SimpleTuner suporta streaming de pr√©vias intermedi√°rias de valida√ß√£o durante a gera√ß√£o usando modelos Tiny AutoEncoder. Isso permite ver imagens de valida√ß√£o sendo geradas passo a passo em tempo real via callbacks de webhook.

Para habilitar:
<details>
<summary>Ver exemplo de configura√ß√£o</summary>

```json
{
  "validation_preview": true,
  "validation_preview_steps": 1
}
```
</details>

**Requisitos:**
- Configura√ß√£o de webhook
- Valida√ß√£o habilitada

Defina `validation_preview_steps` para um valor maior (ex.: 3 ou 5) para reduzir a sobrecarga do Tiny AutoEncoder. Com `validation_num_inference_steps=20` e `validation_preview_steps=5`, voc√™ receber√° imagens de pr√©via nos steps 5, 10, 15 e 20.
