# Guia de In√≠cio R√°pido do LTX Video 2

Neste exemplo, vamos treinar um LoRA do LTX Video 2 usando os VAEs de v√≠deo/√°udio do LTX-2 e um encoder de texto Gemma3.

## Requisitos de hardware

LTX Video 2 √© um modelo pesado de **19B**. Ele combina:
1.  **Gemma3**: O encoder de texto.
2.  **LTX-2 Video VAE** (mais o Audio VAE quando h√° condicionamento por √°udio).
3.  **Video Transformer de 19B**: Um backbone DiT grande.

Este setup √© intensivo em VRAM, e o pr√©-cache do VAE pode aumentar o uso de mem√≥ria.

- **Treino em uma GPU**: Comece com `train_batch_size: 1` e habilite group offload.
  - **Nota**: O **passo inicial de pr√©-cache do VAE** pode exigir mais VRAM. Talvez seja necess√°rio offload para CPU ou uma GPU maior apenas na fase de cache.
  - **Dica**: Defina `"offload_during_startup": true` no seu `config.json` para garantir que o VAE e o encoder de texto n√£o sejam carregados na GPU ao mesmo tempo, reduzindo bastante a press√£o de mem√≥ria do pr√©-cache.
- **Treino multi-GPU**: **FSDP2** ou **Group Offload** agressivo √© recomendado se voc√™ precisa de mais folga.
- **RAM do sistema**: 64GB+ √© recomendado para execu√ß√µes maiores; mais RAM ajuda no cache.

### Desempenho e mem√≥ria observados (relatos de campo)

- **Configura√ß√£o base**: 480p, 17 frames, batch size 2 (m√≠nima dura√ß√£o/resolu√ß√£o).
- **RamTorch (incl. encoder de texto)**: ~13 GB de VRAM em AMD 7900XTX.
  - NVIDIA 3090/4090/5090+ deve ter folga similar ou melhor.
- **Sem offload (int8 TorchAO)**: ~29-30 GB de VRAM; hardware de 32 GB recomendado.
  - Pico de RAM do sistema: ~46 GB ao carregar Gemma3 bf16 e depois quantizar para int8 (~32 GB VRAM).
  - Pico de RAM do sistema: ~34 GB ao carregar o transformer LTX-2 bf16 e depois quantizar para int8 (~30 GB VRAM).
- **Sem offload (bf16 completo)**: ~48 GB de VRAM necess√°rios para treinar o modelo sem offload.
- **Throughput**:
  - ~8 s/step em A100-80G SXM4 (sem compila√ß√£o).
  - ~16 s/step em 7900XTX (execu√ß√£o local).
  - ~30 min para 200 steps em A100-80G SXM4.

### Offload de mem√≥ria (Cr√≠tico)

Para a maioria dos setups de GPU √∫nica treinando o LTX Video 2, √© recomend√°vel habilitar offload em grupo. √â opcional, mas ajuda a manter folga de VRAM em batches/resolu√ß√µes maiores.

Adicione isto ao seu `config.json`:

<details>
<summary>Ver exemplo de config</summary>

```json
{
  "enable_group_offload": true,
  "group_offload_type": "block_level",
  "group_offload_blocks_per_group": 1,
  "group_offload_use_stream": true
}
```
</details>

## Pr√©-requisitos

Garanta que o Python 3.12 esteja instalado.

```bash
python --version
```

## Instala√ß√£o

```bash
pip install 'simpletuner[cuda]'

# CUDA 13 / Blackwell users (NVIDIA B-series GPUs)
pip install 'simpletuner[cuda13]'
```

Veja [INSTALL.md](../INSTALL.md) para op√ß√µes avan√ßadas de instala√ß√£o.

## Configurando o ambiente

### Interface web

```bash
simpletuner server
```
Acesse em http://localhost:8001.

### Configura√ß√£o manual

Execute o script auxiliar:

```bash
simpletuner configure
```

Ou copie o exemplo e edite manualmente:

```bash
cp config/config.json.example config/config.json
```

#### Par√¢metros de configura√ß√£o

Configura√ß√µes-chave para LTX Video 2:

- `model_family`: `ltxvideo2`
- `model_flavour`: `dev` (padr√£o), `dev-fp4` ou `dev-fp8`.
- `pretrained_model_name_or_path`: `Lightricks/LTX-2` (reposit√≥rio com o checkpoint combinado) ou um arquivo `.safetensors` local.
- `train_batch_size`: `1`. N√£o aumente isso a menos que voc√™ tenha um A100/H100.
- `validation_resolution`:
  - `512x768` √© um padr√£o seguro para testes.
  - `720x1280` (720p) √© poss√≠vel, mas pesado.
- `validation_num_video_frames`: **Deve ser compat√≠vel com a compress√£o do VAE (4x).**
  - Para 5s (em ~12-24fps): Use `61` ou `49`.
  - F√≥rmula: `(frames - 1) % 4 == 0`.
- `validation_guidance`: `5.0`.
- `frame_rate`: O padr√£o √© 25.

O LTX-2 √© distribu√≠do como um √∫nico checkpoint `.safetensors` que inclui o transformer, o VAE de v√≠deo,
o VAE de √°udio e o vocoder. O SimpleTuner carrega desse arquivo combinado conforme o `model_flavour` (dev/dev-fp4/dev-fp8).

### Opcional: otimiza√ß√µes de VRAM

Se precisar de mais folga de VRAM:
- **Block swap (Musubi)**: Defina `musubi_blocks_to_swap` (tente `4-8`) e opcionalmente `musubi_block_swap_device` (padr√£o `cpu`) para streamar os √∫ltimos blocos do transformer da CPU. Menor throughput, menor pico de VRAM.
- **Convolu√ß√£o por patches do VAE**: Defina `--vae_enable_patch_conv=true` para habilitar chunking temporal no VAE do LTX-2; espere uma pequena perda de velocidade, mas menor pico de VRAM.
- **Temporal roll do VAE**: Defina `--vae_enable_temporal_roll=true` para um chunking temporal mais agressivo (perda de velocidade maior).
- **VAE tiling**: Defina `--vae_enable_tiling=true` para dividir o encode/decode do VAE em resolu√ß√µes grandes.

### Opcional: regularizador temporal CREPA

Para reduzir flicker e manter assuntos est√°veis entre frames:
- Em **Training ‚Üí Loss functions**, habilite **CREPA**.
- Valores iniciais recomendados: **Block Index = 8**, **Weight = 0.5**, **Adjacent Distance = 1**, **Temporal Decay = 1.0**.
- Mantenha o encoder de vis√£o padr√£o (`dinov2_vitg14`, tamanho `518`) a menos que voc√™ precise de um menor (`dinov2_vits14` + `224`).
- Requer rede (ou um torch hub em cache) para buscar os pesos do DINOv2 na primeira vez.
- S√≥ habilite **Drop VAE Encoder** se voc√™ estiver treinando inteiramente a partir de latentes em cache; caso contr√°rio, deixe desligado.

### Recursos experimentais avan√ßados

<details>
<summary>Mostrar detalhes experimentais avan√ßados</summary>


SimpleTuner inclui recursos experimentais que podem melhorar significativamente a estabilidade e o desempenho do treinamento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduz vi√©s de exposi√ß√£o e melhora a qualidade ao permitir que o modelo gere suas pr√≥prias entradas durante o treinamento.

> ‚ö†Ô∏è Esses recursos aumentam o overhead computacional do treinamento.

#### Considera√ß√µes sobre o dataset

Datasets de v√≠deo exigem configura√ß√£o cuidadosa. Crie `config/multidatabackend.json`:

```json
[
  {
    "id": "my-video-dataset",
    "type": "local",
    "dataset_type": "video",
    "instance_data_dir": "datasets/videos",
    "caption_strategy": "textfile",
    "resolution": 512,
    "video": {
        "num_frames": 61,
        "min_frames": 61,
        "frame_rate": 25,
        "bucket_strategy": "aspect_ratio"
    },
    "audio": {
        "auto_split": true,
        "sample_rate": 16000,
        "channels": 1,
        "duration_interval": 3.0
    },
    "repeats": 10
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/ltxvideo2",
    "disabled": false
  }
]
```

Na subse√ß√£o `video`:
- `num_frames`: Contagem de frames alvo para treino.
- `min_frames`: Comprimento m√≠nimo de v√≠deo (v√≠deos mais curtos s√£o descartados).
- `max_frames`: Filtro de comprimento m√°ximo de v√≠deo.
- `bucket_strategy`: Como os v√≠deos s√£o agrupados em buckets:
  - `aspect_ratio` (padr√£o): agrupa apenas pela propor√ß√£o espacial.
  - `resolution_frames`: agrupa pelo formato `WxH@F` (ex.: `1920x1080@61`) para datasets com resolu√ß√£o/dura√ß√£o mistas.
- `frame_interval`: Ao usar `resolution_frames`, arredonde a contagem de frames para este intervalo.

O auto-split de √°udio fica habilitado por padr√£o em datasets de v√≠deo. Adicione um bloco `audio` para ajustar
sample rate/canais, defina `audio.auto_split: false` para desativar, ou forne√ßa um dataset de √°udio separado e
vincule via `s2v_datasets`. O SimpleTuner vai cachear os latentes de √°udio junto com os latentes de v√≠deo.

> Veja op√ß√µes e requisitos de caption_strategy em [DATALOADER.md](../DATALOADER.md#caption_strategy).

#### Configura√ß√£o de diret√≥rios

```bash
mkdir -p datasets/videos
</details>

# Place .mp4 / .mov files here.
# Place corresponding .txt files with same filename for captions.
```

#### Login

```bash
wandb login
huggingface-cli login
```

### Executando o treinamento

```bash
simpletuner train
```

## Notas e dicas de troubleshooting

### Sem mem√≥ria (OOM)

Treino de v√≠deo √© extremamente exigente. Se der OOM:

1.  **Reduza a resolu√ß√£o**: Tente 480p (`480x854` ou similar).
2.  **Reduza frames**: Baixe `validation_num_video_frames` e `num_frames` do dataset para `33` ou `49`.
3.  **Cheque o offload**: Garanta que `--enable_group_offload` est√° ativo.

### Qualidade do v√≠deo de valida√ß√£o

- **V√≠deos pretos/ru√≠do**: Geralmente causados por `validation_guidance` alto demais (> 6.0) ou baixo demais (< 2.0). Fique em `5.0`.
- **Tremor de movimento**: Verifique se o frame rate do seu dataset corresponde ao frame rate em que o modelo foi treinado (geralmente 25fps).
- **V√≠deo est√°tico**: O modelo pode estar subtreinado ou o prompt n√£o descreve movimento. Use prompts como "camera pans right", "zoom in", "running", etc.

### Treino TREAD

TREAD funciona para v√≠deo tamb√©m e √© altamente recomendado para economizar compute.

Adicione ao `config.json`:

<details>
<summary>Ver exemplo de config</summary>

```json
{
  "tread_config": {
    "routes": [
      {
        "selection_ratio": 0.5,
        "start_layer_idx": 2,
        "end_layer_idx": -2
      }
    ]
  }
}
```
</details>

Isso pode acelerar o treino em ~25-40% dependendo da raz√£o.

### Configura√ß√£o de menor uso de VRAM (7900XTX)

Configura√ß√£o testada em campo que prioriza o menor uso de VRAM no LTX Video 2.

<details>
<summary>Ver configura√ß√£o 7900XTX (menor uso de VRAM)</summary>

```json
{
  "base_model_precision": "int8-quanto",
  "checkpoint_step_interval": 100,
  "data_backend_config": "config/ltx2/multidatabackend.json",
  "disable_benchmark": true,
  "dynamo_mode": "",
  "evaluation_type": "none",
  "hub_model_id": "simpletuner-ltxvideo2-19b-t2v-lora-test",
  "learning_rate": 0.00006,
  "lr_warmup_steps": 50,
  "lycoris_config": "config/lycoris_config.json",
  "max_grad_norm": 0.1,
  "max_train_steps": 200,
  "minimum_image_size": 0,
  "model_family": "ltxvideo2",
  "model_flavour": "dev",
  "model_type": "lora",
  "num_train_epochs": 0,
  "offload_during_startup": true,
  "optimizer": "adamw_bf16",
  "output_dir": "output/examples/ltxvideo2-19b-t2v.peft-lora",
  "override_dataset_config": true,
  "ramtorch": true,
  "ramtorch_text_encoder": true,
  "report_to": "none",
  "resolution": 480,
  "scheduled_sampling_reflexflow": false,
  "seed": 42,
  "skip_file_discovery": "",
  "tracker_project_name": "lora-training",
  "tracker_run_name": "example-training-run",
  "train_batch_size": 2,
  "vae_batch_size": 1,
  "vae_enable_patch_conv": true,
  "vae_enable_slicing": true,
  "vae_enable_temporal_roll": true,
  "vae_enable_tiling": true,
  "validation_disable": true,
  "validation_disable_unconditional": true,
  "validation_guidance": 5,
  "validation_num_inference_steps": 40,
  "validation_num_video_frames": 81,
  "validation_prompt": "üü´ is holding a sign that says hello world from ltxvideo2",
  "validation_resolution": "768x512",
  "validation_seed": 42,
  "validation_using_datasets": false
}
```
</details>

### Fluxos de valida√ß√£o (T2V vs I2V)

- **T2V (texto para v√≠deo)**: Deixe `validation_using_datasets: false` e use `validation_prompt` ou `validation_prompt_library`.
- **I2V (imagem para v√≠deo)**: Defina `validation_using_datasets: true` e aponte `eval_dataset_id` para um split de valida√ß√£o que forne√ßa uma imagem de refer√™ncia. A valida√ß√£o alterna para o pipeline de imagem para v√≠deo e usa essa imagem como condicionamento.
- **S2V (condicionado por √°udio)**: Com `validation_using_datasets: true`, garanta que `eval_dataset_id` aponte para um dataset com `s2v_datasets` (ou o comportamento padr√£o de `audio.auto_split`). A valida√ß√£o carrega os latentes de √°udio em cache automaticamente.

### Adaptadores de valida√ß√£o (LoRAs)

Os LoRAs da Lightricks podem ser aplicados na valida√ß√£o via `validation_adapter_path` (√∫nico) ou
`validation_adapter_config` (v√°rias execu√ß√µes). Esses repos usam filenames de peso n√£o padr√£o, ent√£o use
`repo_id:weight_name`. Veja a cole√ß√£o LTX-2 para os filenames corretos e assets relacionados:
https://huggingface.co/collections/Lightricks/ltx-2
- `Lightricks/LTX-2-19b-IC-LoRA-Canny-Control:ltx-2-19b-ic-lora-canny-control.safetensors`
- `Lightricks/LTX-2-19b-IC-LoRA-Depth-Control:ltx-2-19b-ic-lora-depth-control.safetensors`
- `Lightricks/LTX-2-19b-IC-LoRA-Detailer:ltx-2-19b-ic-lora-detailer.safetensors`
- `Lightricks/LTX-2-19b-IC-LoRA-Pose-Control:ltx-2-19b-ic-lora-pose-control.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-In:ltx-2-19b-lora-camera-control-dolly-in.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Out:ltx-2-19b-lora-camera-control-dolly-out.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Left:ltx-2-19b-lora-camera-control-dolly-left.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Dolly-Right:ltx-2-19b-lora-camera-control-dolly-right.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Down:ltx-2-19b-lora-camera-control-jib-down.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Jib-Up:ltx-2-19b-lora-camera-control-jib-up.safetensors`
- `Lightricks/LTX-2-19b-LoRA-Camera-Control-Static:ltx-2-19b-lora-camera-control-static.safetensors`

Para valida√ß√£o mais r√°pida, use `Lightricks/LTX-2-19b-distilled-lora-384:ltx-2-19b-distilled-lora-384.safetensors`
como adaptador e defina `validation_guidance: 1` junto com `validation_num_inference_steps: 8`.
