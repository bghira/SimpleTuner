## Guia de In√≠cio R√°pido do Qwen Image

> üÜï Procurando os checkpoints de edi√ß√£o? Veja o [guia de In√≠cio R√°pido do Qwen Image Edit](./QWEN_EDIT.md) para instru√ß√µes de treino com refer√™ncia pareada.

Neste exemplo, vamos treinar um LoRA para o Qwen Image, um modelo vis√£o-linguagem de 20B par√¢metros. Devido ao tamanho, precisaremos de t√©cnicas agressivas de otimiza√ß√£o de mem√≥ria.

Uma GPU de 24GB √© o m√≠nimo absoluto e, mesmo assim, voc√™ precisar√° de quantiza√ß√£o extensa e configura√ß√£o cuidadosa. 40GB+ √© fortemente recomendado para uma experi√™ncia mais tranquila.

Ao treinar em 24G, as valida√ß√µes v√£o dar OOM a menos que voc√™ use resolu√ß√£o menor ou quantiza√ß√£o agressiva al√©m de int8.

### Requisitos de hardware

Qwen Image √© um modelo de 20B par√¢metros com um encoder de texto sofisticado que, sozinho, consome ~16GB de VRAM antes de quantiza√ß√£o. O modelo usa um VAE customizado com 16 canais latentes.

**Limita√ß√µes importantes:**
- **N√£o suportado em AMD ROCm ou MacOS** devido √† falta de flash attention eficiente
- Batch size > 1 n√£o funciona corretamente no momento; use gradient accumulation em vez disso
- TREAD (Text-Representation Enhanced Adversarial Diffusion) ainda n√£o √© suportado

### Pr√©-requisitos

Certifique-se de que voc√™ tem Python instalado; o SimpleTuner funciona bem com 3.10 at√© 3.12.

Voc√™ pode verificar executando:

```bash
python --version
```

Se voc√™ n√£o tem o Python 3.12 instalado no Ubuntu, pode tentar o seguinte:

```bash
apt -y install python3.13 python3.13-venv
```

#### Depend√™ncias da imagem de cont√™iner

Para Vast, RunPod e TensorDock (entre outros), o seguinte funciona em uma imagem CUDA 12.2-12.8 para habilitar a compila√ß√£o de extens√µes CUDA:

```bash
apt -y install nvidia-cuda-toolkit
```

### Instala√ß√£o

Instale o SimpleTuner via pip:

```bash
pip install 'simpletuner[cuda]'

# CUDA 13 / Blackwell users (NVIDIA B-series GPUs)
pip install 'simpletuner[cuda13]'
```

Para instala√ß√£o manual ou setup de desenvolvimento, veja a [documenta√ß√£o de instala√ß√£o](../INSTALL.md).

### Configurando o ambiente

Para rodar o SimpleTuner, voc√™ precisar√° configurar um arquivo de configura√ß√£o, os diret√≥rios de dataset e modelo, e um arquivo de configura√ß√£o do dataloader.

#### Arquivo de configura√ß√£o

Um script experimental, `configure.py`, pode permitir que voc√™ pule esta se√ß√£o inteiramente por meio de uma configura√ß√£o interativa passo a passo. Ele cont√©m alguns recursos de seguran√ßa que ajudam a evitar armadilhas comuns.

**Nota:** Isso n√£o configura seu dataloader. Voc√™ ainda precisar√° fazer isso manualmente depois.

Para execut√°-lo:

```bash
simpletuner configure
```

> ‚ö†Ô∏è Para usu√°rios localizados em pa√≠ses onde o Hugging Face Hub n√£o √© facilmente acess√≠vel, voc√™ deve adicionar `HF_ENDPOINT=https://hf-mirror.com` ao seu `~/.bashrc` ou `~/.zshrc` dependendo de qual `$SHELL` seu sistema usa.

Se voc√™ preferir configurar manualmente:

Copie `config/config.json.example` para `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

L√°, voc√™ provavelmente precisar√° modificar as seguintes vari√°veis:

- `model_type` - Defina como `lora`.
- `lora_type` - Defina como `standard` para PEFT LoRA ou `lycoris` para LoKr.
- `model_family` - Defina como `qwen_image`.
- `model_flavour` - Defina como `v1.0`.
- `output_dir` - Defina como o diret√≥rio onde voc√™ quer armazenar seus checkpoints e imagens de valida√ß√£o. √â recomendado usar um caminho completo aqui.
- `train_batch_size` - Deve ser 1 (batch size > 1 n√£o funciona atualmente).
- `gradient_accumulation_steps` - Defina entre 2-8 para simular batch sizes maiores.
- `validation_resolution` - Defina `1024x1024` ou menos por restri√ß√µes de mem√≥ria.
  - 24G n√£o aguenta valida√ß√µes 1024x1024 atualmente - voc√™ precisar√° reduzir o tamanho
  - Outras resolu√ß√µes podem ser especificadas usando v√≠rgulas: `1024x1024,768x768,512x512`
- `validation_guidance` - Use um valor em torno de 3.0-4.0 para bons resultados.
- `validation_num_inference_steps` - Use algo em torno de 30.
- `use_ema` - Definir como `true` ajuda a obter resultados mais suaves, mas usa mais mem√≥ria.

- `optimizer` - Use `optimi-lion` para bons resultados, ou `adamw-bf16` se tiver mem√≥ria de sobra.
- `mixed_precision` - Deve ser definido como `bf16` para o Qwen Image.
- `gradient_checkpointing` - **Obrigat√≥rio** habilitar (`true`) para uso de mem√≥ria aceit√°vel.
- `base_model_precision` - **Fortemente recomendado** definir `int8-quanto` ou `nf4-bnb` para placas de 24GB.
- `quantize_via` - Defina como `cpu` para evitar OOM durante a quantiza√ß√£o em GPUs menores.
- `quantize_activations` - Mantenha como `false` para preservar qualidade de treino.

Configura√ß√µes de otimiza√ß√£o de mem√≥ria para GPUs 24GB:
- `lora_rank` - Use 8 ou menos.
- `lora_alpha` - Igual ao valor de lora_rank.
- `flow_schedule_shift` - Defina como 1.73 (ou experimente entre 1.0-3.0).

Seu config.json vai ficar mais ou menos assim para um setup m√≠nimo:

<details>
<summary>Ver exemplo de config</summary>

```json
{
    "model_type": "lora",
    "model_family": "qwen_image",
    "model_flavour": "v1.0",
    "lora_type": "standard",
    "lora_rank": 8,
    "lora_alpha": 8,
    "output_dir": "output/models-qwen_image",
    "train_batch_size": 1,
    "gradient_accumulation_steps": 4,
    "validation_resolution": "1024x1024",
    "validation_guidance": 4.0,
    "validation_num_inference_steps": 30,
    "validation_seed": 42,
    "validation_prompt": "A photo-realistic image of a cat",
    "validation_step_interval": 100,
    "vae_batch_size": 1,
    "seed": 42,
    "resume_from_checkpoint": "latest",
    "resolution": 1024,
    "resolution_type": "pixel_area",
    "report_to": "tensorboard",
    "optimizer": "optimi-lion",
    "num_train_epochs": 0,
    "num_eval_images": 1,
    "mixed_precision": "bf16",
    "minimum_image_size": 0,
    "max_train_steps": 1000,
    "max_grad_norm": 0.01,
    "lr_warmup_steps": 100,
    "lr_scheduler": "constant_with_warmup",
    "learning_rate": "1e-4",
    "gradient_checkpointing": "true",
    "base_model_precision": "int2-quanto",
    "quantize_via": "cpu",
    "quantize_activations": false,
    "flow_schedule_shift": 1.73,
    "disable_benchmark": false,
    "data_backend_config": "config/qwen_image/multidatabackend.json",
    "checkpoints_total_limit": 5,
    "checkpoint_step_interval": 500,
    "caption_dropout_probability": 0.0,
    "aspect_bucket_rounding": 2
}
```
</details>

> ‚ÑπÔ∏è Usu√°rios multi-GPU podem consultar [este documento](../OPTIONS.md#environment-configuration-variables) para informa√ß√µes sobre como configurar o n√∫mero de GPUs a usar.

> ‚ö†Ô∏è **Cr√≠tico para GPUs 24GB**: O encoder de texto sozinho usa ~16GB de VRAM. Com quantiza√ß√£o `int2-quanto` ou `nf4-bnb`, isso pode ser reduzido significativamente.

Para um sanity check r√°pido com uma configura√ß√£o conhecida:

**Op√ß√£o 1 (Recomendado - pip install):**
```bash
pip install 'simpletuner[cuda]'

# CUDA 13 / Blackwell users (NVIDIA B-series GPUs)
pip install 'simpletuner[cuda13]'
simpletuner train example=qwen_image.peft-lora
```

**Op√ß√£o 2 (M√©todo Git clone):**
```bash
simpletuner train env=examples/qwen_image.peft-lora
```

**Op√ß√£o 3 (M√©todo legado - ainda funciona):**
```bash
ENV=examples/qwen_image.peft-lora ./train.sh
```

### Recursos experimentais avan√ßados

<details>
<summary>Mostrar detalhes experimentais avan√ßados</summary>


SimpleTuner inclui recursos experimentais que podem melhorar significativamente a estabilidade e o desempenho do treinamento.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduz vi√©s de exposi√ß√£o e melhora a qualidade ao permitir que o modelo gere suas pr√≥prias entradas durante o treinamento.

> ‚ö†Ô∏è Esses recursos aumentam o overhead computacional do treinamento.

#### Prompts de valida√ß√£o

Dentro de `config/config.json` est√° o "prompt de valida√ß√£o principal", que normalmente √© o instance_prompt principal que voc√™ est√° treinando para seu √∫nico sujeito ou estilo. Al√©m disso, um arquivo JSON pode ser criado contendo prompts extras para rodar durante valida√ß√µes.

O arquivo de exemplo `config/user_prompt_library.json.example` cont√©m o seguinte formato:

```json
{
  "nickname": "the prompt goes here",
  "another_nickname": "another prompt goes here"
}
```

Os nicknames s√£o o nome do arquivo para a valida√ß√£o, ent√£o mantenha-os curtos e compat√≠veis com seu sistema de arquivos.

Para apontar o trainer para essa biblioteca de prompts, adicione ao seu config.json:
```json
  "validation_prompt_library": "config/user_prompt_library.json",
```

Um conjunto de prompts diversos ajudar√° a determinar se o modelo est√° aprendendo corretamente:

```json
{
    "anime_style": "a breathtaking anime-style portrait with vibrant colors and expressive features",
    "chef_cooking": "a high-quality, detailed photograph of a sous-chef immersed in culinary creation",
    "portrait": "a lifelike and intimate portrait showcasing unique personality and charm",
    "cinematic": "a cinematic, visually stunning photo with dramatic and captivating presence",
    "elegant": "an elegant and timeless portrait exuding grace and sophistication",
    "adventurous": "a dynamic and adventurous photo captured in an exciting moment",
    "mysterious": "a mysterious and enigmatic portrait shrouded in shadows and intrigue",
    "vintage": "a vintage-style portrait evoking the charm and nostalgia of a bygone era",
    "artistic": "an artistic and abstract representation blending creativity with visual storytelling",
    "futuristic": "a futuristic and cutting-edge portrayal set against advanced technology"
}
```

#### Rastreamento de score CLIP

Se voc√™ quiser habilitar avalia√ß√µes para pontuar o desempenho do modelo, veja [este documento](../evaluation/CLIP_SCORES.md) para informa√ß√µes sobre como configurar e interpretar scores CLIP.

#### Perda de avalia√ß√£o est√°vel

Se voc√™ quiser usar perda MSE est√°vel para pontuar o desempenho do modelo, veja [este documento](../evaluation/EVAL_LOSS.md) para informa√ß√µes sobre como configurar e interpretar avalia√ß√£o de loss.

#### Pr√©vias de valida√ß√£o

SimpleTuner suporta streaming de pr√©vias intermedi√°rias de valida√ß√£o durante a gera√ß√£o usando modelos Tiny AutoEncoder. Isso permite ver imagens de valida√ß√£o sendo geradas passo a passo em tempo real via callbacks de webhook.

Para habilitar:
```json
{
  "validation_preview": true,
  "validation_preview_steps": 1
}
```

**Requisitos:**
- Configura√ß√£o de webhook
- Valida√ß√£o habilitada

Defina `validation_preview_steps` para um valor maior (por exemplo, 3 ou 5) para reduzir o overhead do Tiny AutoEncoder. Com `validation_num_inference_steps=20` e `validation_preview_steps=5`, voc√™ receber√° imagens de pr√©via nos steps 5, 10, 15 e 20.

#### Ajuste de schedule de flow

Qwen Image, como modelo de flow-matching, suporta shift do schedule de timesteps para controlar quais partes do processo de gera√ß√£o s√£o treinadas.

O par√¢metro `flow_schedule_shift` controla isso:
- Valores baixos (0.1-1.0): foco em detalhes finos
- Valores m√©dios (1.0-3.0): treino equilibrado (recomendado)
- Valores altos (3.0-6.0): foco em grandes aspectos composicionais

##### Auto-shift
Voc√™ pode habilitar o shift dependente de resolu√ß√£o com `--flow_schedule_auto_shift`, que usa valores de shift maiores para imagens maiores e menores para imagens menores. Isso pode dar resultados est√°veis, mas potencialmente medianos.

##### Especifica√ß√£o manual
Um valor de `--flow_schedule_shift` de 1.73 √© recomendado como ponto de partida para Qwen Image, embora voc√™ possa precisar experimentar baseado no seu dataset e objetivos.

#### Considera√ß√µes sobre o dataset

√â crucial ter um dataset substancial para treinar seu modelo. Existem limita√ß√µes no tamanho do dataset, e voc√™ precisa garantir que seu dataset seja grande o suficiente para treinar de forma eficaz.

> ‚ÑπÔ∏è Com poucas imagens, voc√™ pode ver a mensagem **no images detected in dataset** - aumentar o valor de `repeats` vai superar essa limita√ß√£o.

> ‚ö†Ô∏è **Importante**: Devido √†s limita√ß√µes atuais, mantenha `train_batch_size` em 1 e use `gradient_accumulation_steps` para simular batch sizes maiores.

Crie um documento `--data_backend_config` (`config/multidatabackend.json`) contendo isto:

```json
[
  {
    "id": "pseudo-camera-10k-qwen",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "center",
    "resolution": 1024,
    "minimum_image_size": 512,
    "maximum_image_size": 1024,
    "target_downsample_size": 1024,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/qwen_image/pseudo-camera-10k",
    "instance_data_dir": "datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery",
    "repeats": 0,
    "is_regularisation_data": true
  },
  {
    "id": "dreambooth-subject",
    "type": "local",
    "crop": false,
    "resolution": 1024,
    "minimum_image_size": 512,
    "maximum_image_size": 1024,
    "target_downsample_size": 1024,
    "resolution_type": "pixel_area",
    "cache_dir_vae": "cache/vae/qwen_image/dreambooth-subject",
    "instance_data_dir": "datasets/dreambooth-subject",
    "caption_strategy": "instanceprompt",
    "instance_prompt": "the name of your subject goes here",
    "metadata_backend": "discovery",
    "repeats": 1000
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/qwen_image",
    "disabled": false,
    "write_batch_size": 16
  }
]
```

> ‚ÑπÔ∏è Use `caption_strategy=textfile` se voc√™ tiver arquivos `.txt` contendo legendas.
> Veja op√ß√µes e requisitos de caption_strategy em [DATALOADER.md](../DATALOADER.md#caption_strategy).
> ‚ÑπÔ∏è Observe o `write_batch_size` reduzido para text embeds para evitar OOM.

Depois, crie um diret√≥rio `datasets`:

```bash
mkdir -p datasets
pushd datasets
    huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=pseudo-camera-10k
    mkdir dreambooth-subject
    # place your images into dreambooth-subject/ now
popd
```

Isso vai baixar cerca de 10k amostras de fotografias para o diret√≥rio `datasets/pseudo-camera-10k`, que ser√° criado automaticamente.

Suas imagens de Dreambooth devem ir para o diret√≥rio `datasets/dreambooth-subject`.

#### Login no WandB e Huggingface Hub

Voc√™ vai querer fazer login no WandB e no HF Hub antes de iniciar o treinamento, especialmente se estiver usando `--push_to_hub` e `--report_to=wandb`.

Se voc√™ pretende enviar itens para um reposit√≥rio Git LFS manualmente, tamb√©m deve executar `git config --global credential.helper store`.

Execute os seguintes comandos:

```bash
wandb login
```

e

```bash
huggingface-cli login
```

Siga as instru√ß√µes para fazer login em ambos os servi√ßos.

</details>

### Executando o treinamento

A partir do diret√≥rio do SimpleTuner, basta executar:

```bash
./train.sh
```

Isso vai iniciar o cache em disco das embeddings de texto e sa√≠das do VAE.

Para mais informa√ß√µes, veja os documentos do [dataloader](../DATALOADER.md) e do [tutorial](../TUTORIAL.md).

### Dicas de otimiza√ß√£o de mem√≥ria

#### Configura√ß√£o de menor VRAM (m√≠nimo 24GB)

A configura√ß√£o de menor VRAM do Qwen Image exige cerca de 24GB:

- SO: Ubuntu Linux 24
- GPU: um √∫nico dispositivo NVIDIA CUDA (m√≠nimo 24GB)
- Mem√≥ria do sistema: 64GB+ recomendado
- Precis√£o do modelo base:
  - Para sistemas NVIDIA: `int2-quanto` ou `nf4-bnb` (obrigat√≥rio para placas de 24GB)
  - `int4-quanto` pode funcionar, mas pode ter qualidade menor
- Otimizador: `optimi-lion` ou `bnb-lion8bit-paged` para efici√™ncia de mem√≥ria
- Resolu√ß√£o: comece com 512px ou 768px, suba para 1024px se a mem√≥ria permitir
- Batch size: 1 (obrigat√≥rio devido √†s limita√ß√µes atuais)
- Gradient accumulation steps: 2-8 para simular batches maiores
- Habilite `--gradient_checkpointing` (obrigat√≥rio)
- Use `--quantize_via=cpu` para evitar OOM na inicializa√ß√£o
- Use um rank LoRA pequeno (1-8)
- Definir a vari√°vel de ambiente `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` ajuda a minimizar uso de VRAM

**NOTA**: O pr√©-cache de embeddings do VAE e sa√≠das do encoder de texto vai usar mem√≥ria significativa. Habilite `offload_during_startup=true` se voc√™ encontrar OOM.

### Rodando infer√™ncia no LoRA depois

Como o Qwen Image √© um modelo mais novo, aqui est√° um exemplo funcional de infer√™ncia:

<details>
<summary>Mostrar exemplo de infer√™ncia em Python</summary>

```python
import torch
from diffusers import QwenImagePipeline, QwenImageTransformer2DModel
from transformers import Qwen2Tokenizer, Qwen2_5_VLForConditionalGeneration

model_id = 'Qwen/Qwen-Image'
adapter_id = 'your-username/your-lora-name'

# Load the pipeline
pipeline = QwenImagePipeline.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16
)

# Load LoRA weights
pipeline.load_lora_weights(adapter_id)

# Optional: quantize the model to save VRAM
from optimum.quanto import quantize, freeze, qint8
quantize(pipeline.transformer, weights=qint8)
freeze(pipeline.transformer)

# Move to device
pipeline.to('cuda' if torch.cuda.is_available() else 'cpu')

# Generate an image
prompt = "Your test prompt here"
negative_prompt = 'ugly, cropped, blurry, low-quality, mediocre average'

image = pipeline(
    prompt=prompt,
    negative_prompt=negative_prompt,
    num_inference_steps=30,
    guidance_scale=4.0,
    generator=torch.Generator(device='cuda').manual_seed(42),
    width=1024,
    height=1024,
).images[0]

image.save("output.png", format="PNG")
```
</details>

### Notas e dicas de troubleshooting

#### Limita√ß√µes de batch size

Atualmente, o Qwen Image tem problemas com batch sizes > 1 devido ao tratamento de comprimento de sequ√™ncia no encoder de texto. Sempre use:
- `train_batch_size: 1`
- `gradient_accumulation_steps: 2-8` para simular batches maiores

#### Quantiza√ß√£o

- `int2-quanto` oferece a economia de mem√≥ria mais agressiva, mas pode impactar a qualidade
- `nf4-bnb` oferece bom equil√≠brio entre mem√≥ria e qualidade
- `int4-quanto` √© uma op√ß√£o intermedi√°ria
- Evite `int8` a menos que voc√™ tenha 40GB+ de VRAM

#### Taxas de aprendizado

Para treino de LoRA:
- LoRAs pequenas (rank 1-8): use learning rates em torno de 1e-4
- LoRAs maiores (rank 16-32): use learning rates em torno de 5e-5
- Com otimizador Prodigy: comece em 1.0 e deixe adaptar

#### Artefatos de imagem

Se voc√™ encontrar artefatos:
- Reduza sua taxa de aprendizado
- Aumente gradient accumulation steps
- Garanta que suas imagens s√£o de alta qualidade e bem pr√©-processadas
- Considere usar resolu√ß√µes menores inicialmente

#### Treino com m√∫ltiplas resolu√ß√µes

Comece o treino em resolu√ß√µes menores (512px ou 768px) para acelerar o aprendizado inicial, depois fa√ßa fine-tune em 1024px. Habilite `--flow_schedule_auto_shift` ao treinar em diferentes resolu√ß√µes.

### Limita√ß√µes de plataforma

**N√£o suportado em:**
- AMD ROCm (falta implementa√ß√£o eficiente de flash attention)
- Apple Silicon/MacOS (limita√ß√µes de mem√≥ria e aten√ß√£o)
- GPUs de consumidor com menos de 24GB de VRAM

### Problemas conhecidos atuais

1. Batch size > 1 n√£o funciona corretamente (use gradient accumulation)
2. TREAD ainda n√£o √© suportado
3. Alto uso de mem√≥ria do encoder de texto (~16GB antes da quantiza√ß√£o)
4. Problemas de manuseio de comprimento de sequ√™ncia ([issue upstream](https://github.com/huggingface/diffusers/issues/12075))

Para ajuda adicional e troubleshooting, consulte a [documenta√ß√£o do SimpleTuner](/documentation) ou entre no Discord da comunidade.
