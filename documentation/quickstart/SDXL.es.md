## Inicio r치pido de Stable Diffusion XL

En este ejemplo, entrenaremos un modelo Stable Diffusion XL usando el toolkit SimpleTuner y usaremos el tipo de modelo `lora`.

Comparado con modelos modernos y m치s grandes, SDXL es bastante modesto en tama침o, as칤 que puede ser posible usar entrenamiento `full`, pero eso requerir치 m치s VRAM que el entrenamiento LoRA y otros ajustes de hiperpar치metros.

### Requisitos previos

Aseg칰rate de tener Python instalado; SimpleTuner funciona bien con 3.10 a 3.12 (m치quinas AMD ROCm requieren 3.12).

Puedes comprobarlo ejecutando:

```bash
python --version
```

Si no tienes Python 3.12 instalado en Ubuntu, puedes intentar lo siguiente:

```bash
apt -y install python3.13 python3.13-venv
```

#### Dependencias de imagen de contenedor

Para Vast, RunPod y TensorDock (entre otros), lo siguiente funcionar치 en una imagen CUDA 12.2-12.8 para permitir compilar extensiones CUDA:

```bash
apt -y install nvidia-cuda-toolkit
```

### Instalaci칩n

Instala SimpleTuner v칤a pip:

```bash
pip install 'simpletuner[cuda]'

# CUDA 13 / Blackwell users (NVIDIA B-series GPUs)
pip install 'simpletuner[cuda13]'
```

Para instalaci칩n manual o setup de desarrollo, consulta la [documentaci칩n de instalaci칩n](../INSTALL.md).

### Configuraci칩n del entorno

Para ejecutar SimpleTuner, necesitar치s configurar un archivo de configuraci칩n, los directorios de dataset y modelo, y un archivo de configuraci칩n del dataloader.

#### Archivo de configuraci칩n

Un script experimental, `configure.py`, puede permitirte omitir por completo esta secci칩n mediante una configuraci칩n interactiva paso a paso. Incluye funciones de seguridad que ayudan a evitar errores comunes.

**Nota:** Esto **no** configura completamente tu dataloader. Todav칤a tendr치s que hacerlo manualmente m치s adelante.

Para ejecutarlo:

```bash
simpletuner configure
```
> 丘멆잺 Para usuarios ubicados en pa칤ses donde Hugging Face Hub no es f치cilmente accesible, debes a침adir `HF_ENDPOINT=https://hf-mirror.com` a tu `~/.bashrc` o `~/.zshrc` dependiendo de qu칠 `$SHELL` use tu sistema.

Si prefieres configurar manualmente:

Copia `config/config.json.example` a `config/config.json`:

```bash
cp config/config.json.example config/config.json
```

#### Pasos adicionales para AMD ROCm

Lo siguiente debe ejecutarse para que un AMD MI300X sea utilizable:

```bash
apt install amd-smi-lib
pushd /opt/rocm/share/amd_smi
python3 -m pip install --upgrade pip
python3 -m pip install .
popd
```

Ah칤, necesitar치s modificar las siguientes variables:

<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "model_type": "lora",
  "model_family": "sdxl",
  "model_flavour": "base-1.0",
  "output_dir": "/home/user/output/models",
  "validation_resolution": "1024x1024,1280x768",
  "validation_guidance": 3.4,
  "use_gradient_checkpointing": true,
  "learning_rate": 1e-4
}
```
</details>

- `model_family` - Configura esto en `sdxl`.
- `model_flavour` - Configura esto en `base-1.0`, o usa `pretrained_model_name_or_path` para apuntar a un modelo diferente.
- `model_type` - Configura esto en `lora`.
- `use_dora` - Configura esto en `true` si quieres entrenar DoRA.
- `output_dir` - Configura esto al directorio donde quieras guardar tus checkpoints e im치genes de validaci칩n. Se recomienda usar una ruta completa.
- `validation_resolution` - Configura esto en `1024x1024` para este ejemplo.
  - Adem치s, Stable Diffusion XL fue afinado con buckets multi-aspect, y se pueden especificar otras resoluciones separ치ndolas con comas: `1024x1024,1280x768`
- `validation_guidance` - Usa el valor con el que est칠s c칩modo para pruebas en inferencia. Configura entre `4.2` y `6.4`.
- `use_gradient_checkpointing` - Probablemente esto deber칤a ser `true` a menos que tengas MUCH칈SIMA VRAM y quieras sacrificar algo para hacerlo m치s r치pido.
- `learning_rate` - `1e-4` es bastante com칰n para redes de bajo rango, aunque `1e-5` puede ser una opci칩n m치s conservadora si notas "burning" o sobreentrenamiento temprano.

Hay algunas m치s si usas una m치quina Mac M-series:

- `mixed_precision` deber칤a configurarse en `no`.
  - Esto sol칤a ser cierto en pytorch 2.4, pero quiz치 bf16 pueda usarse ahora a partir de 2.6+
- `attention_mechanism` podr칤a configurarse en `xformers` para aprovecharlo, pero est치 algo obsoleto.

#### Entrenamiento de modelos cuantizados

Probado en sistemas Apple y NVIDIA, Hugging Face Optimum-Quanto puede usarse para reducir la precisi칩n y los requisitos de VRAM del Unet, pero no funciona tan bien como en modelos Diffusion Transformer como SD3/Flux, por lo que no se recomienda.

Si est치s con restricciones de recursos, a칰n puedes usarlo.

Para `config.json`:
<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "base_model_precision": "int8-quanto",
  "text_encoder_1_precision": "no_change",
  "text_encoder_2_precision": "no_change",
  "optimizer": "optimi-lion"
}
```
</details>

#### Funciones experimentales avanzadas

<details>
<summary>Mostrar detalles experimentales avanzados</summary>


SimpleTuner incluye funciones experimentales que pueden mejorar significativamente la estabilidad y el rendimiento del entrenamiento, particularmente para datasets peque침os o arquitecturas antiguas como SDXL.

*   **[Scheduled Sampling (Rollout)](../experimental/SCHEDULED_SAMPLING.md):** reduce el sesgo de exposici칩n y mejora la calidad de salida dejando que el modelo genere sus propias entradas durante el entrenamiento.
*   **[Diff2Flow](../experimental/DIFF2FLOW.md):** permite entrenar SDXL con un objetivo Flow Matching, potencialmente mejorando la rectitud y la calidad de generaci칩n.

> 丘멆잺 Estas funciones aumentan la sobrecarga computacional del entrenamiento.

</details>

#### Consideraciones de dataset

Es crucial contar con un dataset sustancial para entrenar tu modelo. Hay limitaciones en el tama침o del dataset y debes asegurarte de que sea lo suficientemente grande para entrenar el modelo de manera efectiva. Ten en cuenta que el tama침o m칤nimo del dataset es `TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS`. El dataset no ser치 detectable por el trainer si es demasiado peque침o.

> 游눠 **Consejo:** Para datasets grandes donde el espacio en disco es una preocupaci칩n, puedes usar `--vae_cache_disable` para realizar la codificaci칩n VAE en l칤nea sin cachear los resultados a disco. Esto se habilita impl칤citamente si usas `--vae_cache_ondemand`, pero a침adir `--vae_cache_disable` asegura que no se escriba nada a disco.

Dependiendo del dataset que tengas, necesitar치s configurar el directorio de dataset y el archivo de configuraci칩n del dataloader de forma diferente. En este ejemplo, usaremos [pseudo-camera-10k](https://huggingface.co/datasets/bghira/pseudo-camera-10k) como dataset.

En tu directorio `OUTPUT_DIR`, crea un multidatabackend.json:

<details>
<summary>Ver ejemplo de config</summary>

```json
[
  {
    "id": "pseudo-camera-10k-sdxl",
    "type": "local",
    "crop": true,
    "crop_aspect": "square",
    "crop_style": "random",
    "resolution": 1.0,
    "minimum_image_size": 0.25,
    "maximum_image_size": 1.0,
    "target_downsample_size": 1.0,
    "resolution_type": "area",
    "cache_dir_vae": "cache/vae/sdxl/pseudo-camera-10k",
    "instance_data_dir": "/home/user/simpletuner/datasets/pseudo-camera-10k",
    "disabled": false,
    "skip_file_discovery": "",
    "caption_strategy": "filename",
    "metadata_backend": "discovery"
  },
  {
    "id": "text-embeds",
    "type": "local",
    "dataset_type": "text_embeds",
    "default": true,
    "cache_dir": "cache/text/sdxl/pseudo-camera-10k",
    "disabled": false,
    "write_batch_size": 128
  }
]
```
</details>

> Consulta las opciones y requisitos de caption_strategy en [DATALOADER.md](../DATALOADER.md#caption_strategy).

Luego, crea un directorio `datasets`:

```bash
mkdir -p datasets
huggingface-cli download --repo-type=dataset bghira/pseudo-camera-10k --local-dir=datasets/pseudo-camera-10k
```

Esto descargar치 alrededor de 10k muestras de fotograf칤as a tu directorio `datasets/pseudo-camera-10k`, que se crear치 autom치ticamente.

#### Inicia sesi칩n en WandB y Huggingface Hub

Querr치s iniciar sesi칩n en WandB y en HF Hub antes de comenzar el entrenamiento, especialmente si est치s usando `push_to_hub: true` y `--report_to=wandb`.

Si vas a subir elementos manualmente a un repositorio Git LFS, tambi칠n deber칤as ejecutar `git config --global credential.helper store`

Ejecuta los siguientes comandos:

```bash
wandb login
```

y

```bash
huggingface-cli login
```

Sigue las instrucciones para iniciar sesi칩n en ambos servicios.

### Ejecutar el entrenamiento

Desde el directorio SimpleTuner, solo tienes que ejecutar:

```bash
bash train.sh
```

Esto iniciar치 el cach칠 en disco de los embeddings de texto y las salidas VAE.

Para m치s informaci칩n, consulta los documentos [dataloader](../DATALOADER.md) y [tutorial](../TUTORIAL.md).

### Seguimiento de puntuaci칩n CLIP

Si deseas habilitar evaluaciones para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/CLIP_SCORES.md) para informaci칩n sobre c칩mo configurar e interpretar las puntuaciones CLIP.

# P칠rdida de evaluaci칩n estable

Si deseas usar p칠rdida MSE estable para puntuar el rendimiento del modelo, consulta [este documento](../evaluation/EVAL_LOSS.md) para informaci칩n sobre c칩mo configurar e interpretar la p칠rdida de evaluaci칩n.

#### Vistas previas de validaci칩n

SimpleTuner soporta streaming de vistas previas intermedias de validaci칩n durante la generaci칩n usando modelos Tiny AutoEncoder. Esto te permite ver im치genes de validaci칩n gener치ndose paso a paso en tiempo real v칤a callbacks de webhook.

Para habilitarlo:
<details>
<summary>Ver ejemplo de config</summary>

```json
{
  "validation_preview": true,
  "validation_preview_steps": 1
}
```
</details>

**Requisitos:**
- Configuraci칩n de webhook
- Validaci칩n habilitada

Configura `validation_preview_steps` a un valor m치s alto (p. ej., 3 o 5) para reducir el overhead del Tiny AutoEncoder. Con `validation_num_inference_steps=20` y `validation_preview_steps=5`, recibir치s vistas previas en los pasos 5, 10, 15 y 20.
