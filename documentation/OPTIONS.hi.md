# SimpleTuner Training Script ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™

## Overview

‡§Ø‡§π ‡§ó‡§æ‡§á‡§° SimpleTuner ‡§ï‡•á `train.py` ‡§∏‡•ç‡§ï‡•ç‡§∞‡§ø‡§™‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß command‚Äëline ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™‡•ã‡§Ç ‡§ï‡§æ user‚Äëfriendly ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡•á ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§â‡§ö‡•ç‡§ö ‡§∏‡•ç‡§§‡§∞ ‡§ï‡§æ customization ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Ü‡§™ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§Ö‡§™‡§®‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

### JSON Configuration file format

‡§Ö‡§™‡•á‡§ï‡•ç‡§∑‡§ø‡§§ JSON ‡§´‡§º‡§æ‡§á‡§≤‚Äë‡§®‡§æ‡§Æ `config.json` ‡§π‡•à ‡§î‡§∞ key ‡§®‡§æ‡§Æ ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è `--arguments` ‡§ú‡•à‡§∏‡•á ‡§π‡•Ä ‡§π‡•à‡§Ç‡•§ JSON ‡§´‡§º‡§æ‡§á‡§≤ ‡§Æ‡•á‡§Ç ‡§Ö‡§ó‡•ç‡§∞‡§£‡•Ä `--` ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§ö‡§æ‡§π‡•á‡§Ç ‡§§‡•ã ‡§∞‡§ñ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

Ready‚Äëto‚Äërun ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§¢‡•Ç‡§Å‡§¢ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç? [simpletuner/examples/README.md](/simpletuner/examples/README.md) ‡§Æ‡•á‡§Ç curated presets ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

### Easy configure script (***RECOMMENDED***)

`simpletuner configure` ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§∏‡•á `config.json` ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï‡§§‡§∞ ‡§Ü‡§¶‡§∞‡•ç‡§∂ ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡•á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

#### ‡§Æ‡•å‡§ú‡•Ç‡§¶‡§æ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§¨‡§¶‡§≤‡§®‡§æ

`configure` ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§è‡§ï ‡§π‡•Ä argument ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§è‡§ï compatible `config.json`, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§Ü‡§™ training setup ‡§ï‡•ã ‡§á‡§Ç‡§ü‡§∞‡•à‡§ï‡•ç‡§ü‡§ø‡§µ ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§¨‡§¶‡§≤ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç:

```bash
simpletuner configure config/foo/config.json
```

‡§Ø‡§π‡§æ‡§Å `foo` ‡§Ü‡§™‡§ï‡§æ config environment ‡§π‡•à ‚Äî ‡§Ø‡§æ ‡§Ø‡§¶‡§ø ‡§Ü‡§™ config environments ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç, ‡§§‡•ã `config/config.json` ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§

<img width="1484" height="560" alt="image" src="https://github.com/user-attachments/assets/67dec8d8-3e41-42df-96e6-f95892d2814c" />

> ‚ö†Ô∏è ‡§ú‡§ø‡§® ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç Hugging Face Hub ‡§Ü‡§∏‡§æ‡§®‡•Ä ‡§∏‡•á ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§µ‡§π‡§æ‡§Å ‡§ï‡•á ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§ï‡•á `$SHELL` ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ `~/.bashrc` ‡§Ø‡§æ `~/.zshrc` ‡§Æ‡•á‡§Ç `HF_ENDPOINT=https://hf-mirror.com` ‡§ú‡•ã‡§°‡§º‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§

---

## üåü Core Model Configuration

### `--model_type`

- **What**: ‡§Ø‡§π ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à ‡§ï‡§ø LoRA ‡§Ø‡§æ full fine‚Äëtune ‡§¨‡§®‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§è‡§ó‡§æ‡•§
- **Choices**: lora, full.
- **Default**: lora
  - ‡§Ø‡§¶‡§ø lora ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã, ‡§§‡•ã `--lora_type` ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø PEFT ‡§Ø‡§æ LyCORIS ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•à‡•§ ‡§ï‡•Å‡§õ ‡§Æ‡•â‡§°‡§≤ (PixArt) ‡§ï‡•á‡§µ‡§≤ LyCORIS adapters ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--model_family`

- **What**: ‡§Ø‡§π ‡§®‡§ø‡§∞‡•ç‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡•å‡§®‚Äë‡§∏‡§æ model architecture train ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à‡•§
- **Choices**: pixart_sigma, flux, sd3, sdxl, kolors, legacy

### `--lora_format`

- **What**: load/save ‡§ï‡•á ‡§≤‡§ø‡§è LoRA checkpoint key format ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à‡•§
- **Choices**: `diffusers` (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü), `comfyui`
- **Notes**:
  - `diffusers` standard PEFT/Diffusers layout ‡§π‡•à‡•§
  - `comfyui` keys ‡§ï‡•ã ComfyUI‚Äëstyle ‡§Æ‡•á‡§Ç convert ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (`diffusion_model.*` ‡§ï‡•á ‡§∏‡§æ‡§• `lora_A/lora_B` ‡§î‡§∞ `.alpha` tensors)‡•§ Flux, Flux2, Lumina2, ‡§î‡§∞ Z‚ÄëImage ComfyUI inputs ‡§ï‡•ã auto‚Äëdetect ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á ‡§≠‡§≤‡•á ‡§π‡•Ä ‡§Ø‡§π `diffusers` ‡§™‡§∞ ‡§π‡•ã, ‡§≤‡•á‡§ï‡§ø‡§® saving ‡§ï‡•á ‡§≤‡§ø‡§è ComfyUI output force ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `comfyui` ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§

### `--fuse_qkv_projections`

- **What**: ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á attention blocks ‡§Æ‡•á‡§Ç QKV projections ‡§ï‡•ã fuse ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø hardware ‡§ï‡§æ ‡§Ö‡§ß‡§ø‡§ï ‡§ï‡•Å‡§∂‡§≤ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡•§
- **Note**: ‡§ï‡•á‡§µ‡§≤ NVIDIA H100 ‡§Ø‡§æ H200 ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à ‡§ú‡§¨ Flash Attention 3 ‡§Æ‡•à‡§®‡•ç‡§Ø‡•Å‡§Ö‡§≤‡•Ä install ‡§π‡•ã‡•§

### `--offload_during_startup`

- **What**: VAE caching ‡§ö‡§≤‡§®‡•á ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® text encoder weights ‡§ï‡•ã CPU ‡§™‡§∞ offload ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: HiDream ‡§î‡§∞ Wan 2.1 ‡§ú‡•à‡§∏‡•á ‡§¨‡§°‡§º‡•á ‡§Æ‡•â‡§°‡§≤‡•ç‡§∏ ‡§Æ‡•á‡§Ç VAE cache ‡§≤‡•ã‡§° ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø OOM ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ training quality ‡§ï‡•ã ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ, ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡•á text encoders ‡§Ø‡§æ ‡§ß‡•Ä‡§Æ‡•á CPUs ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§ï‡§à datasets ‡§™‡§∞ startup time ‡§ï‡§æ‡§´‡§º‡•Ä ‡§¨‡§¢‡§º ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡•Ä ‡§ï‡§æ‡§∞‡§£ ‡§Ø‡§π ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á disabled ‡§π‡•à‡•§
- **Tip**: ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á memory‚Äëconstrained systems ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡•Ä‡§ö‡•á ‡§¶‡§ø‡§è group offloading ‡§´‡•Ä‡§ö‡§∞ ‡§ï‡•á ‡§∏‡§æ‡§• ‡§™‡•Ç‡§∞‡§ï ‡§π‡•à‡•§

### `--offload_during_save`

- **What**: `save_hooks.py` checkpoints ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§™‡•Ç‡§∞‡•á pipeline ‡§ï‡•ã ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä ‡§∞‡•Ç‡§™ ‡§∏‡•á CPU ‡§™‡§∞ ‡§≤‡•á ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§∏‡§≠‡•Ä FP8/quantized weights device ‡§∏‡•á ‡§¨‡§æ‡§π‡§∞ ‡§≤‡§ø‡§ñ‡•á ‡§ú‡§æ‡§è‡§Å‡•§
- **Why**: fp8‚Äëquanto weights ‡§∏‡•á‡§µ ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ VRAM ‡§â‡§™‡§Ø‡•ã‡§ó ‡§Ö‡§ö‡§æ‡§®‡§ï ‡§¨‡§¢‡§º ‡§∏‡§ï‡§§‡§æ ‡§π‡•à (‡§â‡§¶‡§æ., `state_dict()` serialization ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§®)‡•§ ‡§Ø‡§π ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ training ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã accelerator ‡§™‡§∞ ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® save trigger ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§•‡•ã‡§°‡§º‡•Ä ‡§¶‡•á‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è offload ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø CUDA OOM ‡§∏‡•á ‡§¨‡§ö‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡•§
- **Tip**: ‡§ï‡•á‡§µ‡§≤ ‡§§‡§¨ ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç ‡§ú‡§¨ saving OOM errors ‡§¶‡•á ‡§∞‡§π‡•Ä ‡§π‡•ã; loader ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§æ‡§™‡§∏ ‡§≤‡•á ‡§Ü‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø training seamless ‡§∞‡§π‡•á‡•§

### `--delete_model_after_load`

- **What**: ‡§Æ‡•â‡§°‡§≤ files ‡§ï‡•ã HuggingFace cache ‡§∏‡•á delete ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ ‡§µ‡•á memory ‡§Æ‡•á‡§Ç load ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Why**: ‡§â‡§® ‡§∏‡•á‡§ü‡§Ö‡§™‡•ç‡§∏ ‡§Æ‡•á‡§Ç disk usage ‡§ò‡§ü‡§æ‡§§‡§æ ‡§π‡•à ‡§ú‡§π‡§æ‡§Å GB ‡§ï‡•á ‡§π‡§ø‡§∏‡§æ‡§¨ ‡§∏‡•á billing ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§Æ‡•â‡§°‡§≤ VRAM/RAM ‡§Æ‡•á‡§Ç ‡§≤‡•ã‡§° ‡§π‡•ã ‡§ú‡§æ‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ on‚Äëdisk cache ‡§Ö‡§ó‡§≤‡•Ä run ‡§§‡§ï ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§∞‡§π‡§§‡§æ‡•§ ‡§Ø‡§π ‡§≠‡§æ‡§∞ storage ‡§∏‡•á network bandwidth ‡§™‡§∞ ‡§∂‡§ø‡§´‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§Ö‡§ó‡§≤‡•Ä ‡§∞‡§® ‡§Æ‡•á‡§Ç‡•§
- **Notes**:
  - ‡§Ø‡§¶‡§ø validation ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•à ‡§§‡•ã VAE **delete ‡§®‡§π‡•Ä‡§Ç** ‡§π‡•ã‡§ó‡§æ, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø validation images ‡§¨‡§®‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏‡§ï‡•Ä ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§π‡•à‡•§
  - Text encoders ‡§ï‡•ã data backend factory ‡§ï‡•á startup ‡§™‡•Ç‡§∞‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ delete ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à (embed caching ‡§ï‡•á ‡§¨‡§æ‡§¶)‡•§
  - Transformer/UNet ‡§Æ‡•â‡§°‡§≤‡•ç‡§∏ load ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§¨‡§æ‡§¶ delete ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§
  - Multi‚Äënode setups ‡§Æ‡•á‡§Ç, ‡§π‡§∞ node ‡§™‡§∞ ‡§ï‡•á‡§µ‡§≤ local‚Äërank 0 deletion ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ Shared network storage ‡§™‡§∞ race conditions ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è deletion failures silently ignore ‡§ï‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§
  - ‡§Ø‡§π saved training checkpoints ‡§™‡§∞ **‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§®‡§π‡•Ä‡§Ç** ‡§°‡§æ‡§≤‡§§‡§æ ‚Äî ‡§ï‡•á‡§µ‡§≤ pre‚Äëtrained base model cache ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§

### `--enable_group_offload`

- **What**: diffusers ‡§ï‡•Ä grouped module offloading ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø forward passes ‡§ï‡•á ‡§¨‡•Ä‡§ö model blocks ‡§ï‡•ã CPU (‡§Ø‡§æ disk) ‡§™‡§∞ stage ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡•§
- **Why**: ‡§¨‡§°‡§º‡•á transformers (Flux, Wan, Auraflow, LTXVideo, Cosmos2Image) ‡§™‡§∞ peak VRAM usage ‡§ï‡•ã ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ñ‡§æ‡§∏‡§ï‡§∞ CUDA streams ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§î‡§∞ performance ‡§™‡§∞ ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§™‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§
- **Notes**:
  - `--enable_model_cpu_offload` ‡§ï‡•á ‡§∏‡§æ‡§• mutually exclusive ‚Äî ‡§™‡•ç‡§∞‡§§‡§ø run ‡§è‡§ï ‡§π‡•Ä strategy ‡§ö‡•Å‡§®‡•á‡§Ç‡•§
  - diffusers **v0.33.0** ‡§Ø‡§æ ‡§®‡§Ø‡§æ required ‡§π‡•à‡•§

### `--group_offload_type`

- **Choices**: `block_level` (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü), `leaf_level`
- **What**: layers ‡§ï‡•ã ‡§ï‡•à‡§∏‡•á group ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§è ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ `block_level` VRAM ‡§¨‡§ö‡§§ ‡§î‡§∞ throughput ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à, ‡§ú‡§¨‡§ï‡§ø `leaf_level` ‡§Ö‡§ß‡§ø‡§ï CPU transfers ‡§ï‡•Ä ‡§ï‡•Ä‡§Æ‡§§ ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§¨‡§ö‡§§ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--group_offload_blocks_per_group`

- **What**: `block_level` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø, ‡§è‡§ï offload group ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§§‡§®‡•á transformer blocks bundle ‡§ï‡§ø‡§è ‡§ú‡§æ‡§è‡§Å‡•§
- **Default**: `1`
- **Why**: ‡§á‡§∏ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§ï‡•ã ‡§¨‡§¢‡§º‡§æ‡§®‡•á ‡§∏‡•á transfer frequency ‡§ï‡§Æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à (‡§§‡•á‡§ú‡§º), ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ö‡§ß‡§ø‡§ï parameters accelerator ‡§™‡§∞ resident ‡§∞‡§π‡§§‡•á ‡§π‡•à‡§Ç (‡§Ö‡§ß‡§ø‡§ï VRAM)‡•§

### `--group_offload_use_stream`

- **What**: host/device transfers ‡§ï‡•ã compute ‡§ï‡•á ‡§∏‡§æ‡§• overlap ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è dedicated CUDA stream ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `False`
- **Notes**:
  - non‚ÄëCUDA backends (Apple MPS, ROCm, CPU) ‡§™‡§∞ ‡§∏‡•ç‡§µ‡§§‡§É CPU‚Äëstyle transfers ‡§™‡§∞ fallback ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - NVIDIA GPUs ‡§™‡§∞ training ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø, ‡§î‡§∞ copy engine capacity spare ‡§π‡•ã, ‡§§‡•ã ‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§ø‡§§‡•§

### `--group_offload_to_disk_path`

- **What**: directory path ‡§ú‡§π‡§æ‡§Å grouped parameters ‡§ï‡•ã RAM ‡§ï‡•Ä ‡§¨‡§ú‡§æ‡§Ø disk ‡§™‡§∞ spill ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§è‡§ó‡§æ‡•§
- **Why**: ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ tight CPU RAM ‡§¨‡§ú‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä (‡§ú‡•à‡§∏‡•á ‡§¨‡§°‡§º‡•á NVMe drive ‡§µ‡§æ‡§≤‡§æ workstation)‡•§
- **Tip**: ‡§§‡•á‡§ú‡§º local SSD ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç; network filesystems training ‡§ï‡•ã ‡§ï‡§æ‡§´‡•Ä ‡§ß‡•Ä‡§Æ‡§æ ‡§ï‡§∞ ‡§¶‡•á‡§Ç‡§ó‡•á‡•§

### `--musubi_blocks_to_swap`

- **What**: LongCat‚ÄëVideo, Wan, LTXVideo, Kandinsky5‚ÄëVideo, Qwen‚ÄëImage, Flux, Flux.2, Cosmos2Image, ‡§î‡§∞ HunyuanVideo ‡§ï‡•á ‡§≤‡§ø‡§è Musubi block swap ‚Äî ‡§Ü‡§ñ‡§º‡§ø‡§∞‡•Ä N transformer blocks ‡§ï‡•ã CPU ‡§™‡§∞ ‡§∞‡§ñ‡•á‡§Ç ‡§î‡§∞ forward ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§™‡•ç‡§∞‡§§‡§ø block weights stream ‡§ï‡§∞‡•á‡§Ç‡•§
- **Default**: `0` (disabled)
- **Notes**: Musubi‚Äëstyle weight offload; throughput ‡§≤‡§æ‡§ó‡§§ ‡§™‡§∞ VRAM ‡§ï‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ gradients ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ skip ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§

### `--musubi_block_swap_device`

- **What**: swapped transformer blocks ‡§ï‡•ã ‡§∏‡•ç‡§ü‡•ã‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è device string (‡§â‡§¶‡§æ. `cpu`, `cuda:0`)‡•§
- **Default**: `cpu`
- **Notes**: ‡§ï‡•á‡§µ‡§≤ ‡§§‡§¨ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ `--musubi_blocks_to_swap` > 0 ‡§π‡•ã‡•§

### `--ramtorch`

- **What**: `nn.Linear` layers ‡§ï‡•ã RamTorch CPU‚Äëstreamed implementations ‡§∏‡•á replace ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: Linear weights ‡§ï‡•ã CPU memory ‡§Æ‡•á‡§Ç ‡§∏‡§æ‡§ù‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§â‡§®‡•ç‡§π‡•á‡§Ç accelerator ‡§™‡§∞ stream ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø VRAM pressure ‡§ï‡§Æ ‡§π‡•ã‡•§
- **Notes**:
  - CUDA ‡§Ø‡§æ ROCm ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à (Apple/MPS ‡§™‡§∞ ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç)‡•§
  - `--enable_group_offload` ‡§ï‡•á ‡§∏‡§æ‡§• mutually exclusive‡•§
  - `--set_grads_to_none` ‡§∏‡•ç‡§µ‡§§‡§É ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--ramtorch_target_modules`

- **What**: Comma‚Äëseparated glob patterns ‡§ú‡•ã ‡§Ø‡§π ‡§∏‡•Ä‡§Æ‡§ø‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡§ø ‡§ï‡•å‡§®‚Äë‡§∏‡•á Linear modules ‡§ï‡•ã RamTorch ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§æ ‡§ú‡§æ‡§è‡•§
- **Default**: ‡§Ø‡§¶‡§ø ‡§ï‡•ã‡§à pattern ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ, ‡§§‡•ã ‡§∏‡§≠‡•Ä Linear layers convert ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç‡•§
- **Notes**:
  - `fnmatch` glob syntax ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á fully qualified module names ‡§Ø‡§æ class names ‡§ï‡•ã match ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - ‡§ï‡§ø‡§∏‡•Ä block ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§ï‡•Ä layers ‡§ï‡•ã match ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è patterns ‡§Æ‡•á‡§Ç trailing `.*` wildcard ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è, `transformer_blocks.0.*` block 0 ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§∏‡§≠‡•Ä layers ‡§ï‡•ã match ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§¨‡§ï‡§ø `transformer_blocks.*` ‡§∏‡§≠‡•Ä transformer blocks ‡§ï‡•ã match ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ `transformer_blocks.0` ‡§ú‡•à‡§∏‡§æ bare name ‡§¨‡§ø‡§®‡§æ `.*` ‡§ï‡•á ‡§≠‡•Ä ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡•á‡§ó‡§æ (‡§Ø‡§π automatically expand ‡§π‡•ã‡§§‡§æ ‡§π‡•à), ‡§≤‡•á‡§ï‡§ø‡§® clarity ‡§ï‡•á ‡§≤‡§ø‡§è explicit wildcard form recommended ‡§π‡•à‡•§
  - ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `"transformer_blocks.*,single_transformer_blocks.0.*,single_transformer_blocks.1.*"`

### `--ramtorch_text_encoder`

- **What**: ‡§∏‡§≠‡•Ä text encoder Linear layers ‡§™‡§∞ RamTorch replacements ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `False`

### `--ramtorch_vae`

- **What**: VAE mid‚Äëblock Linear layers ‡§ï‡•á ‡§≤‡§ø‡§è experimental RamTorch conversion‡•§
- **Default**: `False`
- **Notes**: VAE up/down convolutional blocks unchanged ‡§∞‡§π‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--ramtorch_controlnet`

- **What**: ControlNet training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ControlNet Linear layers ‡§™‡§∞ RamTorch replacements ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `False`

### `--ramtorch_transformer_percent`

- **What**: RamTorch ‡§ï‡•á ‡§∏‡§æ‡§• offload ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è transformer Linear layers ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ (0-100)‡•§
- **Default**: `100` (‡§∏‡§≠‡•Ä eligible layers)
- **Why**: VRAM ‡§¨‡§ö‡§§ ‡§î‡§∞ performance ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§∏‡§Ç‡§§‡•Å‡§≤‡§® ‡§ï‡•á ‡§≤‡§ø‡§è partial offloading ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§ ‡§ï‡§Æ values GPU ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï layers ‡§∞‡§ñ‡§§‡•Ä ‡§π‡•à‡§Ç ‡§ú‡§ø‡§∏‡§∏‡•á training ‡§§‡•á‡§ú ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§ú‡§¨‡§ï‡§ø memory usage ‡§≠‡•Ä ‡§ï‡§Æ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§
- **Notes**: Layers module traversal order ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§∏‡•á select ‡§ï‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ `--ramtorch_target_modules` ‡§ï‡•á ‡§∏‡§æ‡§• combine ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

### `--ramtorch_text_encoder_percent`

- **What**: RamTorch ‡§ï‡•á ‡§∏‡§æ‡§• offload ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è text encoder Linear layers ‡§ï‡§æ ‡§™‡•ç‡§∞‡§§‡§ø‡§∂‡§§ (0-100)‡•§
- **Default**: `100` (‡§∏‡§≠‡•Ä eligible layers)
- **Why**: ‡§ú‡§¨ `--ramtorch_text_encoder` enabled ‡§π‡•ã ‡§§‡§¨ text encoders ‡§ï‡•Ä partial offloading ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Notes**: ‡§ï‡•á‡§µ‡§≤ ‡§§‡§¨ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ `--ramtorch_text_encoder` enabled ‡§π‡•ã‡•§

### `--ramtorch_disable_sync_hooks`

- **What**: RamTorch layers ‡§ï‡•á ‡§¨‡§æ‡§¶ add ‡§ï‡§ø‡§è ‡§ó‡§è CUDA synchronization hooks ‡§ï‡•ã disable ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `False` (sync hooks enabled)
- **Why**: Sync hooks RamTorch ‡§ï‡•á ping-pong buffering system ‡§Æ‡•á‡§Ç race conditions ‡§ï‡•ã fix ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•ã non-deterministic outputs ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§¨‡§® ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ Disable ‡§ï‡§∞‡§®‡•á ‡§∏‡•á performance ‡§¨‡•á‡§π‡§§‡§∞ ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® incorrect results ‡§ï‡§æ risk ‡§π‡•à‡•§
- **Notes**: ‡§ï‡•á‡§µ‡§≤ ‡§§‡§¨ disable ‡§ï‡§∞‡•á‡§Ç ‡§ú‡§¨ sync hooks ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ ‡§π‡•ã ‡§Ø‡§æ ‡§â‡§®‡§ï‡•á ‡§¨‡§ø‡§®‡§æ test ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡•§

### `--ramtorch_disable_extensions`

- **What**: ‡§ï‡•á‡§µ‡§≤ Linear layers ‡§™‡§∞ RamTorch apply ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, Embedding/RMSNorm/LayerNorm/Conv ‡§ï‡•ã skip ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `True` (extensions disabled)
- **Why**: SimpleTuner RamTorch ‡§ï‡•ã Linear layers ‡§∏‡•á ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡§æ‡§ï‡§∞ Embedding, RMSNorm, LayerNorm, ‡§î‡§∞ Conv layers ‡§ï‡•ã include ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§® extensions ‡§ï‡•ã disable ‡§ï‡§∞‡§ï‡•á ‡§ï‡•á‡§µ‡§≤ Linear layers offload ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- **Notes**: VRAM savings ‡§ï‡§Æ ‡§π‡•ã ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® extended layer types ‡§ï‡•Ä ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§ï‡•ã debug ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

### `--pretrained_model_name_or_path`

- **What**: pretrained model ‡§ï‡§æ path ‡§Ø‡§æ <https://huggingface.co/models> ‡§∏‡•á ‡§â‡§∏‡§ï‡§æ identifier.
- **Why**: ‡§â‡§∏ base model ‡§ï‡•ã ‡§®‡§ø‡§∞‡•ç‡§¶‡§ø‡§∑‡•ç‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡§ø‡§∏‡§∏‡•á training ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§ó‡•Ä‡•§ Repository ‡§∏‡•á specific versions ‡§ö‡•Å‡§®‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `--revision` ‡§î‡§∞ `--variant` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ø‡§π SDXL, Flux, ‡§î‡§∞ SD3.x ‡§ï‡•á ‡§≤‡§ø‡§è single‚Äëfile `.safetensors` paths ‡§≠‡•Ä ‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--pretrained_t5_model_name_or_path`

- **What**: pretrained T5 model ‡§ï‡§æ path ‡§Ø‡§æ <https://huggingface.co/models> ‡§∏‡•á ‡§â‡§∏‡§ï‡§æ identifier.
- **Why**: PixArt ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Ü‡§™ ‡§Ö‡§™‡§®‡•á T5 weights ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•ã‡§à specific source ‡§ö‡•Å‡§®‡§®‡§æ ‡§ö‡§æ‡§π ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø base model switch ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§¨‡§æ‡§∞‚Äë‡§¨‡§æ‡§∞ download ‡§® ‡§ï‡§∞‡§®‡§æ ‡§™‡§°‡§º‡•á‡•§

### `--pretrained_gemma_model_name_or_path`

- **What**: pretrained Gemma model ‡§ï‡§æ path ‡§Ø‡§æ <https://huggingface.co/models> ‡§∏‡•á ‡§â‡§∏‡§ï‡§æ identifier.
- **Why**: Gemma‚Äëbased models (‡§ú‡•à‡§∏‡•á LTX-2, Sana, Lumina2) ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Ü‡§™ base diffusion model path ‡§¨‡§¶‡§≤‡•á ‡§¨‡§ø‡§®‡§æ Gemma weights ‡§ï‡§æ source specify ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--custom_text_encoder_intermediary_layers`

- **What**: FLUX.2 models ‡§ï‡•á ‡§≤‡§ø‡§è text encoder ‡§∏‡•á extract ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä hidden state layers ‡§ï‡•ã override ‡§ï‡§∞‡•á‡§Ç‡•§
- **Format**: Layer indices ‡§ï‡§æ JSON array, ‡§ú‡•à‡§∏‡•á `[10, 20, 30]`
- **Default**: ‡§∏‡•á‡§ü ‡§® ‡§π‡•ã‡§®‡•á ‡§™‡§∞ model-specific defaults ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç:
  - FLUX.2-dev (Mistral-3): `[10, 20, 30]`
  - FLUX.2-klein (Qwen3): `[9, 18, 27]`
- **Why**: Custom alignment ‡§Ø‡§æ research ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® text encoder hidden state combinations ‡§ï‡•á ‡§∏‡§æ‡§• experiment ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Note**: ‡§Ø‡§π option experimental ‡§π‡•à ‡§î‡§∞ ‡§ï‡•á‡§µ‡§≤ FLUX.2 models ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ Layer indices ‡§¨‡§¶‡§≤‡§®‡•á ‡§∏‡•á cached text embeddings invalid ‡§π‡•ã ‡§ú‡§æ‡§è‡§Ç‡§ó‡•á ‡§î‡§∞ regenerate ‡§ï‡§∞‡§®‡•á ‡§π‡•ã‡§Ç‡§ó‡•á‡•§ Layers ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ model ‡§ï‡•Ä expected input (3 layers) ‡§∏‡•á match ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è‡•§

### `--gradient_checkpointing`

- **What**: Training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® gradients layerwise compute ‡§π‡•ã‡§ï‡§∞ accumulate ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø peak VRAM ‡§ï‡§Æ ‡§π‡•ã, ‡§≤‡•á‡§ï‡§ø‡§® training ‡§ß‡•Ä‡§Æ‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§

### `--gradient_checkpointing_interval`

- **What**: ‡§π‡§∞ *n* blocks ‡§™‡§∞ checkpoint ‡§ï‡§∞‡•á‡§Ç, ‡§ú‡§π‡§æ‡§Å *n* ‡§∂‡•Ç‡§®‡•ç‡§Ø ‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§Æ‡§æ‡§® ‡§π‡•à‡•§ 1 ‡§ï‡§æ ‡§Æ‡§æ‡§® `--gradient_checkpointing` enabled ‡§ú‡•à‡§∏‡§æ ‡§π‡•à, ‡§î‡§∞ 2 ‡§π‡§∞ ‡§¶‡•Ç‡§∏‡§∞‡•á block ‡§™‡§∞ checkpoint ‡§ï‡§∞‡•á‡§ó‡§æ‡•§
- **Note**: ‡§Ø‡§π ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§´‡§ø‡§≤‡§π‡§æ‡§≤ ‡§ï‡•á‡§µ‡§≤ SDXL ‡§î‡§∞ Flux ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡•§ SDXL ‡§á‡§∏‡§Æ‡•á‡§Ç hackish implementation ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--gradient_checkpointing_backend`

- **Choices**: `torch`, `unsloth`
- **What**: Gradient checkpointing ‡§ï‡•á ‡§≤‡§ø‡§è implementation ‡§ö‡•Å‡§®‡•á‡§Ç‡•§
  - `torch` (default): Standard PyTorch checkpointing ‡§ú‡•ã backward pass ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® activations ‡§ï‡•ã recalculate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ~20% time overhead‡•§
  - `unsloth`: Recalculate ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø activations ‡§ï‡•ã asynchronously CPU ‡§™‡§∞ offload ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ~30% ‡§Ö‡§ß‡§ø‡§ï memory ‡§¨‡§ö‡§§ ‡§ï‡•á‡§µ‡§≤ ~2% overhead ‡§ï‡•á ‡§∏‡§æ‡§•‡•§ Fast PCIe bandwidth ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§
- **Note**: ‡§ï‡•á‡§µ‡§≤ `--gradient_checkpointing` enabled ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä‡•§ `unsloth` backend ‡§ï‡•á ‡§≤‡§ø‡§è CUDA ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§

### `--refiner_training`

- **What**: custom mixture‚Äëof‚Äëexperts ‡§Æ‡•â‡§°‡§≤ ‡§∂‡•ç‡§∞‡•É‡§Ç‡§ñ‡§≤‡§æ training ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§® ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™‡•ã‡§Ç ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ï‡•á ‡§≤‡§ø‡§è [Mixture-of-Experts](MIXTURE_OF_EXPERTS.md) ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

## Precision

### `--quantize_via`

- **Choices**: `cpu`, `accelerator`, `pipeline`
  - `accelerator` ‡§™‡§∞ ‡§Ø‡§π ‡§Æ‡§ß‡•ç‡§Ø‡§Æ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§§‡•á‡§ú‡§º ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® Flux ‡§ú‡•à‡§∏‡•á ‡§¨‡§°‡§º‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è 24G cards ‡§™‡§∞ OOM ‡§ï‡§æ ‡§ú‡•ã‡§ñ‡§ø‡§Æ ‡§∞‡§π‡§§‡§æ ‡§π‡•à‡•§
  - `cpu` ‡§™‡§∞ quantisation ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§≠‡§ó 30 seconds ‡§≤‡§ó‡§§‡•á ‡§π‡•à‡§Ç‡•§ (**Default**)
  - `pipeline` Diffusers ‡§ï‡•ã `--quantization_config` ‡§Ø‡§æ pipeline‚Äëcapable presets (‡§â‡§¶‡§æ. `nf4-bnb`, `int8-torchao`, `fp8-torchao`, `int8-quanto`, ‡§Ø‡§æ `.gguf` checkpoints) ‡§ï‡•á ‡§∏‡§æ‡§• quantization delegate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--base_model_precision`

- **What**: model precision ‡§ò‡§ü‡§æ‡§è‡§Å ‡§î‡§∞ ‡§ï‡§Æ memory ‡§Æ‡•á‡§Ç training ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§§‡•Ä‡§® ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ quantisation backends ‡§π‡•à‡§Ç: BitsAndBytes (pipeline), TorchAO (pipeline ‡§Ø‡§æ manual), ‡§î‡§∞ Optimum Quanto (pipeline ‡§Ø‡§æ manual)‡•§

#### Diffusers pipeline presets

- `nf4-bnb` Diffusers ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á 4‚Äëbit NF4 BitsAndBytes config ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≤‡•ã‡§° ‡§π‡•ã‡§§‡§æ ‡§π‡•à (CUDA only)‡•§ `bitsandbytes` ‡§î‡§∞ BnB support ‡§µ‡§æ‡§≤‡•Ä diffusers build ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§
- `int4-torchao`, `int8-torchao`, ‡§î‡§∞ `fp8-torchao` Diffusers ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á TorchAoConfig ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (CUDA)‡•§ `torchao` ‡§î‡§∞ recent diffusers/transformers ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§
- `int8-quanto`, `int4-quanto`, `int2-quanto`, `fp8-quanto`, ‡§î‡§∞ `fp8uz-quanto` Diffusers ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á QuantoConfig ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ Diffusers FP8-NUZ ‡§ï‡•ã float8 weights ‡§™‡§∞ map ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; NUZ variant ‡§ï‡•á ‡§≤‡§ø‡§è manual quanto quantization ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- `.gguf` checkpoints auto‚Äëdetect ‡§π‡•ã‡§ï‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§®‡•á ‡§™‡§∞ `GGUFQuantizationConfig` ‡§ï‡•á ‡§∏‡§æ‡§• ‡§≤‡•ã‡§° ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ GGUF support ‡§ï‡•á ‡§≤‡§ø‡§è recent diffusers/transformers install ‡§ï‡§∞‡•á‡§Ç‡•§

#### Optimum Quanto

Hugging Face ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡•Ä ‡§ó‡§à optimum‚Äëquanto ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§∏‡§≠‡•Ä ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•ç‡§∏ ‡§™‡§∞ ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§∏‡§Æ‡§∞‡•ç‡§•‡§® ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡•§

- `int8-quanto` ‡§∏‡§¨‡§∏‡•á ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á compatible ‡§π‡•à ‡§î‡§∞ ‡§∏‡§Ç‡§≠‡§µ‡§§‡§É ‡§∏‡§∞‡•ç‡§µ‡•ã‡§§‡•ç‡§§‡§Æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à
  - RTX4090 ‡§î‡§∞ ‡§∏‡§Ç‡§≠‡§µ‡§§‡§É ‡§Ö‡§®‡•ç‡§Ø GPUs ‡§™‡§∞ ‡§∏‡§¨‡§∏‡•á ‡§§‡•á‡§ú‡§º training
  - CUDA devices ‡§™‡§∞ int8, int4 ‡§ï‡•á ‡§≤‡§ø‡§è hardware‚Äëaccelerated matmul ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
    - int4 ‡§Ö‡§≠‡•Ä ‡§≠‡•Ä ‡§¨‡§π‡•Å‡§§ ‡§ß‡•Ä‡§Æ‡§æ ‡§π‡•à
  - `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- `fp8uz-quanto` CUDA ‡§î‡§∞ ROCm devices ‡§ï‡•á ‡§≤‡§ø‡§è experimental fp8 variant ‡§π‡•à‡•§
  - Instinct ‡§Ø‡§æ ‡§®‡§à architecture ‡§µ‡§æ‡§≤‡•á AMD silicon ‡§™‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ supported
  - 4090 ‡§™‡§∞ training ‡§Æ‡•á‡§Ç `int8-quanto` ‡§∏‡•á ‡§•‡•ã‡§°‡§º‡§æ ‡§§‡•á‡§ú‡§º ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® inference ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç (1 second ‡§ß‡•Ä‡§Æ‡§æ)
  - `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- `fp8-quanto` ‡§´‡§ø‡§≤‡§π‡§æ‡§≤ fp8 matmul ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡•á‡§ó‡§æ, Apple systems ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ‡•§
  - CUDA ‡§Ø‡§æ ROCm ‡§™‡§∞ hardware fp8 matmul ‡§Ö‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è ‡§∏‡§Ç‡§≠‡§µ‡§§‡§É int8 ‡§∏‡•á ‡§ï‡§æ‡§´‡•Ä ‡§ß‡•Ä‡§Æ‡§æ ‡§π‡•ã‡§ó‡§æ
    - fp8 GEMM ‡§ï‡•á ‡§≤‡§ø‡§è MARLIN kernel ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
  - dynamo ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§∏‡§Ç‡§ó‡§§, ‡§á‡§∏ ‡§∏‡§Ç‡§Ø‡•ã‡§ú‡§® ‡§ï‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ dynamo ‡§∏‡•ç‡§µ‡§§‡§É disabled ‡§π‡•ã ‡§ú‡§æ‡§è‡§ó‡§æ‡•§

#### TorchAO

PyTorch ‡§ï‡•Ä ‡§è‡§ï ‡§®‡§à ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä, AO ‡§π‡§Æ‡•á‡§Ç linears ‡§î‡§∞ 2D convolutions (‡§â‡§¶‡§æ. unet style models) ‡§ï‡•ã quantised counterparts ‡§∏‡•á ‡§¨‡§¶‡§≤‡§®‡•á ‡§¶‡•á‡§§‡•Ä ‡§π‡•à‡•§
<!-- Additionally, it provides an experimental CPU offload optimiser that essentially provides a simpler reimplementation of DeepSpeed. -->

- `int8-torchao` memory consumption ‡§ï‡•ã Quanto ‡§ï‡•á precision levels ‡§ú‡•à‡§∏‡§æ ‡§ï‡§Æ ‡§ï‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à
  - ‡§≤‡§ø‡§ñ‡§§‡•á ‡§∏‡§Æ‡§Ø, Apple MPS ‡§™‡§∞ Quanto (9s/iter) ‡§ï‡•Ä ‡§§‡•Å‡§≤‡§®‡§æ ‡§Æ‡•á‡§Ç ‡§•‡•ã‡§°‡§º‡§æ ‡§ß‡•Ä‡§Æ‡§æ (11s/iter)
  - `torch.compile` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§® ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ CUDA devices ‡§™‡§∞ `int8-quanto` ‡§ú‡•à‡§∏‡•Ä speed ‡§î‡§∞ memory, ROCm ‡§™‡§∞ speed profile ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§
  - `torch.compile` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ `int8-quanto` ‡§∏‡•á ‡§ß‡•Ä‡§Æ‡§æ
- `fp8-torchao` ‡§ï‡•á‡§µ‡§≤ Hopper (H100, H200) ‡§Ø‡§æ ‡§®‡§è (Blackwell B200) accelerators ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à

##### Optimisers

TorchAO ‡§Æ‡•á‡§Ç ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø 4bit ‡§î‡§∞ 8bit optimisers ‡§π‡•à‡§Ç: `ao-adamw8bit`, `ao-adamw4bit`

‡§Ø‡§π Hopper (H100 ‡§Ø‡§æ ‡§¨‡•á‡§π‡§§‡§∞) users ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¶‡•ã optimisers ‡§≠‡•Ä ‡§¶‡•á‡§§‡§æ ‡§π‡•à: `ao-adamfp8`, ‡§î‡§∞ `ao-adamwfp8`

#### SDNQ (SD.Next Quantization Engine)

[SDNQ](https://github.com/disty0/sdnq) ‡§è‡§ï training‚Äëoptimized quantization ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§∏‡§≠‡•Ä ‡§™‡•ç‡§≤‡•á‡§ü‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•ç‡§∏ ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à: AMD (ROCm), Apple (MPS), ‡§î‡§∞ NVIDIA (CUDA)‡•§ ‡§Ø‡§π stochastic rounding ‡§î‡§∞ quantized optimizer states ‡§ï‡•á ‡§∏‡§æ‡§• memory‚Äëefficient quantized training ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§

##### Recommended Precision Levels

**Full finetuning ‡§ï‡•á ‡§≤‡§ø‡§è** (model weights update ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç):
- `uint8-sdnq` - memory ‡§¨‡§ö‡§§ ‡§î‡§∞ training quality ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§∏‡§Ç‡§§‡•Å‡§≤‡§®
- `uint16-sdnq` - ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ quality ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§ö‡•ç‡§ö precision (‡§â‡§¶‡§æ. Stable Cascade)
- `int16-sdnq` - signed 16‚Äëbit ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™
- `fp16-sdnq` - quantized FP16, SDNQ ‡§≤‡§æ‡§≠‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ precision

**LoRA training ‡§ï‡•á ‡§≤‡§ø‡§è** (base model weights frozen):
- `int8-sdnq` - signed 8‚Äëbit, ‡§Ö‡§ö‡•ç‡§õ‡§æ general purpose ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™
- `int6-sdnq`, `int5-sdnq` - lower precision, ‡§õ‡•ã‡§ü‡§æ memory footprint
- `uint5-sdnq`, `uint4-sdnq`, `uint3-sdnq`, `uint2-sdnq` - aggressive compression

**Note:** `int7-sdnq` ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç (‡§ß‡•Ä‡§Æ‡§æ ‡§î‡§∞ int8 ‡§∏‡•á ‡§¨‡§π‡•Å‡§§ ‡§õ‡•ã‡§ü‡§æ ‡§®‡§π‡•Ä‡§Ç)‡•§

**Important:** 5‚Äëbit ‡§∏‡•á ‡§®‡•Ä‡§ö‡•á SDNQ ‡§ó‡•Å‡§£‡§µ‡§§‡•ç‡§§‡§æ ‡§¨‡§®‡§æ‡§è ‡§∞‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•ç‡§µ‡§§‡§É SVD (Singular Value Decomposition) ‡§ï‡•ã 8 steps ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ SVD quantize ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§ß‡§ø‡§ï ‡§∏‡§Æ‡§Ø ‡§≤‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ non‚Äëdeterministic ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è Disty0 HuggingFace ‡§™‡§∞ pre‚Äëquantized SVD ‡§Æ‡•â‡§°‡§≤ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§ SVD training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® compute overhead ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è full finetuning ‡§Æ‡•á‡§Ç (‡§ú‡§π‡§æ‡§Å weights ‡§∏‡§ï‡•ç‡§∞‡§ø‡§Ø ‡§∞‡•Ç‡§™ ‡§∏‡•á update ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç) ‡§á‡§∏‡§∏‡•á ‡§¨‡§ö‡•á‡§Ç‡•§

**Key features:**
- Cross‚Äëplatform: AMD, Apple, ‡§î‡§∞ NVIDIA ‡§π‡§æ‡§∞‡•ç‡§°‡§µ‡•á‡§Ø‡§∞ ‡§™‡§∞ ‡§∏‡§Æ‡§æ‡§® ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Training‚Äëoptimized: stochastic rounding ‡§∏‡•á quantization error accumulation ‡§ï‡§Æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à
- Memory efficient: quantized optimizer state buffers ‡§∏‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- Decoupled matmul: weight precision ‡§î‡§∞ matmul precision ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞ ‡§π‡•à‡§Ç (INT8/FP8/FP16 matmul ‡§â‡§™‡§≤‡§¨‡•ç‡§ß)

##### SDNQ Optimisers

SDNQ ‡§Æ‡•á‡§Ç ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ memory ‡§¨‡§ö‡§§ ‡§ï‡•á ‡§≤‡§ø‡§è optional quantized state buffers ‡§µ‡§æ‡§≤‡•á optimizers ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç:

- `sdnq-adamw` - AdamW with quantized state buffers (uint8, group_size=32)
- `sdnq-adamw+no_quant` - ‡§¨‡§ø‡§®‡§æ quantized states ‡§ï‡•á AdamW (comparison ‡§ï‡•á ‡§≤‡§ø‡§è)
- `sdnq-adafactor` - quantized state buffers ‡§ï‡•á ‡§∏‡§æ‡§• Adafactor
- `sdnq-came` - quantized state buffers ‡§ï‡•á ‡§∏‡§æ‡§• CAME optimizer
- `sdnq-lion` - quantized state buffers ‡§ï‡•á ‡§∏‡§æ‡§• Lion optimizer
- `sdnq-muon` - quantized state buffers ‡§ï‡•á ‡§∏‡§æ‡§• Muon optimizer
- `sdnq-muon+quantized_matmul` - zeropower computation ‡§Æ‡•á‡§Ç INT8 matmul ‡§ï‡•á ‡§∏‡§æ‡§• Muon

‡§∏‡§≠‡•Ä SDNQ optimizers ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á stochastic rounding ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ custom settings ‡§ï‡•á ‡§≤‡§ø‡§è `--optimizer_config` ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§ø‡§è ‡§ú‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡•à‡§∏‡•á `use_quantized_buffers=false` ‡§ú‡§ø‡§∏‡§∏‡•á state quantization ‡§¨‡§Ç‡§¶ ‡§π‡•ã ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§

**Muon‚Äëspecific options:**
- `use_quantized_matmul` - zeropower_via_newtonschulz5 ‡§Æ‡•á‡§Ç INT8/FP8/FP16 matmul ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç
- `quantized_matmul_dtype` - matmul precision: `int8` (consumer GPUs), `fp8` (datacenter), `fp16`
- `zeropower_dtype` - zeropower computation ‡§ï‡•á ‡§≤‡§ø‡§è precision (‡§ú‡§¨ `use_quantized_matmul=True` ‡§π‡•ã ‡§§‡§¨ ignore)
- Muon ‡§¨‡§®‡§æ‡§Æ AdamW fallback ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§≤‡§ó values ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§π‡•á‡§§‡•Å args ‡§ï‡•ã `muon_` ‡§Ø‡§æ `adamw_` prefix ‡§ï‡§∞‡•á‡§Ç

**Pre‚Äëquantized models:** Disty0 pre‚Äëquantized uint4 SVD models [huggingface.co/collections/Disty0/sdnq](https://huggingface.co/collections/Disty0/sdnq) ‡§™‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§®‡•ç‡§π‡•á‡§Ç ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç, ‡§´‡§ø‡§∞ SDNQ import ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ `convert_sdnq_model_to_training()` ‡§∏‡•á convert ‡§ï‡§∞‡•á‡§Ç (register ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è loading ‡§∏‡•á ‡§™‡§π‡§≤‡•á SDNQ import ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è)‡•§

**Note on checkpointing:** SDNQ training models ‡§ï‡•ã training resumption ‡§ï‡•á ‡§≤‡§ø‡§è native PyTorch format (`.pt`) ‡§î‡§∞ inference ‡§ï‡•á ‡§≤‡§ø‡§è safetensors format ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§µ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§ Proper training resumption ‡§ï‡•á ‡§≤‡§ø‡§è native format ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø SDNQ ‡§ï‡•Ä `SDNQTensor` class custom serialization ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•Ä ‡§π‡•à‡•§

**Disk space tip:** disk space ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§™ ‡§ï‡•á‡§µ‡§≤ quantized weights ‡§∞‡§ñ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ inference ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡§º‡§∞‡•Ç‡§∞‡§§ ‡§™‡§°‡§º‡§®‡•á ‡§™‡§∞ SDNQ ‡§ï‡•Ä [dequantize_sdnq_training.py](https://github.com/Disty0/sdnq/blob/main/scripts/dequantize_sdnq_training.py) script ‡§∏‡•á dequantize ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--quantization_config`

- **What**: `--quantize_via=pipeline` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø Diffusers `quantization_config` overrides ‡§ï‡§æ JSON object ‡§Ø‡§æ file path.
- **How**: inline JSON (‡§Ø‡§æ file) ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç per‚Äëcomponent entries ‡§π‡•ã‡§Ç‡•§ Keys ‡§Æ‡•á‡§Ç `unet`, `transformer`, `text_encoder`, ‡§Ø‡§æ `default` ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Examples**:

```json
{
  "unet": {"load_in_4bit": true, "bnb_4bit_quant_type": "nf4", "bnb_4bit_compute_dtype": "bfloat16"},
  "text_encoder": {"quant_type": {"group_size": 128}}
}
```

‡§Ø‡§π ‡§â‡§¶‡§æ‡§π‡§∞‡§£ UNet ‡§™‡§∞ 4‚Äëbit NF4 BnB ‡§î‡§∞ text encoder ‡§™‡§∞ TorchAO int4 ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

#### Torch Dynamo

WebUI ‡§∏‡•á `torch.compile()` ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è **Hardware ‚Üí Accelerate (advanced)** ‡§™‡§∞ ‡§ú‡§æ‡§è‡§Å ‡§î‡§∞ **Torch Dynamo Backend** ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§™‡§∏‡§Ç‡§¶‡•Ä‡§¶‡§æ compiler (‡§â‡§¶‡§æ. *inductor*) ‡§™‡§∞ ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ toggles ‡§Ü‡§™‡§ï‡•ã optimisation **mode** ‡§ö‡•Å‡§®‡§®‡•á, **dynamic shape** guards ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡•á, ‡§Ø‡§æ **regional compilation** opt‚Äëin ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø ‡§¨‡§π‡•Å‡§§ ‡§ó‡§π‡§∞‡•á transformer models ‡§™‡§∞ cold starts ‡§§‡•á‡§ú‡§º ‡§π‡•ã ‡§∏‡§ï‡•á‡§Ç‡•§

‡§µ‡§π‡•Ä ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® `config/config.env` ‡§Æ‡•á‡§Ç ‡§á‡§∏ ‡§§‡§∞‡§π ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§ï‡•Ä ‡§ú‡§æ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à:

```bash
TRAINING_DYNAMO_BACKEND=inductor
```

‡§Ü‡§™ ‡§á‡§∏‡•á ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á `--dynamo_mode=max-autotune` ‡§Ø‡§æ UI ‡§Æ‡•á‡§Ç ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§Ö‡§®‡•ç‡§Ø Dynamo flags ‡§ï‡•á ‡§∏‡§æ‡§• pair ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø finer control ‡§Æ‡§ø‡§≤‡•á‡•§

‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç ‡§ï‡§ø training ‡§ï‡•á ‡§™‡§π‡§≤‡•á ‡§ï‡§à steps ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§∏‡•á ‡§ß‡•Ä‡§Æ‡•á ‡§π‡•ã‡§Ç‡§ó‡•á ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø compilation background ‡§Æ‡•á‡§Ç ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§

Settings ‡§ï‡•ã `config.json` ‡§Æ‡•á‡§Ç persist ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§Æ‡§æ‡§® keys ‡§ú‡•ã‡§°‡§º‡•á‡§Ç:

```json
{
  "dynamo_backend": "inductor",
  "dynamo_mode": "max-autotune",
  "dynamo_fullgraph": false,
  "dynamo_dynamic": false,
  "dynamo_use_regional_compilation": true
}
```

‡§Ø‡§¶‡§ø ‡§Ü‡§™ Accelerate defaults inherit ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡•ã ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ entries ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§Ç (‡§â‡§¶‡§æ., `dynamo_mode` ‡§® ‡§¶‡•á‡§Ç ‡§§‡§æ‡§ï‡§ø automatic selection ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã)‡•§

### `--attention_mechanism`

Alternative attention mechanisms ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡§Ç, ‡§ú‡§ø‡§®‡§ï‡•á compatibility ‡§∏‡•ç‡§§‡§∞ ‡§Ø‡§æ trade‚Äëoffs ‡§Ö‡§≤‡§ó ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç:

- `diffusers` PyTorch ‡§ï‡•á native SDPA kernels ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§π‡•à‡•§
- `xformers` Meta ‡§ï‡•á [xformers](https://github.com/facebookresearch/xformers) attention kernel (training + inference) ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§¨ underlying ‡§Æ‡•â‡§°‡§≤ `enable_xformers_memory_efficient_attention` expose ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- `flash-attn`, `flash-attn-2`, `flash-attn-3`, ‡§î‡§∞ `flash-attn-3-varlen` Diffusers ‡§ï‡•á ‡§®‡§è `attention_backend` helper ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è FlashAttention v1/2/3 kernels ‡§Æ‡•á‡§Ç route ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ `flash-attn` / `flash-attn-interface` wheels install ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç ‡§ï‡§ø FA3 ‡§´‡§ø‡§≤‡§π‡§æ‡§≤ Hopper GPUs ‡§ï‡•Ä ‡§Æ‡§æ‡§Ç‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- `flex` PyTorch 2.5 ‡§ï‡§æ FlexAttention backend ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à (CUDA ‡§™‡§∞ FP16/BF16)‡•§ ‡§Ü‡§™‡§ï‡•ã Flex kernels ‡§Ö‡§≤‡§ó ‡§∏‡•á compile/install ‡§ï‡§∞‡§®‡•á ‡§π‡•ã‡§Ç‡§ó‡•á ‚Äî ‡§¶‡•á‡§ñ‡•á‡§Ç [documentation/attention/FLEX.md](attention/FLEX.md)‡•§
- `cudnn`, `native-efficient`, `native-flash`, `native-math`, `native-npu`, ‡§î‡§∞ `native-xla` `torch.nn.attention.sdpa_kernel` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ expose ‡§ï‡§ø‡§è ‡§ó‡§è matching SDPA backend ‡§ö‡•Å‡§®‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ø‡•á ‡§§‡§¨ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à‡§Ç ‡§ú‡§¨ ‡§Ü‡§™‡§ï‡•ã determinism (`native-math`), CuDNN SDPA kernel, ‡§Ø‡§æ vendor‚Äënative accelerators (NPU/XLA) ‡§ö‡§æ‡§π‡§ø‡§è‡•§
- `sla` [Sparse‚ÄìLinear Attention (SLA)](https://github.com/thu-ml/SLA) ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã fine‚Äëtunable sparse/linear hybrid kernel ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ training ‡§§‡§•‡§æ validation ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§ø‡§®‡§æ ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ gating ‡§ï‡•á ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
  - SLA package install ‡§ï‡§∞‡•á‡§Ç (‡§â‡§¶‡§æ. `pip install -e ~/src/SLA`) ‡§á‡§∏ backend ‡§ï‡•ã ‡§ö‡•Å‡§®‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á‡•§
  - SimpleTuner SLA ‡§ï‡•á learned projection weights ‡§π‡§∞ checkpoint ‡§Æ‡•á‡§Ç `sla_attention.pt` ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§µ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; resume ‡§î‡§∞ inference ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏ ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã ‡§¨‡§æ‡§ï‡•Ä checkpoint ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∞‡§ñ‡•á‡§Ç‡•§
  - ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø backbone ‡§ï‡•ã SLA ‡§ï‡•á mixed sparse/linear behavior ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ ‡§ü‡•ç‡§Ø‡•Ç‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, inference ‡§∏‡§Æ‡§Ø ‡§™‡§∞ ‡§≠‡•Ä SLA ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã‡§ó‡§æ‡•§ focused ‡§ó‡§æ‡§á‡§° ‡§ï‡•á ‡§≤‡§ø‡§è `documentation/attention/SLA.md` ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§
  - ‡§Ø‡§¶‡§ø ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§π‡•ã ‡§§‡•ã SLA runtime defaults override ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `--sla_config '{"topk":0.15,"blkq":32,"tie_feature_map_qk":false}'` (JSON ‡§Ø‡§æ Python dict syntax) ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- `sageattention`, `sageattention-int8-fp16-triton`, `sageattention-int8-fp16-cuda`, ‡§î‡§∞ `sageattention-int8-fp8-cuda` ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ [SageAttention](https://github.com/thu-ml/SageAttention) kernels ‡§ï‡•ã wrap ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ø‡•á inference‚Äëoriented ‡§π‡•à‡§Ç ‡§î‡§∞ accidental training ‡§∏‡•á ‡§¨‡§ö‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `--sageattention_usage` ‡§ï‡•á ‡§∏‡§æ‡§• ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è‡•§
  - ‡§∏‡§∞‡§≤ ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§Æ‡•á‡§Ç, SageAttention inference ‡§ï‡•á compute requirement ‡§ï‡•ã ‡§ï‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à

> ‚ÑπÔ∏è Flash/Flex/PyTorch backend selectors Diffusers ‡§ï‡•á `attention_backend` dispatcher ‡§™‡§∞ ‡§®‡§ø‡§∞‡•ç‡§≠‡§∞ ‡§π‡•à‡§Ç, ‡§á‡§∏‡§≤‡§ø‡§è ‡§µ‡•á ‡§µ‡§∞‡•ç‡§§‡§Æ‡§æ‡§® ‡§Æ‡•á‡§Ç transformer‚Äëstyle models ‡§Æ‡•á‡§Ç ‡§Ö‡§ß‡§ø‡§ï ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§á‡§∏ code path ‡§ï‡•ã ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (Flux, Wan 2.x, LTXVideo, QwenImage, ‡§Ü‡§¶‡§ø)‡•§ Classic SD/SDXL UNets ‡§Ö‡§≠‡•Ä ‡§∏‡•Ä‡§ß‡•á PyTorch SDPA ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§

`--sageattention_usage` ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è SageAttention ‡§ï‡•á ‡§∏‡§æ‡§• training ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡§æ ‡§∏‡§æ‡§µ‡§ß‡§æ‡§®‡•Ä ‡§∏‡•á ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è, ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§Ø‡§π QKV linears ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§™‡§®‡•Ä custom CUDA implementations ‡§∏‡•á gradients track ‡§Ø‡§æ propagate ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ‡•§

- ‡§á‡§∏‡§∏‡•á ‡§Ø‡•á layers ‡§™‡•Ç‡§∞‡•Ä ‡§§‡§∞‡§π untrained ‡§∞‡§π ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡•ã model collapse ‡§Ø‡§æ ‡§õ‡•ã‡§ü‡•á training runs ‡§Æ‡•á‡§Ç ‡§π‡§≤‡•ç‡§ï‡§æ ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§

---

## üì∞ Publishing

### `--push_to_hub`

- **What**: ‡§Ø‡§¶‡§ø ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ, ‡§§‡•ã training ‡§™‡•Ç‡§∞‡•Ä ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§Ü‡§™‡§ï‡§æ ‡§Æ‡•â‡§°‡§≤ [Huggingface Hub](https://huggingface.co) ‡§™‡§∞ upload ‡§π‡•ã‡§ó‡§æ‡•§ `--push_checkpoints_to_hub` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§π‡§∞ intermediary checkpoint ‡§≠‡•Ä push ‡§π‡•ã‡§ó‡§æ‡•§

### `--push_to_hub_background`

- **What**: Hugging Face Hub ‡§™‡§∞ background worker ‡§∏‡•á uploads ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø checkpoint pushes training loop ‡§ï‡•ã pause ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: Hub uploads asynchronous ‡§∞‡§π‡§§‡•á ‡§π‡•Å‡§è training ‡§î‡§∞ validation ‡§ö‡§≤‡§§‡•Ä ‡§∞‡§π‡§§‡•Ä ‡§π‡•à‡•§ Run ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á final uploads ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à ‡§§‡§æ‡§ï‡§ø failures surface ‡§π‡•ã‡§Ç‡•§

### `--webhook_config`

- **What**: webhook targets (‡§â‡§¶‡§æ. Discord, custom endpoints) ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§§‡§æ‡§ï‡§ø real‚Äëtime training events ‡§Æ‡§ø‡§≤ ‡§∏‡§ï‡•á‡§Ç‡•§
- **Why**: ‡§¨‡§æ‡§π‡§∞‡•Ä tools ‡§î‡§∞ dashboards ‡§ï‡•á ‡§∏‡§æ‡§• training runs ‡§Æ‡•â‡§®‡§ø‡§ü‡§∞ ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø training ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§™‡§∞ notifications ‡§≠‡•á‡§ú‡§§‡§æ ‡§π‡•à‡•§
- **Notes**: webhook payloads ‡§Æ‡•á‡§Ç `job_id` ‡§´‡§º‡•Ä‡§≤‡•ç‡§° `SIMPLETUNER_JOB_ID` environment variable ‡§∏‡•á‡§ü ‡§ï‡§∞‡§ï‡•á ‡§≠‡§∞‡•Ä ‡§ú‡§æ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à:
  ```bash
  export SIMPLETUNER_JOB_ID="my-training-run-name"
  python train.py
  ```
‡§Ø‡§π ‡§â‡§® monitoring tools ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§ï‡§à training runs ‡§∏‡•á webhooks ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§§‡§æ‡§ï‡§ø ‡§Ø‡§π ‡§™‡§§‡§æ ‡§ö‡§≤‡•á ‡§ï‡§ø ‡§ï‡§ø‡§∏ config ‡§®‡•á event ‡§≠‡•á‡§ú‡§æ‡•§ ‡§Ø‡§¶‡§ø SIMPLETUNER_JOB_ID ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã webhook payloads ‡§Æ‡•á‡§Ç job_id null ‡§π‡•ã‡§ó‡§æ‡•§

### `--publishing_config`

- **What**: non‚ÄëHugging Face publishing targets (S3‚Äëcompatible storage, Backblaze B2, Azure Blob Storage, Dropbox) ‡§ï‡•ã ‡§µ‡§∞‡•ç‡§£‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡§æ optional JSON/dict/file path.
- **Why**: `--webhook_config` parsing ‡§ï‡•ã mirror ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø artifacts ‡§ï‡•ã Hub ‡§ï‡•á ‡§¨‡§æ‡§π‡§∞ ‡§≠‡•Ä fan out ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡•á‡•§ Publishing validation ‡§ï‡•á ‡§¨‡§æ‡§¶ main process ‡§™‡§∞ current `output_dir` ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§ö‡§≤‡§§‡§æ ‡§π‡•à‡•§
- **Notes**: Providers `--push_to_hub` ‡§ï‡•á additive ‡§π‡•à‡§Ç‡•§ provider SDKs (‡§ú‡•à‡§∏‡•á `boto3`, `azure-storage-blob`, `dropbox`) ‡§ï‡•ã `.venv` ‡§Æ‡•á‡§Ç install ‡§ï‡§∞‡•á‡§Ç ‡§ú‡§¨ ‡§Ü‡§™ ‡§á‡§®‡•ç‡§π‡•á‡§Ç enable ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§â‡§¶‡§æ‡§π‡§∞‡§£‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è `documentation/publishing/README.md` ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

### `--hub_model_id`

- **What**: Huggingface Hub ‡§Æ‡•â‡§°‡§≤ ‡§î‡§∞ local results directory ‡§ï‡§æ ‡§®‡§æ‡§Æ‡•§
- **Why**: ‡§Ø‡§π ‡§Æ‡§æ‡§® `--output_dir` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§®‡§ø‡§∞‡•ç‡§¶‡§ø‡§∑‡•ç‡§ü ‡§∏‡•ç‡§•‡§æ‡§® ‡§ï‡•á ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ó‡§§ directory ‡§®‡§æ‡§Æ ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§¶‡§ø `--push_to_hub` ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à, ‡§§‡•ã ‡§Ø‡§π‡•Ä Huggingface Hub ‡§™‡§∞ ‡§Æ‡•â‡§°‡§≤ ‡§ï‡§æ ‡§®‡§æ‡§Æ ‡§π‡•ã‡§ó‡§æ‡•§

### `--modelspec_comment`

- **What**: safetensors ‡§´‡§º‡§æ‡§á‡§≤ metadata ‡§Æ‡•á‡§Ç `modelspec.comment` ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç embedded text
- **Default**: None (disabled)
- **Notes**:
  - ‡§¨‡§æ‡§π‡§∞‡•Ä model viewers (ComfyUI, model info tools) ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§æ‡§à ‡§¶‡•á‡§§‡§æ ‡§π‡•à
  - string ‡§Ø‡§æ strings ‡§ï‡•Ä array (newlines ‡§∏‡•á ‡§ú‡•Å‡§°‡§º‡•Ä) ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
  - environment variable substitution ‡§ï‡•á ‡§≤‡§ø‡§è `{env:VAR_NAME}` placeholders support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
  - ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï checkpoint save ‡§ï‡•á ‡§∏‡§Æ‡§Ø current config value ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à

**Example (string)**:
```json
"modelspec_comment": "‡§Æ‡•á‡§∞‡•á custom dataset v2.1 ‡§™‡§∞ trained"
```

**Example (array multi-line ‡§ï‡•á ‡§≤‡§ø‡§è)**:
```json
"modelspec_comment": [
  "Training run: experiment-42",
  "Dataset: custom-portraits-v2",
  "Notes: {env:TRAINING_NOTES}"
]
```

### `--disable_benchmark`

- **What**: step 0 ‡§™‡§∞ base model ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä startup validation/benchmark ‡§ï‡•ã disable ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡•á outputs ‡§Ü‡§™‡§ï‡•Ä trained model validation images ‡§ï‡•á ‡§¨‡§æ‡§è‡§Å ‡§π‡§ø‡§∏‡•ç‡§∏‡•á ‡§Æ‡•á‡§Ç stitched ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§

## üìÇ Data Storage and Management

### `--data_backend_config`

- **What**: ‡§Ü‡§™‡§ï‡•á SimpleTuner dataset ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§ï‡§æ path.
- **Why**: ‡§Ö‡§≤‡§ó‚Äë‡§Ö‡§≤‡§ó storage ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ‡•ã‡§Ç ‡§™‡§∞ ‡§ï‡§à datasets ‡§ï‡•ã ‡§è‡§ï training session ‡§Æ‡•á‡§Ç ‡§ú‡•ã‡§°‡§º‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Example**: ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§ï‡•á ‡§≤‡§ø‡§è [multidatabackend.json.example](/multidatabackend.json.example) ‡§¶‡•á‡§ñ‡•á‡§Ç, ‡§î‡§∞ data loader ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è [‡§Ø‡§π ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º](DATALOADER.md) ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

### `--override_dataset_config`

- **What**: ‡§¶‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡•á ‡§™‡§∞, SimpleTuner cached config ‡§î‡§∞ current values ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§Ö‡§Ç‡§§‡§∞ ‡§ï‡•ã ignore ‡§ï‡§∞‡•á‡§ó‡§æ‡•§
- **Why**: ‡§ï‡§ø‡§∏‡•Ä dataset ‡§™‡§∞ SimpleTuner ‡§™‡§π‡§≤‡•Ä ‡§¨‡§æ‡§∞ ‡§ö‡§≤‡§®‡•á ‡§™‡§∞ ‡§Ø‡§π dataset ‡§ï‡•Ä ‡§π‡§∞ ‡§ö‡•Ä‡§ú‡§º ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§µ‡§æ‡§≤‡§æ cache document ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç dataset config ‡§≠‡•Ä ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§ú‡•à‡§∏‡•á ‡§á‡§∏‡§ï‡•á "crop" ‡§î‡§∞ "resolution" ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§Æ‡§æ‡§®‡•§ ‡§á‡§®‡•ç‡§π‡•á‡§Ç ‡§Æ‡§®‡§Æ‡§æ‡§®‡•á ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§Ø‡§æ ‡§ó‡§≤‡§§‡•Ä ‡§∏‡•á ‡§¨‡§¶‡§≤‡§®‡•á ‡§™‡§∞ training jobs ‡§¨‡•á‡§§‡§∞‡§§‡•Ä‡§¨ ‡§∞‡•Ç‡§™ ‡§∏‡•á crash ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§á‡§∏‡§≤‡§ø‡§è ‡§á‡§∏ parameter ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§® ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§ï‡§ø‡§∏‡•Ä ‡§Ö‡§®‡•ç‡§Ø ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§∏‡§ø‡§´‡§º‡§æ‡§∞‡§ø‡§∂ ‡§π‡•à‡•§

### `--data_backend_sampling`

- **What**: ‡§ï‡§à data backends ‡§ï‡•á ‡§∏‡§æ‡§• sampling ‡§Ö‡§≤‡§ó strategies ‡§∏‡•á ‡§ï‡•Ä ‡§ú‡§æ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§
- **Options**:
  - `uniform` - v0.9.8.1 ‡§î‡§∞ ‡§™‡§π‡§≤‡•á ‡§ï‡§æ behavior ‡§ú‡§π‡§æ‡§Å dataset length consider ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•Ä, ‡§ï‡•á‡§µ‡§≤ manual probability weightings ‡§≤‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§•‡•Ä‡§Ç‡•§
  - `auto-weighting` - ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü behavior ‡§ú‡§π‡§æ‡§Å dataset length ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á ‡§∏‡§≠‡•Ä datasets ‡§ï‡•ã ‡§∏‡§Æ‡§æ‡§® ‡§∞‡•Ç‡§™ ‡§∏‡•á sample ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§§‡§æ‡§ï‡§ø ‡§™‡•Ç‡§∞‡•á data distribution ‡§™‡§∞ uniform sampling ‡§¨‡§®‡•Ä ‡§∞‡§π‡•á‡•§
    - ‡§Ø‡§π ‡§§‡§¨ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à ‡§ú‡§¨ ‡§Ü‡§™‡§ï‡•á datasets ‡§Ö‡§≤‡§ó‚Äë‡§Ö‡§≤‡§ó sizes ‡§ï‡•á ‡§π‡•ã‡§Ç ‡§î‡§∞ ‡§Ü‡§™ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•ã‡§Ç ‡§ï‡§ø ‡§Æ‡•â‡§°‡§≤ ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§∏‡§Æ‡§æ‡§® ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§∏‡•Ä‡§ñ‡•á‡•§
    - ‡§≤‡•á‡§ï‡§ø‡§® Dreambooth images ‡§ï‡•ã regularisation set ‡§ï‡•á ‡§µ‡§ø‡§∞‡•Å‡§¶‡•ç‡§ß ‡§∏‡§π‡•Ä ‡§§‡§∞‡§π sample ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `repeats` ‡§ï‡•ã manually adjust ‡§ï‡§∞‡§®‡§æ **‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï** ‡§π‡•à

### `--vae_cache_scan_behaviour`

- **What**: integrity scan check ‡§ï‡§æ ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞ ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: dataset ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ó‡§≤‡§§ settings training ‡§ï‡•á ‡§ï‡§à ‡§ö‡§∞‡§£‡•ã‡§Ç ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§ú‡•à‡§∏‡•á ‡§Ø‡§¶‡§ø ‡§Ü‡§™ ‡§ó‡§≤‡§§‡•Ä ‡§∏‡•á dataset ‡§∏‡•á `.json` cache files delete ‡§ï‡§∞ ‡§¶‡•á‡§Ç ‡§î‡§∞ data backend config ‡§ï‡•ã aspect‚Äëcrops ‡§ï‡•Ä ‡§ú‡§ó‡§π square images ‡§ï‡•á ‡§≤‡§ø‡§è switch ‡§ï‡§∞ ‡§¶‡•á‡§Ç‡•§ ‡§á‡§∏‡§∏‡•á data cache inconsistent ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡•á `multidatabackend.json` ‡§Æ‡•á‡§Ç `scan_for_errors` ‡§ï‡•ã `true` ‡§∏‡•á‡§ü ‡§ï‡§∞‡§ï‡•á ‡§†‡•Ä‡§ï ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§ú‡§¨ ‡§Ø‡§π scan ‡§ö‡§≤‡§§‡§æ ‡§π‡•à, ‡§§‡•ã ‡§Ø‡§π `--vae_cache_scan_behaviour` ‡§ï‡•á ‡§Æ‡§æ‡§® ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ inconsistency ‡§ï‡•ã resolve ‡§ï‡§∞‡§§‡§æ ‡§π‡•à: `recreate` (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü) offending cache entry ‡§ï‡•ã ‡§π‡§ü‡§æ‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§µ‡§π ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡§®‡§æ‡§à ‡§ú‡§æ ‡§∏‡§ï‡•á, ‡§î‡§∞ `sync` bucket metadata ‡§ï‡•ã ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ‡§ø‡§ï training sample ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∞‡•Ç‡§™ ‡§Ö‡§™‡§°‡•á‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§ø‡§§ ‡§Æ‡§æ‡§®: `recreate`.

### `--dataloader_prefetch`

- **What**: batches ‡§ï‡•ã ahead‚Äëof‚Äëtime retrieve ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§¨‡§°‡§º‡•á batch sizes ‡§ï‡•á ‡§∏‡§æ‡§•, samples disk (‡§Ø‡§π‡§æ‡§Å ‡§§‡§ï ‡§ï‡§ø NVMe) ‡§∏‡•á ‡§≤‡•ã‡§° ‡§π‡•ã‡§®‡•á ‡§™‡§∞ training "pause" ‡§π‡•ã‡§§‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á GPU utilisation metrics ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ Dataloader prefetch ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§™‡•Ç‡§∞‡•á batches ‡§ï‡§æ buffer ‡§≠‡§∞‡§ï‡§∞ ‡§∞‡§ñ‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§µ‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ load ‡§π‡•ã ‡§∏‡§ï‡•á‡§Ç‡•§

> ‚ö†Ô∏è ‡§Ø‡§π ‡§µ‡§æ‡§∏‡•ç‡§§‡§µ ‡§Æ‡•á‡§Ç ‡§ï‡•á‡§µ‡§≤ H100 ‡§Ø‡§æ ‡§¨‡•á‡§π‡§§‡§∞ GPU ‡§™‡§∞ ‡§ï‡§Æ resolution ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à ‡§ú‡§π‡§æ‡§Å I/O bottleneck ‡§¨‡§®‡§§‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§Ö‡§®‡•ç‡§Ø ‡§â‡§™‡§Ø‡•ã‡§ó ‡§Æ‡§æ‡§Æ‡§≤‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§Ø‡§π ‡§Ö‡§®‡§æ‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ú‡§ü‡§ø‡§≤‡§§‡§æ ‡§π‡•à‡•§

### `--dataloader_prefetch_qlen`

- **What**: memory ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡•á ‡§ó‡§è batches ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§¨‡§¢‡§º‡§æ‡§§‡§æ ‡§Ø‡§æ ‡§ò‡§ü‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Why**: dataloader prefetch ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§™‡•ç‡§∞‡§§‡§ø GPU/process 10 entries memory ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ß‡§ø‡§ï ‡§Ø‡§æ ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏ ‡§Æ‡§æ‡§® ‡§ï‡•ã ‡§¨‡§¶‡§≤‡§ï‡§∞ batches ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§æ‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§ï‡•Ä ‡§ú‡§æ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§

### `--compress_disk_cache`

- **What**: VAE ‡§î‡§∞ text embed caches ‡§ï‡•ã disk ‡§™‡§∞ compress ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: DeepFloyd, SD3, ‡§î‡§∞ PixArt ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ T5 encoder ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡•á text embeds ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à ‡§ú‡•ã ‡§õ‡•ã‡§ü‡•á ‡§Ø‡§æ redundant captions ‡§ï‡•á ‡§≤‡§ø‡§è mostly empty space ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ `--compress_disk_cache` ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡•á ‡§∏‡•á space ‡§â‡§™‡§Ø‡•ã‡§ó 75% ‡§§‡§ï ‡§ò‡§ü ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§î‡§∏‡§§‡§® 40% ‡§¨‡§ö‡§§ ‡§ï‡•á ‡§∏‡§æ‡§•‡•§

> ‚ö†Ô∏è ‡§Ü‡§™‡§ï‡•ã ‡§Æ‡•å‡§ú‡•Ç‡§¶‡§æ cache directories ‡§Æ‡•à‡§®‡•ç‡§Ø‡•Å‡§Ö‡§≤ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§π‡§ü‡§æ‡§®‡•á ‡§π‡•ã‡§Ç‡§ó‡•á ‡§§‡§æ‡§ï‡§ø trainer ‡§â‡§®‡•ç‡§π‡•á‡§Ç compression ‡§ï‡•á ‡§∏‡§æ‡§• ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡§®‡§æ ‡§∏‡§ï‡•á‡•§

---

## üåà Image ‡§î‡§∞ Text Processing

‡§ï‡§à ‡§∏‡•á‡§ü‡§ø‡§Ç‡§ó‡•ç‡§∏ [dataloader config](DATALOADER.md) ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§ü ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ø‡•á global ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§Ç‡§ó‡•Ä‡•§

### `--resolution_type`

- **What**: ‡§Ø‡§π SimpleTuner ‡§ï‡•ã ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø `area` size calculations ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•Ä ‡§π‡•à‡§Ç ‡§Ø‡§æ `pixel` edge calculations‡•§ `pixel_area` ‡§ï‡§æ hybrid approach ‡§≠‡•Ä ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ ‡§π‡•à, ‡§ú‡•ã `area` measurements ‡§ï‡•á ‡§≤‡§ø‡§è megapixel ‡§ï‡•Ä ‡§¨‡§ú‡§æ‡§Ø pixel ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Options**:
  - `resolution_type=pixel_area`
    - `resolution` ‡§ï‡§æ ‡§Æ‡§æ‡§® 1024 ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§Ø‡§π internally efficient aspect bucketing ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§ü‡•Ä‡§ï area measurement ‡§Æ‡•á‡§Ç ‡§Æ‡•à‡§™ ‡§π‡•ã‡§ó‡§æ‡•§
    - `1024` ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§Ü‡§ï‡§æ‡§∞: 1024x1024, 1216x832, 832x1216
  - `resolution_type=pixel`
    - dataset ‡§ï‡•Ä ‡§∏‡§≠‡•Ä images ‡§ï‡§æ ‡§õ‡•ã‡§ü‡§æ edge ‡§á‡§∏ resolution ‡§§‡§ï resize ‡§π‡•ã‡§ó‡§æ, ‡§ú‡§ø‡§∏‡§∏‡•á resulting images ‡§¨‡§°‡§º‡•á ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ VRAM ‡§â‡§™‡§Ø‡•ã‡§ó ‡§¨‡§¢‡§º ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
    - `1024` ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§Ü‡§ï‡§æ‡§∞: 1024x1024, 1766x1024, 1024x1766
  - `resolution_type=area`
    - **Deprecated**. ‡§á‡§∏‡§ï‡•Ä ‡§ú‡§ó‡§π `pixel_area` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§

### `--resolution`

- **What**: input image resolution, pixel edge length ‡§Æ‡•á‡§Ç ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§
- **Default**: 1024
- **Note**: ‡§Ø‡§¶‡§ø dataset ‡§Æ‡•á‡§Ç resolution ‡§∏‡•á‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã ‡§Ø‡§π‡•Ä global default ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§ó‡§æ‡•§

### `--validation_resolution`

- **What**: output image resolution, pixels ‡§Æ‡•á‡§Ç; ‡§Ø‡§æ `widthxheight` ‡§´‡§º‡•â‡§∞‡•ç‡§Æ‡•à‡§ü ‡§Æ‡•á‡§Ç, ‡§ú‡•à‡§∏‡•á `1024x1024`‡•§ Multiple resolutions ‡§ï‡•ã comma ‡§∏‡•á ‡§Ö‡§≤‡§ó ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Why**: validation ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§¨‡§®‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä ‡§∏‡§≠‡•Ä images ‡§á‡§∏‡•Ä resolution ‡§™‡§∞ ‡§π‡•ã‡§Ç‡§ó‡•Ä‡•§ ‡§Ø‡§π ‡§§‡§¨ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à ‡§ú‡§¨ ‡§Æ‡•â‡§°‡§≤ ‡§Ö‡§≤‡§ó resolution ‡§™‡§∞ train ‡§π‡•ã ‡§∞‡§π‡§æ ‡§π‡•ã‡•§

### `--validation_method`

- **What**: validation runs ‡§ï‡•à‡§∏‡•á execute ‡§π‡•ã‡§Ç, ‡§Ø‡§π ‡§ö‡•Å‡§®‡•á‡§Ç‡•§
- **Options**: `simpletuner-local` (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü) built‚Äëin pipeline ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à; `external-script` user‚Äëprovided executable ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Why**: training ‡§ï‡•ã local pipeline work ‡§Æ‡•á‡§Ç ‡§∞‡•ã‡§ï‡•á ‡§¨‡§ø‡§®‡§æ validation ‡§ï‡•ã external system ‡§Æ‡•á‡§Ç ‡§π‡•à‡§Ç‡§°‚Äë‡§ë‡§´ ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--validation_external_script`

- **What**: `--validation_method=external-script` ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ executable‡•§ ‡§Ø‡§π shell‚Äëstyle splitting ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§á‡§∏‡§≤‡§ø‡§è command string ‡§ï‡•ã ‡§†‡•Ä‡§ï ‡§∏‡•á quote ‡§ï‡§∞‡•á‡§Ç‡•§
- **Placeholders**: ‡§Ü‡§™ training context ‡§™‡§æ‡§∏ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§® tokens ‡§ï‡•ã embed ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç (`.format` ‡§ï‡•á ‡§∏‡§æ‡§•)‡•§ Missing values ‡§ñ‡§æ‡§≤‡•Ä string ‡§∏‡•á replace ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç ‡§ú‡§¨ ‡§§‡§ï ‡§ï‡§ø ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ ‡§® ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•ã:
  - `{local_checkpoint_path}` ‚Üí `output_dir` ‡§ï‡•á ‡§Ö‡§Ç‡§§‡§∞‡•ç‡§ó‡§§ latest checkpoint directory (‡§ï‡§Æ ‡§∏‡•á ‡§ï‡§Æ ‡§è‡§ï checkpoint ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï)‡•§
  - `{global_step}` ‚Üí current global step.
  - `{tracker_run_name}` ‚Üí `--tracker_run_name` ‡§ï‡§æ ‡§Æ‡§æ‡§®.
  - `{tracker_project_name}` ‚Üí `--tracker_project_name` ‡§ï‡§æ ‡§Æ‡§æ‡§®.
  - `{model_family}` ‚Üí `--model_family` ‡§ï‡§æ ‡§Æ‡§æ‡§®.
  - `{model_type}` / `{lora_type}` ‚Üí model type ‡§î‡§∞ LoRA flavour.
  - `{huggingface_path}` ‚Üí `--hub_model_id` ‡§ï‡§æ ‡§Æ‡§æ‡§® (‡§Ø‡§¶‡§ø ‡§∏‡•á‡§ü ‡§π‡•ã)‡•§
  - `{remote_checkpoint_path}` ‚Üí last upload ‡§ï‡§æ remote URL (validation hook ‡§ï‡•á ‡§≤‡§ø‡§è empty)‡•§
  - ‡§ï‡•ã‡§à ‡§≠‡•Ä `validation_*` config value (‡§â‡§¶‡§æ., `validation_num_inference_steps`, `validation_guidance`, `validation_noise_scheduler`).
- **Example**: `--validation_external_script="/opt/tools/validate.sh {local_checkpoint_path} {global_step}"`

### `--validation_external_background`

- **What**: ‡§∏‡•á‡§ü ‡§π‡•ã‡§®‡•á ‡§™‡§∞ `--validation_external_script` background ‡§Æ‡•á‡§Ç launch ‡§π‡•ã‡§§‡§æ ‡§π‡•à (fire‚Äëand‚Äëforget)‡•§
- **Why**: external script ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§º‡§æ‡§∞ ‡§ï‡§ø‡§è ‡§¨‡§ø‡§®‡§æ training ‡§ö‡§≤‡§§‡•Ä ‡§∞‡§π‡§§‡•Ä ‡§π‡•à; ‡§á‡§∏ ‡§Æ‡•ã‡§° ‡§Æ‡•á‡§Ç exit codes check ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§§‡•á‡•§

### `--post_upload_script`

- **What**: ‡§π‡§∞ publishing provider ‡§î‡§∞ Hugging Face Hub upload (final model ‡§î‡§∞ checkpoint uploads) ‡§ï‡•á ‡§¨‡§æ‡§¶ optional executable ‡§ö‡§≤‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π asynchronous ‡§ö‡§≤‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø training block ‡§® ‡§π‡•ã‡•§
- **Placeholders**: `--validation_external_script` ‡§ú‡•à‡§∏‡•á replacements, ‡§∏‡§æ‡§• ‡§π‡•Ä `{remote_checkpoint_path}` (provider ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§≤‡•å‡§ü‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ URI) ‡§§‡§æ‡§ï‡§ø ‡§Ü‡§™ published URL downstream systems ‡§ï‡•ã ‡§≠‡•á‡§ú ‡§∏‡§ï‡•á‡§Ç‡•§
- **Notes**:
  - scripts ‡§π‡§∞ provider/upload ‡§™‡§∞ ‡§ö‡§≤‡§§‡•Ä ‡§π‡•à‡§Ç; errors ‡§≤‡•â‡§ó ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® training ‡§∞‡•ã‡§ï‡§§‡•á ‡§®‡§π‡•Ä‡§Ç‡•§
  - ‡§ú‡§¨ ‡§ï‡•ã‡§à remote upload ‡§® ‡§π‡•ã ‡§§‡§¨ ‡§≠‡•Ä scripts invoke ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç, ‡§§‡§æ‡§ï‡§ø ‡§Ü‡§™ local automation (‡§â‡§¶‡§æ., ‡§¶‡•Ç‡§∏‡§∞‡•á GPU ‡§™‡§∞ inference) ‡§ö‡§≤‡§æ ‡§∏‡§ï‡•á‡§Ç‡•§
  - SimpleTuner ‡§Ü‡§™‡§ï‡•Ä script ‡§ï‡•á ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ingest ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡§æ ‚Äî metrics ‡§Ø‡§æ images ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡•Ä‡§ß‡•á ‡§Ö‡§™‡§®‡•á tracker ‡§™‡§∞ ‡§≤‡•â‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- **Example**:
  ```bash
  --post_upload_script='/opt/hooks/notify.sh {remote_checkpoint_path} {tracker_project_name} {tracker_run_name}'
  ```
  ‡§ú‡§π‡§æ‡§Å `/opt/hooks/notify.sh` ‡§Ü‡§™‡§ï‡•á tracking system ‡§ï‡•ã post ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à:
  ```bash
  #!/usr/bin/env bash
  REMOTE="$1"
  PROJECT="$2"
  RUN="$3"
  curl -X POST "https://tracker.internal/api/runs/${PROJECT}/${RUN}/artifacts" \
       -H "Content-Type: application/json" \
       -d "{\"remote_uri\":\"${REMOTE}\"}"
  ```
- **Working samples**:
  - `simpletuner/examples/external-validation/replicate_post_upload.py` ‡§è‡§ï Replicate hook ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à ‡§ú‡•ã `{remote_checkpoint_path}`, `{model_family}`, `{model_type}`, `{lora_type}`, ‡§î‡§∞ `{huggingface_path}` consume ‡§ï‡§∞‡§ï‡•á uploads ‡§ï‡•á ‡§¨‡§æ‡§¶ inference ‡§ü‡•ç‡§∞‡§ø‡§ó‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - `simpletuner/examples/external-validation/wavespeed_post_upload.py` ‡§µ‡§π‡•Ä placeholders ‡§ï‡•á ‡§∏‡§æ‡§• WaveSpeed hook ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ WaveSpeed ‡§ï‡•Ä async polling ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - `simpletuner/examples/external-validation/fal_post_upload.py` fal.ai Flux LoRA hook ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à (`FAL_KEY` ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï)‡•§
  - `simpletuner/examples/external-validation/use_second_gpu.py` secondary GPU ‡§™‡§∞ Flux LoRA inference ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ remote uploads ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ ‡§≠‡•Ä ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--post_checkpoint_script`

- **What**: ‡§π‡§∞ checkpoint directory disk ‡§™‡§∞ ‡§≤‡§ø‡§ñ‡•á ‡§ú‡§æ‡§®‡•á ‡§ï‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§¨‡§æ‡§¶ ‡§ö‡§≤‡§æ‡§Ø‡§æ ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ executable (uploads ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á)‡•§ main process ‡§™‡§∞ asynchronous ‡§ö‡§≤‡§§‡§æ ‡§π‡•à‡•§
- **Placeholders**: `--validation_external_script` ‡§ú‡•à‡§∏‡•á replacements, ‡§ú‡§ø‡§®‡§Æ‡•á‡§Ç `{local_checkpoint_path}`, `{global_step}`, `{tracker_run_name}`, `{tracker_project_name}`, `{model_family}`, `{model_type}`, `{lora_type}`, `{huggingface_path}` ‡§î‡§∞ ‡§ï‡•ã‡§à ‡§≠‡•Ä `validation_*` config value ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç‡•§ `{remote_checkpoint_path}` ‡§á‡§∏ hook ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ñ‡§æ‡§≤‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Notes**:
  - Scheduled, manual, ‡§î‡§∞ rolling checkpoints ‡§™‡§∞ ‡§Ø‡§π ‡§§‡§¨ fire ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ local save ‡§™‡•Ç‡§∞‡§æ ‡§π‡•ã ‡§ú‡§æ‡§è‡•§
  - Local automation (‡§¶‡•Ç‡§∏‡§∞‡•á volume ‡§™‡§∞ copy, eval jobs ‡§ö‡§≤‡§æ‡§®‡§æ) ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à, uploads ‡§ï‡•á ‡§ñ‡§§‡•ç‡§Æ ‡§π‡•ã‡§®‡•á ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§º‡§æ‡§∞ ‡§ï‡§ø‡§è ‡§¨‡§ø‡§®‡§æ‡•§
- **Example**:
  ```bash
  --post_checkpoint_script='/opt/hooks/run_eval.sh {local_checkpoint_path} {global_step}'
  ```


### `--validation_adapter_path`

- **What**: scheduled validations ‡§ö‡§≤‡§æ‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§è‡§ï single LoRA adapter ‡§≤‡•ã‡§° ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Formats**:
  - Hugging Face repo: `org/repo` ‡§Ø‡§æ `org/repo:weight_name.safetensors` (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü `pytorch_lora_weights.safetensors`).
  - Local file ‡§Ø‡§æ directory path ‡§ú‡•ã safetensors adapter ‡§ï‡•Ä ‡§ì‡§∞ ‡§á‡§∂‡§æ‡§∞‡§æ ‡§ï‡§∞‡•á‡•§
- **Notes**:
  - `--validation_adapter_config` ‡§ï‡•á ‡§∏‡§æ‡§• mutually exclusive; ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§¶‡•á‡§®‡•á ‡§™‡§∞ error ‡§Ü‡§§‡§æ ‡§π‡•à‡•§
  - adapter ‡§ï‡•á‡§µ‡§≤ validation runs ‡§ï‡•á ‡§≤‡§ø‡§è attach ‡§π‡•ã‡§§‡§æ ‡§π‡•à (baseline training weights untouched ‡§∞‡§π‡§§‡•á ‡§π‡•à‡§Ç)‡•§

### `--validation_adapter_name`

- **What**: `--validation_adapter_path` ‡§∏‡•á ‡§≤‡•ã‡§° ‡§ï‡§ø‡§è ‡§ó‡§è ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä adapter ‡§ï‡•á ‡§≤‡§ø‡§è optional identifier.
- **Why**: logs/Web UI ‡§Æ‡•á‡§Ç adapter run ‡§ï‡•ã label ‡§ï‡§∞‡§®‡•á ‡§î‡§∞ ‡§ï‡§à adapters sequentially test ‡§π‡•ã‡§®‡•á ‡§™‡§∞ predictable names ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è‡•§

### `--validation_adapter_strength`

- **What**: ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä adapter enable ‡§π‡•ã‡§®‡•á ‡§™‡§∞ strength multiplier (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü `1.0`).
- **Why**: training state ‡§¨‡§¶‡§≤‡•á ‡§¨‡§ø‡§®‡§æ validation ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§π‡§≤‡•ç‡§ï‡§æ/‡§§‡•á‡§ú‡§º LoRA scaling sweep ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à; ‡§∂‡•Ç‡§®‡•ç‡§Ø ‡§∏‡•á ‡§¨‡§°‡§º‡§æ ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§Æ‡§æ‡§® ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§

### `--validation_adapter_mode`

- **Choices**: `adapter_only`, `comparison`, `none`
- **What**:
  - `adapter_only`: ‡§ï‡•á‡§µ‡§≤ ‡§Ö‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä adapter ‡§ï‡•á ‡§∏‡§æ‡§• validations ‡§ö‡§≤‡§æ‡§è‡§Å‡•§
  - `comparison`: base‚Äëmodel ‡§î‡§∞ adapter‚Äëenabled samples ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ú‡§®‡§∞‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç ‡§§‡§æ‡§ï‡§ø side‚Äëby‚Äëside ‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§π‡•ã ‡§∏‡§ï‡•á‡•§
  - `none`: adapter attach ‡§ï‡§∞‡§®‡§æ ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§Ç (CLI flags ‡§π‡§ü‡§æ‡§è ‡§¨‡§ø‡§®‡§æ ‡§´‡•Ä‡§ö‡§∞ disable ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä)‡•§

### `--validation_adapter_config`

- **What**: multiple validation adapter combinations ‡§ï‡•ã iterate ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è JSON file ‡§Ø‡§æ inline JSON.
- **Format**: entries ‡§ï‡•Ä array ‡§Ø‡§æ `runs` array ‡§µ‡§æ‡§≤‡§æ object‡•§ ‡§π‡§∞ entry ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à:
  - `label`: logs/UI ‡§Æ‡•á‡§Ç ‡§¶‡§ø‡§ñ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ friendly name.
  - `path`: Hugging Face repo ID ‡§Ø‡§æ local path (`--validation_adapter_path` ‡§ú‡•à‡§∏‡§æ format)‡•§
  - `adapter_name`: ‡§™‡•ç‡§∞‡§§‡§ø adapter optional identifier.
  - `strength`: optional scalar override.
  - `adapters`/`paths`: ‡§è‡§ï ‡§π‡•Ä run ‡§Æ‡•á‡§Ç multiple adapters ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è objects/strings ‡§ï‡•Ä array.
- **Notes**:
  - ‡§Ø‡§π ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§π‡•ã‡§®‡•á ‡§™‡§∞ single‚Äëadapter options (`--validation_adapter_path`, `--validation_adapter_name`, `--validation_adapter_strength`, `--validation_adapter_mode`) UI ‡§Æ‡•á‡§Ç ignore/disable ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§
  - ‡§π‡§∞ run ‡§ï‡•ã ‡§è‡§ï‚Äë‡§è‡§ï ‡§ï‡§∞‡§ï‡•á load ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§Ö‡§ó‡§≤‡§æ ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§™‡•Ç‡§∞‡•Ä ‡§§‡§∞‡§π detach ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§

### `--validation_preview`

- **What**: Tiny AutoEncoders ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á diffusion sampling ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® intermediate validation previews stream ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **Default**: False
- **Why**: validation images ‡§ï‡•á generate ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® real‚Äëtime preview ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; lightweight Tiny AutoEncoder models ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ decode ‡§π‡•ã‡§ï‡§∞ webhook callbacks ‡§∏‡•á ‡§≠‡•á‡§ú‡•á ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§∏‡§∏‡•á ‡§Ü‡§™ ‡§™‡•Ç‡§∞‡•Ä generation ‡§ï‡§æ ‡§á‡§Ç‡§§‡§ú‡§º‡§æ‡§∞ ‡§ï‡§ø‡§è ‡§¨‡§ø‡§®‡§æ step‚Äëby‚Äëstep progress ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Notes**:
  - ‡§ï‡•á‡§µ‡§≤ Tiny AutoEncoder support ‡§µ‡§æ‡§≤‡•á model families ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß (‡§â‡§¶‡§æ., Flux, SDXL, SD3)
  - preview images ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è webhook configuration ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï
  - previews ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¨‡§æ‡§∞ decode ‡§π‡•ã‡§Ç, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `--validation_preview_steps` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç

### `--validation_preview_steps`

- **What**: validation previews decode ‡§î‡§∞ stream ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ interval
- **Default**: 1
- **Why**: validation sampling ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® intermediate latents ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¨‡§æ‡§∞ decode ‡§π‡•ã‡§Ç, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§â‡§ö‡•ç‡§ö ‡§Æ‡§æ‡§® (‡§â‡§¶‡§æ. 3) Tiny AutoEncoder ‡§ï‡§æ overhead ‡§ò‡§ü‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§π‡§∞ N steps ‡§™‡§∞ decode ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Example**: `--validation_num_inference_steps=20` ‡§î‡§∞ `--validation_preview_steps=5` ‡§ï‡•á ‡§∏‡§æ‡§•, generation process ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® 4 preview images ‡§Æ‡§ø‡§≤‡•á‡§Ç‡§ó‡•Ä (steps 5, 10, 15, 20 ‡§™‡§∞)‡•§

### `--evaluation_type`

- **What**: validations ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® generated images ‡§ï‡•Ä CLIP evaluation ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: CLIP scores validation prompt ‡§ï‡•á ‡§∏‡§æ‡§• generated image features ‡§ï‡•Ä ‡§¶‡•Ç‡§∞‡•Ä ‡§®‡§ø‡§ï‡§æ‡§≤‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§∏‡§∏‡•á prompt adherence ‡§Æ‡•á‡§Ç ‡§∏‡•Å‡§ß‡§æ‡§∞ ‡§ï‡§æ ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§Æ‡§ø‡§≤ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡§æ‡§∞‡•ç‡§•‡§ï ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ‡•ã‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡§°‡§º‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Æ‡•á‡§Ç validation prompts ‡§ö‡§æ‡§π‡§ø‡§è‡•§
- **Options**: "none" ‡§Ø‡§æ "clip"
- **Scheduling**: step‚Äëbased scheduling ‡§ï‡•á ‡§≤‡§ø‡§è `--eval_steps_interval` ‡§Ø‡§æ epoch‚Äëbased scheduling ‡§ï‡•á ‡§≤‡§ø‡§è `--eval_epoch_interval` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç (fractions ‡§ú‡•à‡§∏‡•á `0.5` ‡§™‡•ç‡§∞‡§§‡§ø epoch ‡§ï‡§à ‡§¨‡§æ‡§∞ ‡§ö‡§≤‡•á‡§Ç‡§ó‡•á)‡•§ ‡§Ø‡§¶‡§ø ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§∏‡•á‡§ü ‡§π‡•ã‡§Ç, trainer warning ‡§≤‡•â‡§ó ‡§ï‡§∞‡•á‡§ó‡§æ ‡§î‡§∞ ‡§¶‡•ã‡§®‡•ã‡§Ç schedules ‡§ö‡§≤‡§æ‡§è‡§ó‡§æ‡•§

### `--eval_loss_disable`

- **What**: validation ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® evaluation loss ‡§ó‡§£‡§®‡§æ disable ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: ‡§ú‡§¨ eval dataset ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§π‡•ã, loss ‡§∏‡•ç‡§µ‡§§‡§É ‡§ó‡§£‡§®‡§æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§¶‡§ø CLIP evaluation ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•à, ‡§§‡•ã ‡§¶‡•ã‡§®‡•ã‡§Ç ‡§ö‡§≤‡•á‡§Ç‡§ó‡•á‡•§ ‡§Ø‡§π flag eval loss ‡§ï‡•ã disable ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à ‡§ú‡§¨‡§ï‡§ø CLIP evaluation ‡§ö‡§æ‡§≤‡•Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•à‡•§

### `--validation_using_datasets`

- **What**: pure text-to-image generation ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø training datasets ‡§∏‡•á images validation ‡§ï‡•á ‡§≤‡§ø‡§è use ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: image-to-image (img2img) ‡§Ø‡§æ image-to-video (i2v) validation mode enable ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡§π‡§æ‡§Å model training images ‡§ï‡•ã conditioning inputs ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç use ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à:
  - Edit/inpainting models test ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç input images ‡§ö‡§æ‡§π‡§ø‡§è
  - Model image structure ‡§ï‡•ã ‡§ï‡§ø‡§§‡§®‡§æ preserve ‡§ï‡§∞‡§§‡§æ ‡§π‡•à evaluate ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è
  - Dual text-to-image AND image-to-image workflows support ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á models ‡§ï‡•á ‡§≤‡§ø‡§è (‡§ú‡•à‡§∏‡•á, Flux2, LTXVideo2)
  - **I2V video models** (HunyuanVideo, WAN, Kandinsky5Video): image dataset ‡§∏‡•á images ‡§ï‡•ã video generation validation ‡§ï‡•á ‡§≤‡§ø‡§è first-frame conditioning input ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç use ‡§ï‡§∞‡•á‡§Ç
- **Notes**:
  - Model ‡§Æ‡•á‡§Ç `IMG2IMG` ‡§Ø‡§æ `IMG2VIDEO` pipeline registered ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è
  - `--eval_dataset_id` ‡§ï‡•á ‡§∏‡§æ‡§• combine ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç specific dataset ‡§∏‡•á images ‡§≤‡•á‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è
  - i2v models ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ø‡§π training ‡§Æ‡•á‡§Ç use ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä complex conditioning dataset pairing setup ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ simple image dataset validation ‡§ï‡•á ‡§≤‡§ø‡§è use ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à
  - Denoising strength normal validation timestep settings ‡§∏‡•á control ‡§π‡•ã‡§§‡•Ä ‡§π‡•à

### `--eval_dataset_id`

- **What**: Evaluation/validation image sourcing ‡§ï‡•á ‡§≤‡§ø‡§è specific dataset ID‡•§
- **Why**: `--validation_using_datasets` ‡§Ø‡§æ conditioning-based validation use ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø, ‡§Ø‡§π control ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡•å‡§® ‡§∏‡§æ dataset input images provide ‡§ï‡§∞‡•á:
  - ‡§á‡§∏ option ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ, images ‡§∏‡§≠‡•Ä training datasets ‡§∏‡•á randomly select ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç
  - ‡§á‡§∏ option ‡§ï‡•á ‡§∏‡§æ‡§•, ‡§ï‡•á‡§µ‡§≤ specified dataset validation inputs ‡§ï‡•á ‡§≤‡§ø‡§è use ‡§π‡•ã‡§§‡§æ ‡§π‡•à
- **Notes**:
  - Dataset ID ‡§Ü‡§™‡§ï‡•á dataloader config ‡§Æ‡•á‡§Ç configured dataset ‡§∏‡•á match ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è
  - Dedicated eval dataset use ‡§ï‡§∞‡§ï‡•á consistent evaluation maintain ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è useful
  - Conditioning models ‡§ï‡•á ‡§≤‡§ø‡§è, dataset ‡§ï‡§æ conditioning data (‡§Ø‡§¶‡§ø ‡§π‡•ã) ‡§≠‡•Ä use ‡§π‡•ã‡§ó‡§æ

---

## Conditioning ‡§î‡§∞ Validation Modes ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§®‡§æ

SimpleTuner conditioning inputs (reference images, control signals, ‡§Ü‡§¶‡§ø) use ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á models ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•Ä‡§® ‡§Æ‡•Å‡§ñ‡•ç‡§Ø paradigms support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à:

### 1. Models ‡§ú‡•ã Conditioning REQUIRE ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç

‡§ï‡•Å‡§õ models conditioning inputs ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ function ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á:

- **Flux Kontext**: Edit-style training ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§Æ‡•á‡§∂‡§æ reference images ‡§ö‡§æ‡§π‡§ø‡§è
- **ControlNet training**: Control signal images require ‡§ï‡§∞‡§§‡§æ ‡§π‡•à

‡§á‡§® models ‡§ï‡•á ‡§≤‡§ø‡§è, conditioning dataset mandatory ‡§π‡•à‡•§ WebUI conditioning options ‡§ï‡•ã required ‡§¶‡§ø‡§ñ‡§æ‡§è‡§ó‡•Ä, ‡§î‡§∞ training ‡§á‡§®‡§ï‡•á ‡§¨‡§ø‡§®‡§æ fail ‡§π‡•ã‡§ó‡•Ä‡•§

### 2. Models ‡§ú‡•ã Optional Conditioning SUPPORT ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç

‡§ï‡•Å‡§õ models text-to-image AND image-to-image ‡§¶‡•ã‡§®‡•ã‡§Ç modes ‡§Æ‡•á‡§Ç operate ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç:

- **Flux2**: Optional reference images ‡§ï‡•á ‡§∏‡§æ‡§• dual T2I/I2I training support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **LTXVideo2**: Optional first-frame conditioning ‡§ï‡•á ‡§∏‡§æ‡§• T2V ‡§î‡§∞ I2V (image-to-video) ‡§¶‡•ã‡§®‡•ã‡§Ç support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **LongCat-Video**: Optional frame conditioning support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **HunyuanVideo i2v**: First-frame conditioning ‡§ï‡•á ‡§∏‡§æ‡§• I2V support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (flavours: `i2v-480p`, `i2v-720p`, ‡§Ü‡§¶‡§ø)
- **WAN i2v**: First-frame conditioning ‡§ï‡•á ‡§∏‡§æ‡§• I2V support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **Kandinsky5Video i2v**: First-frame conditioning ‡§ï‡•á ‡§∏‡§æ‡§• I2V support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à

‡§á‡§® models ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ü‡§™ conditioning datasets ADD ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç ‡§™‡§∞ ‡§ú‡§∞‡•Ç‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç‡•§ WebUI conditioning options ‡§ï‡•ã optional ‡§¶‡§ø‡§ñ‡§æ‡§è‡§ó‡•Ä‡•§

**I2V Validation Shortcut**: i2v video models ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ü‡§™ `--validation_using_datasets` ‡§ï‡•ã image dataset (via `--eval_dataset_id` specified) ‡§ï‡•á ‡§∏‡§æ‡§• use ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç validation conditioning images directly ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è, training ‡§Æ‡•á‡§Ç use ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•Ä full conditioning dataset pairing setup ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ‡•§

### 3. Validation Modes

| Mode | Flag | Behavior |
|------|------|----------|
| **Text-to-Image/Video** | (default) | ‡§ï‡•á‡§µ‡§≤ text prompts ‡§∏‡•á generate |
| **Dataset-based (img2img)** | `--validation_using_datasets` | Datasets ‡§∏‡•á images partially denoise |
| **Dataset-based (i2v)** | `--validation_using_datasets` | i2v video models ‡§ï‡•á ‡§≤‡§ø‡§è, images ‡§ï‡•ã first-frame conditioning ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç use |
| **Conditioning-based** | (auto ‡§ú‡§¨ conditioning configured ‡§π‡•ã) | Validation ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® conditioning inputs use |

**Modes combine ‡§ï‡§∞‡§®‡§æ**: ‡§ú‡§¨ model conditioning support ‡§ï‡§∞‡§§‡§æ ‡§π‡•à AND `--validation_using_datasets` enabled ‡§π‡•à:
- Validation system datasets ‡§∏‡•á images ‡§≤‡•á‡§§‡§æ ‡§π‡•à
- ‡§Ø‡§¶‡§ø ‡§â‡§® datasets ‡§Æ‡•á‡§Ç conditioning data ‡§π‡•à, ‡§§‡•ã automatically use ‡§π‡•ã‡§§‡§æ ‡§π‡•à
- `--eval_dataset_id` use ‡§ï‡§∞‡•á‡§Ç control ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•å‡§® ‡§∏‡§æ dataset inputs provide ‡§ï‡§∞‡•á

**I2V models ‡§ï‡•á ‡§∏‡§æ‡§• `--validation_using_datasets`**: i2v video models (HunyuanVideo, WAN, Kandinsky5Video) ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§Ø‡§π flag enable ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ validation ‡§ï‡•á ‡§≤‡§ø‡§è simple image dataset use ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ Images validation videos generate ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è first-frame conditioning inputs ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç use ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡§Ç, complex conditioning dataset pairing setup ‡§ï‡•Ä ‡§ú‡§∞‡•Ç‡§∞‡§§ ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ‡•§

### Conditioning Data Types

Different models different conditioning data expect ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç:

| Type | Models | Dataset Setting |
|------|--------|-----------------|
| `conditioning` | ControlNet, Control | Dataset config ‡§Æ‡•á‡§Ç `type: conditioning` |
| `image` | Flux Kontext | `type: image` (standard image dataset) |
| `latents` | Flux, Flux2 | Conditioning automatically VAE-encoded ‡§π‡•ã‡§§‡§æ ‡§π‡•à |

---

### `--caption_strategy`

- **What**: image captions derive ‡§ï‡§∞‡§®‡•á ‡§ï‡•Ä ‡§∞‡§£‡§®‡•Ä‡§§‡§ø‡•§ **Choices**: `textfile`, `filename`, `parquet`, `instanceprompt`
- **Why**: training images ‡§ï‡•á captions ‡§ï‡•à‡§∏‡•á ‡§¨‡§®‡§æ‡§è ‡§ú‡§æ‡§è‡§Å, ‡§Ø‡§π ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - `textfile` image ‡§ï‡•á ‡§∏‡§Æ‡§æ‡§® ‡§´‡§º‡§æ‡§á‡§≤‚Äë‡§®‡§æ‡§Æ ‡§µ‡§æ‡§≤‡•Ä `.txt` ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•á contents ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§ó‡§æ
  - `filename` ‡§´‡§º‡§æ‡§á‡§≤‚Äë‡§®‡§æ‡§Æ ‡§ï‡•ã ‡§ï‡•Å‡§õ cleanup ‡§ï‡§∞‡§ï‡•á caption ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§ó‡§æ‡•§
  - `parquet` dataset ‡§Æ‡•á‡§Ç parquet ‡§´‡§º‡§æ‡§á‡§≤ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ `caption` column ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§ó‡§æ ‡§ú‡§¨ ‡§§‡§ï `parquet_caption_column` ‡§® ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•ã‡•§ ‡§∏‡§≠‡•Ä captions ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§π‡•ã‡§®‡•á ‡§ö‡§æ‡§π‡§ø‡§è ‡§ú‡§¨ ‡§§‡§ï `parquet_fallback_caption_column` ‡§® ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•ã‡•§
  - `instanceprompt` dataset config ‡§Æ‡•á‡§Ç `instance_prompt` ‡§Æ‡§æ‡§® ‡§ï‡•ã ‡§π‡§∞ image ‡§ï‡•á prompt ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§ó‡§æ‡•§

### `--conditioning_multidataset_sampling` {#--conditioning_multidataset_sampling}

- **What**: multiple conditioning datasets ‡§∏‡•á sampling ‡§ï‡•à‡§∏‡•á ‡§ï‡•Ä ‡§ú‡§æ‡§è‡•§ **Choices**: `combined`, `random`
- **Why**: multiple conditioning datasets (‡§â‡§¶‡§æ., multiple reference images ‡§Ø‡§æ control signals) ‡§ï‡•á ‡§∏‡§æ‡§• training ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Ø‡§π ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§â‡§®‡•ç‡§π‡•á‡§Ç ‡§ï‡•à‡§∏‡•á ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§è:
  - `combined` conditioning inputs ‡§ï‡•ã stitch ‡§ï‡§∞‡§ï‡•á training ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§∏‡§æ‡§• ‡§¶‡§ø‡§ñ‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π multi‚Äëimage compositing tasks ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à‡•§
  - `random` ‡§π‡§∞ sample ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï conditioning dataset ‡§∞‡•à‡§Ç‡§°‡§Æ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§ö‡•Å‡§®‡§§‡§æ ‡§π‡•à, training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® conditions ‡§¨‡§¶‡§≤‡§§‡•á ‡§π‡•Å‡§è‡•§
- **Note**: `combined` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§Ü‡§™ conditioning datasets ‡§™‡§∞ ‡§Ö‡§≤‡§ó `captions` ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§ø‡§§ ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á; source dataset ‡§ï‡•á captions ‡§π‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **See also**: multiple conditioning datasets ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è [DATALOADER.md](DATALOADER.md#conditioning_data) ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

---

## üéõ Training Parameters

### `--num_train_epochs`

- **What**: training epochs ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ (‡§ï‡§ø‡§§‡§®‡•Ä ‡§¨‡§æ‡§∞ ‡§∏‡§≠‡•Ä images ‡§¶‡•á‡§ñ‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç)‡•§ ‡§á‡§∏‡•á 0 ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ `--max_train_steps` ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§π‡•à‡•§
- **Why**: image repeats ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§§‡§Ø ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡•ã training duration ‡§ï‡•ã ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ö‡§ß‡§ø‡§ï epochs ‡§Ü‡§Æ ‡§§‡•å‡§∞ ‡§™‡§∞ overfitting ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§¨‡§®‡§§‡•á ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§™‡§ï‡•á concepts ‡§∏‡•Ä‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§â‡§ö‡§ø‡§§ ‡§Æ‡§æ‡§® 5 ‡§∏‡•á 50 ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

### `--max_train_steps`

- **What**: ‡§á‡§§‡§®‡•á training steps ‡§ï‡•á ‡§¨‡§æ‡§¶ training ‡§¨‡§Ç‡§¶ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ 0 ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ `--num_train_epochs` ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§π‡•à‡•§
- **Why**: training ‡§ï‡•ã ‡§õ‡•ã‡§ü‡§æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä‡•§

### `--ignore_final_epochs`

- **What**: ‡§Ö‡§Ç‡§§‡§ø‡§Æ ‡§ó‡§ø‡§®‡•á ‡§ó‡§è epochs ‡§ï‡•ã ignore ‡§ï‡§∞‡§ï‡•á `--max_train_steps` ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Why**: dataloader length ‡§¨‡§¶‡§≤‡§®‡•á ‡§™‡§∞ epoch calculation ‡§¨‡§¶‡§≤ ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à ‡§î‡§∞ training ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§ñ‡§§‡•ç‡§Æ ‡§π‡•ã ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§π ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§Ö‡§Ç‡§§‡§ø‡§Æ epochs ‡§ï‡•ã ignore ‡§ï‡§∞‡§ï‡•á `--max_train_steps` ‡§§‡§ï training ‡§ú‡§æ‡§∞‡•Ä ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à‡•§

### `--learning_rate`

- **What**: ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ warmup ‡§ï‡•á ‡§¨‡§æ‡§¶ initial learning rate‡•§
- **Why**: learning rate gradient updates ‡§ï‡•á ‡§≤‡§ø‡§è ‡§è‡§ï ‡§§‡§∞‡§π ‡§ï‡§æ "step size" ‡§π‡•à ‚Äî ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ß‡§ø‡§ï ‡§π‡•ã‡§®‡•á ‡§™‡§∞ solution ‡§∏‡•á ‡§Ü‡§ó‡•á ‡§®‡§ø‡§ï‡§≤ ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç, ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ideal solution ‡§§‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§™‡§π‡•Å‡§Å‡§ö‡§§‡•á‡•§ `full` tune ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§Æ‡§æ‡§® `1e-7` ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ `1e-6` ‡§§‡§ï ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§ú‡§¨‡§ï‡§ø `lora` tuning ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ `1e-5` ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ `1e-3` ‡§§‡§ï ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§â‡§ö‡•ç‡§ö learning rate ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ EMA network ‡§î‡§∞ warmup ‡§≤‡§æ‡§≠‡§¶‡§æ‡§Ø‡§ï ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‚Äî ‡§¶‡•á‡§ñ‡•á‡§Ç `--use_ema`, `--lr_warmup_steps`, ‡§î‡§∞ `--lr_scheduler`‡•§

### `--lr_scheduler`

- **What**: ‡§∏‡§Æ‡§Ø ‡§ï‡•á ‡§∏‡§æ‡§• learning rate ‡§ï‡•à‡§∏‡•á scale ‡§π‡•ã‡•§
- **Choices**: constant, constant_with_warmup, cosine, cosine_with_restarts, **polynomial** (‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§ø‡§§), linear
- **Why**: loss landscape ‡§ï‡•ã explore ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è learning rate ‡§ï‡•ã ‡§∏‡§Æ‡§Ø‚Äë‡§∏‡§Æ‡§Ø ‡§™‡§∞ ‡§¨‡§¶‡§≤‡§®‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§π‡•à‡•§ cosine schedule ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á training ‡§¶‡•ã extremes ‡§ï‡•á ‡§¨‡•Ä‡§ö smooth ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§ö‡§≤‡§§‡•Ä ‡§π‡•à‡•§ constant learning rate ‡§Æ‡•á‡§Ç ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§¨‡§π‡•Å‡§§ ‡§ä‡§Å‡§ö‡§æ ‡§Ø‡§æ ‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ ‡§Æ‡§æ‡§® ‡§ö‡•Å‡§® ‡§≤‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á divergence (‡§¨‡§π‡•Å‡§§ ‡§ä‡§Å‡§ö‡§æ) ‡§Ø‡§æ local minima ‡§Æ‡•á‡§Ç ‡§´‡§Å‡§∏‡§®‡§æ (‡§¨‡§π‡•Å‡§§ ‡§ï‡§Æ) ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ polynomial schedule warmup ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§¨‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§∞‡§π‡§§‡§æ ‡§π‡•à, ‡§ú‡§π‡§æ‡§Å ‡§Ø‡§π ‡§ß‡•Ä‡§∞‡•á‚Äë‡§ß‡•Ä‡§∞‡•á `learning_rate` ‡§§‡§ï ‡§™‡§π‡•Å‡§Å‡§ö‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§´‡§ø‡§∞ ‡§ß‡•Ä‡§∞‡•á‚Äë‡§ß‡•Ä‡§∞‡•á `--lr_end` ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§™‡§π‡•Å‡§Å‡§ö‡§§‡§æ ‡§π‡•à‡•§

### `--optimizer`

- **What**: training ‡§ï‡•á ‡§≤‡§ø‡§è optimizer‡•§
- **Choices**: adamw_bf16, ao-adamw8bit, ao-adamw4bit, ao-adamfp8, ao-adamwfp8, adamw_schedulefree, adamw_schedulefree+aggressive, adamw_schedulefree+no_kahan, optimi-stableadamw, optimi-adamw, optimi-lion, optimi-radam, optimi-ranger, optimi-adan, optimi-adam, optimi-sgd, soap, bnb-adagrad, bnb-adagrad8bit, bnb-adam, bnb-adam8bit, bnb-adamw, bnb-adamw8bit, bnb-adamw-paged, bnb-adamw8bit-paged, bnb-lion, bnb-lion8bit, bnb-lion-paged, bnb-lion8bit-paged, bnb-ademamix, bnb-ademamix8bit, bnb-ademamix-paged, bnb-ademamix8bit-paged, prodigy

> Note: ‡§ï‡•Å‡§õ optimisers non‚ÄëNVIDIA hardware ‡§™‡§∞ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã ‡§∏‡§ï‡§§‡•á‡•§

### `--optimizer_config`

- **What**: optimizer settings ‡§ï‡•ã fine‚Äëtune ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: optimizers ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡§æ‡§∞‡•á settings ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç, ‡§π‡§∞ ‡§è‡§ï ‡§ï‡•á ‡§≤‡§ø‡§è CLI argument ‡§¶‡•á‡§®‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§π‡§æ‡§∞‡§ø‡§ï ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§á‡§∏‡§≤‡§ø‡§è ‡§Ü‡§™ comma‚Äëseparated ‡§∏‡•Ç‡§ö‡•Ä ‡§¶‡•á‡§ï‡§∞ default settings override ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Example**: **prodigy** optimizer ‡§ï‡•á ‡§≤‡§ø‡§è `d_coef` ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡§æ: `--optimizer_config=d_coef=0.1`

> Note: Optimizer betas dedicated parameters `--optimizer_beta1`, `--optimizer_beta2` ‡§∏‡•á override ‡§ï‡§ø‡§è ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--train_batch_size`

- **What**: training data loader ‡§ï‡•á ‡§≤‡§ø‡§è batch size‡•§
- **Why**: model memory consumption, convergence quality, ‡§î‡§∞ training speed ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§¨‡§°‡§º‡§æ batch size ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§É ‡§¨‡•á‡§π‡§§‡§∞ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§¨‡§°‡§º‡§æ batch size overfitting ‡§Ø‡§æ destabilized training ‡§ï‡§æ ‡§ï‡§æ‡§∞‡§£ ‡§¨‡§® ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§î‡§∞ training ‡§Ö‡§µ‡§ß‡§ø ‡§≠‡•Ä ‡§¨‡§¢‡§º‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ú‡§º‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§É ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§Ø‡§π ‡§π‡•à ‡§ï‡§ø training speed ‡§ò‡§ü‡§æ‡§è ‡§¨‡§ø‡§®‡§æ VRAM ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§Æ‡•á‡§Ç ‡§π‡•ã‡•§

### `--gradient_accumulation_steps`

- **What**: backward/update pass ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á accumulate ‡§ï‡§ø‡§è ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡•á steps ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ; ‡§Ø‡§π memory ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§æ‡§Æ ‡§ï‡•ã ‡§ï‡§à batches ‡§Æ‡•á‡§Ç ‡§¨‡§æ‡§Å‡§ü ‡§¶‡•á‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® training runtime ‡§¨‡§¢‡§º‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§¨‡§°‡§º‡•á models ‡§Ø‡§æ datasets ‡§ï‡•ã ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§®‡•á ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä‡•§

> Note: gradient accumulation steps ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä optimizer ‡§ï‡•á ‡§≤‡§ø‡§è fused backward pass enable ‡§® ‡§ï‡§∞‡•á‡§Ç‡•§

### `--allow_dataset_oversubscription` {#--allow_dataset_oversubscription}

- **What**: dataset effective batch size ‡§∏‡•á ‡§õ‡•ã‡§ü‡§æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ `repeats` ‡§∏‡•ç‡§µ‡§§‡§É adjust ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: multi‚ÄëGPU ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞‡•á‡§∂‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ requirements ‡§™‡•Ç‡§∞‡•Ä ‡§® ‡§π‡•ã‡§®‡•á ‡§™‡§∞ training failure ‡§ï‡•ã ‡§∞‡•ã‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **How it works**:
  - **effective batch size** ‡§ï‡•Ä ‡§ó‡§£‡§®‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à: `train_batch_size √ó num_gpus √ó gradient_accumulation_steps`
  - ‡§Ø‡§¶‡§ø ‡§ï‡§ø‡§∏‡•Ä aspect bucket ‡§Æ‡•á‡§Ç effective batch size ‡§∏‡•á ‡§ï‡§Æ samples ‡§π‡•à‡§Ç, ‡§§‡•ã `repeats` ‡§∏‡•ç‡§µ‡§§‡§É ‡§¨‡§¢‡§º‡§æ‡§§‡§æ ‡§π‡•à
  - ‡§ï‡•á‡§µ‡§≤ ‡§§‡§¨ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§ú‡§¨ dataset config ‡§Æ‡•á‡§Ç `repeats` explicitly ‡§∏‡•á‡§ü ‡§® ‡§π‡•ã
  - adjustment ‡§î‡§∞ reasoning ‡§¶‡§ø‡§ñ‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è warning ‡§≤‡•â‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à
- **Use cases**:
  - ‡§ï‡§à GPUs ‡§ï‡•á ‡§∏‡§æ‡§• ‡§õ‡•ã‡§ü‡•á datasets (< 100 images)
  - datasets ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó‡§∞ ‡§ï‡§ø‡§è ‡§¨‡§ø‡§®‡§æ ‡§Ö‡§≤‡§ó batch sizes ‡§ï‡•á ‡§∏‡§æ‡§• experimentation
  - full dataset ‡§á‡§ï‡§ü‡•ç‡§†‡§æ ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á prototyping
- **Example**: 25 images, 8 GPUs, ‡§î‡§∞ `train_batch_size=4` ‡§ï‡•á ‡§∏‡§æ‡§• effective batch size 32 ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π flag `repeats=1` ‡§∏‡•ç‡§µ‡§§‡§É ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§ó‡§æ ‡§§‡§æ‡§ï‡§ø 50 samples (25 √ó 2) ‡§Æ‡§ø‡§≤‡•á‡§Ç‡•§
- **Note**: ‡§Ø‡§π dataloader ‡§ï‡•â‡§®‡•ç‡§´‡§º‡§ø‡§ó ‡§Æ‡•á‡§Ç manually‚Äëset `repeats` values ‡§ï‡•ã override **‡§®‡§π‡•Ä‡§Ç** ‡§ï‡§∞‡•á‡§ó‡§æ‡•§ `--disable_bucket_pruning` ‡§ï‡•Ä ‡§§‡§∞‡§π, ‡§Ø‡§π flag ‡§¨‡§ø‡§®‡§æ surprising behavior ‡§ï‡•á ‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

Multi‚ÄëGPU training ‡§ï‡•á ‡§≤‡§ø‡§è dataset sizing ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§µ‡§∞‡§£ [DATALOADER.md](DATALOADER.md#automatic-dataset-oversubscription) ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§

---

## üõ† Advanced Optimizations

### `--use_ema`

- **What**: ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á training ‡§ú‡•Ä‡§µ‡§®‡§ï‡§æ‡§≤ ‡§Æ‡•á‡§Ç weights ‡§ï‡§æ exponential moving average ‡§∞‡§ñ‡§®‡§æ, ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã ‡§∏‡§Æ‡§Ø‚Äë‡§∏‡§Æ‡§Ø ‡§™‡§∞ ‡§ñ‡•Å‡§¶ ‡§Æ‡•á‡§Ç back‚Äëmerge ‡§ï‡§∞‡§®‡•á ‡§ú‡•à‡§∏‡§æ ‡§π‡•à‡•§
- **Why**: ‡§Ö‡§ß‡§ø‡§ï system resources ‡§î‡§∞ ‡§•‡•ã‡§°‡§º‡§æ ‡§Ö‡§ß‡§ø‡§ï runtime ‡§ñ‡§∞‡•ç‡§ö ‡§ï‡§∞‡§ï‡•á training stability ‡§¨‡•á‡§π‡§§‡§∞ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§

### `--ema_device`

- **Choices**: `cpu`, `accelerator`; default: `cpu`
- **What**: EMA weights updates ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§ï‡§π‡§æ‡§Å ‡§∞‡§ñ‡•Ä ‡§ú‡§æ‡§è‡§Å‡•§
- **Why**: EMA ‡§ï‡•ã accelerator ‡§™‡§∞ ‡§∞‡§ñ‡§®‡•á ‡§∏‡•á updates ‡§§‡•á‡§ú‡§º ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® VRAM ‡§≤‡§æ‡§ó‡§§ ‡§¨‡§¢‡§º‡§§‡•Ä ‡§π‡•à‡•§ CPU ‡§™‡§∞ ‡§∞‡§ñ‡§®‡•á ‡§∏‡•á memory ‡§¶‡§¨‡§æ‡§µ ‡§ï‡§Æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® `--ema_cpu_only` ‡§∏‡•á‡§ü ‡§® ‡§π‡•ã‡§®‡•á ‡§™‡§∞ weights ‡§ï‡•ã ‡§∂‡§ü‡§≤ ‡§ï‡§∞‡§®‡§æ ‡§™‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§

### `--ema_cpu_only`

- **What**: `--ema_device=cpu` ‡§π‡•ã‡§®‡•á ‡§™‡§∞ EMA weights ‡§ï‡•ã updates ‡§ï‡•á ‡§≤‡§ø‡§è accelerator ‡§™‡§∞ ‡§µ‡§æ‡§™‡§∏ ‡§≤‡•á ‡§ú‡§æ‡§®‡•á ‡§∏‡•á ‡§∞‡•ã‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§¨‡§°‡§º‡•á EMAs ‡§ï‡•á ‡§≤‡§ø‡§è host‚Äëto‚Äëdevice transfer ‡§∏‡§Æ‡§Ø ‡§î‡§∞ VRAM ‡§â‡§™‡§Ø‡•ã‡§ó ‡§¨‡§ö‡§æ‡§§‡§æ ‡§π‡•à‡•§ `--ema_device=accelerator` ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§á‡§∏‡§ï‡§æ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø weights ‡§™‡§π‡§≤‡•á ‡§∏‡•á accelerator ‡§™‡§∞ ‡§π‡•à‡§Ç‡•§

### `--ema_foreach_disable`

- **What**: EMA updates ‡§ï‡•á ‡§≤‡§ø‡§è `torch._foreach_*` kernels ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó disable ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§ï‡•Å‡§õ back‚Äëends ‡§Ø‡§æ hardware combinations ‡§Æ‡•á‡§Ç foreach ops ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‡§ó‡•ç‡§∞‡§∏‡•ç‡§§ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§á‡§®‡•ç‡§π‡•á‡§Ç disable ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ scalar implementation ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡•Ä ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á updates ‡§•‡•ã‡§°‡§º‡§æ ‡§ß‡•Ä‡§Æ‡•á ‡§π‡•ã ‡§ú‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--ema_update_interval`

- **What**: EMA shadow parameters ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¨‡§æ‡§∞ update ‡§π‡•ã‡§Ç, ‡§Ø‡§π ‡§ï‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§π‡§∞ step ‡§™‡§∞ update ‡§ï‡§∞‡§®‡§æ ‡§ï‡§à workflows ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§®‡§π‡•Ä‡§Ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è, `--ema_update_interval=100` ‡§π‡§∞ 100 optimizer steps ‡§™‡§∞ EMA update ‡§ï‡§∞‡•á‡§ó‡§æ, ‡§ú‡§ø‡§∏‡§∏‡•á `--ema_device=cpu` ‡§Ø‡§æ `--ema_cpu_only` ‡§ï‡•á ‡§∏‡§æ‡§• overhead ‡§ò‡§ü‡§§‡§æ ‡§π‡•à‡•§

### `--ema_decay`

- **What**: EMA updates ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø smoothing factor ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§â‡§ö‡•ç‡§ö ‡§Æ‡§æ‡§® (‡§â‡§¶‡§æ. `0.999`) EMA ‡§ï‡•ã ‡§ß‡•Ä‡§∞‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§¶‡•á‡§®‡•á ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡§π‡•Å‡§§ ‡§∏‡•ç‡§•‡§ø‡§∞ weights ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§ï‡§Æ ‡§Æ‡§æ‡§® (‡§â‡§¶‡§æ. `0.99`) ‡§®‡§è training signals ‡§ï‡•á ‡§∏‡§æ‡§• ‡§§‡•á‡§ú‡§º adapt ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--snr_gamma`

- **What**: min‚ÄëSNR weighted loss factor ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: Minimum SNR gamma loss factor ‡§ï‡•ã schedule ‡§Æ‡•á‡§Ç timestep ‡§ï‡•Ä ‡§∏‡•ç‡§•‡§ø‡§§‡§ø ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞ weigh ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§¨‡§π‡•Å‡§§ noisy timesteps ‡§ï‡§æ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§ï‡§Æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§ï‡§Æ‚Äënoise timesteps ‡§ï‡§æ ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§¨‡§¢‡§º‡§§‡§æ ‡§π‡•à‡•§ ‡§Æ‡•Ç‡§≤ ‡§™‡•á‡§™‡§∞ ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ö‡§®‡•Å‡§∂‡§Ç‡§∏‡§ø‡§§ ‡§Æ‡§æ‡§® **5** ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ü‡§™ **1** ‡§∏‡•á **20** ‡§§‡§ï ‡§Æ‡§æ‡§® ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç (‡§Ü‡§Æ‡§§‡•å‡§∞ ‡§™‡§∞ 20 ‡§ï‡•ã max ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à; 20 ‡§∏‡•á ‡§ä‡§™‡§∞ ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§ï‡§Æ ‡§π‡•ã‡§§‡§æ ‡§π‡•à)‡•§ **1** ‡§∏‡§¨‡§∏‡•á ‡§Æ‡§ú‡§¨‡•Ç‡§§ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--use_soft_min_snr`

- **What**: loss landscape ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï gradual weighting ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Æ‡•â‡§°‡§≤ ‡§ü‡•ç‡§∞‡•á‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: pixel diffusion models training ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü loss weighting schedule ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ degrade ‡§π‡•ã ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ DeepFloyd ‡§Æ‡•á‡§Ç soft‚Äëmin‚Äësnr‚Äëgamma ‡§≤‡§ó‡§≠‡§ó ‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§™‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§ Latent diffusion models ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§ï‡•ã ‡§∏‡§´‡§≤‡§§‡§æ ‡§Æ‡§ø‡§≤ ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§õ‡•ã‡§ü‡•á ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∏‡•á blurry results ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ ‡§¶‡§ø‡§ñ‡•Ä‡•§

### `--diff2flow_enabled`

- **What**: epsilon ‡§Ø‡§æ v‚Äëprediction models ‡§ï‡•á ‡§≤‡§ø‡§è Diffusion‚Äëto‚ÄëFlow bridge ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: model architecture ‡§¨‡§¶‡§≤‡•á ‡§¨‡§ø‡§®‡§æ standard diffusion objectives ‡§µ‡§æ‡§≤‡•á ‡§Æ‡•â‡§°‡§≤‡•ç‡§∏ ‡§ï‡•ã flow‚Äëmatching targets (noise - latents) ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Note**: Experimental ‡§´‡•Ä‡§ö‡§∞‡•§

### `--diff2flow_loss`

- **What**: native prediction loss ‡§ï‡•Ä ‡§¨‡§ú‡§æ‡§Ø Flow Matching loss ‡§™‡§∞ training‡•§
- **Why**: `--diff2flow_enabled` ‡§ï‡•á ‡§∏‡§æ‡§• enabled ‡§π‡•ã‡§®‡•á ‡§™‡§∞, loss ‡§ï‡•ã model ‡§ï‡•á native target (epsilon ‡§Ø‡§æ velocity) ‡§ï‡•Ä ‡§ú‡§ó‡§π flow target (noise - latents) ‡§ï‡•á ‡§ñ‡§ø‡§≤‡§æ‡§´ compute ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Note**: `--diff2flow_enabled` ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§π‡•à‡•§

### `--scheduled_sampling_max_step_offset`

- **What**: training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® "roll out" ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•á steps ‡§ï‡•Ä ‡§Ö‡§ß‡§ø‡§ï‡§§‡§Æ ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡•§
- **Why**: Scheduled Sampling (Rollout) ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡§π‡§æ‡§Å ‡§Æ‡•â‡§°‡§≤ training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§ï‡•Å‡§õ steps ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§™‡§®‡•á inputs ‡§ñ‡•Å‡§¶ generate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§á‡§∏‡§∏‡•á ‡§Æ‡•â‡§°‡§≤ ‡§Ö‡§™‡§®‡•Ä ‡§ó‡§≤‡§§‡§ø‡§Ø‡§æ‡§Å ‡§∏‡•Å‡§ß‡§æ‡§∞‡§®‡§æ ‡§∏‡•Ä‡§ñ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ exposure bias ‡§ò‡§ü‡§§‡§æ ‡§π‡•à‡•§
- **Default**: 0 (disabled). ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§ï‡§æ‡§∞‡§æ‡§§‡•ç‡§Æ‡§ï integer (‡§â‡§¶‡§æ., 5 ‡§Ø‡§æ 10) ‡§¶‡•á‡§Ç‡•§

### `--scheduled_sampling_strategy`

- **What**: rollout offset ‡§ö‡•Å‡§®‡§®‡•á ‡§ï‡•Ä ‡§∞‡§£‡§®‡•Ä‡§§‡§ø‡•§
- **Choices**: `uniform`, `biased_early`, `biased_late`.
- **Default**: `uniform`.
- **Why**: rollout ‡§≤‡§Ç‡§¨‡§æ‡§á‡§Ø‡•ã‡§Ç ‡§ï‡§æ distribution ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ `uniform` ‡§∏‡§Æ‡§æ‡§® ‡§∞‡•Ç‡§™ ‡§∏‡•á sample ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; `biased_early` ‡§õ‡•ã‡§ü‡•á rollouts ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à; `biased_late` ‡§≤‡§Ç‡§¨‡•á rollouts ‡§ï‡•ã ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï‡§§‡§æ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--scheduled_sampling_probability`

- **What**: ‡§ï‡§ø‡§∏‡•Ä sample ‡§ï‡•á ‡§≤‡§ø‡§è non‚Äëzero rollout offset ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§®‡•á ‡§ï‡•Ä ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ‡•§
- **Default**: 0.0.
- **Why**: scheduled sampling ‡§ï‡§ø‡§§‡§®‡•Ä ‡§¨‡§æ‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ 0.0 ‡§á‡§∏‡•á disable ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ö‡§æ‡§π‡•á `max_step_offset` > 0 ‡§π‡•ã‡•§ 1.0 ‡§π‡§∞ sample ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--scheduled_sampling_prob_start`

- **What**: ramp ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç scheduled sampling ‡§ï‡•Ä initial probability‡•§
- **Default**: 0.0.

### `--scheduled_sampling_prob_end`

- **What**: ramp ‡§ï‡•á ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç scheduled sampling ‡§ï‡•Ä final probability‡•§
- **Default**: 0.5.

### `--scheduled_sampling_ramp_steps`

- **What**: `prob_start` ‡§∏‡•á `prob_end` ‡§§‡§ï probability ‡§¨‡§¢‡§º‡§æ‡§®‡•á ‡§ï‡•á steps ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡•§
- **Default**: 0 (‡§ï‡•ã‡§à ramp ‡§®‡§π‡•Ä‡§Ç)‡•§

### `--scheduled_sampling_start_step`

- **What**: scheduled sampling ramp ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§®‡•á ‡§ï‡§æ global step‡•§
- **Default**: 0.0.

### `--scheduled_sampling_ramp_shape`

- **What**: probability ramp ‡§ï‡§æ ‡§Ü‡§ï‡§æ‡§∞‡•§
- **Choices**: `linear`, `cosine`.
- **Default**: `linear`.

### `--scheduled_sampling_sampler`

- **What**: rollout generation steps ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ solver‡•§
- **Choices**: `unipc`, `euler`, `dpm`, `rk4`.
- **Default**: `unipc`.

### `--scheduled_sampling_order`

- **What**: rollout ‡§ï‡•á ‡§≤‡§ø‡§è solver ‡§ï‡§æ order‡•§
- **Default**: 2.

### `--scheduled_sampling_reflexflow`

- **What**: flow‚Äëmatching models ‡§ï‡•á ‡§≤‡§ø‡§è scheduled sampling ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ReflexFlow‚Äëstyle enhancements (anti‚Äëdrift + frequency‚Äëcompensated weighting) ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: directional regularization ‡§î‡§∞ bias‚Äëaware loss weighting ‡§ú‡•ã‡§°‡§º‡§ï‡§∞ flow‚Äëmatching models ‡§Æ‡•á‡§Ç exposure bias ‡§ò‡§ü‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `--scheduled_sampling_max_step_offset` > 0 ‡§π‡•ã‡§®‡•á ‡§™‡§∞ flow‚Äëmatching models ‡§ï‡•á ‡§≤‡§ø‡§è auto‚Äëenable; `--scheduled_sampling_reflexflow=false` ‡§∏‡•á override ‡§ï‡§∞‡•á‡§Ç‡•§

### `--scheduled_sampling_reflexflow_alpha`

- **What**: exposure bias ‡§∏‡•á ‡§®‡§ø‡§ï‡§≤‡•á frequency‚Äëcompensation weight ‡§ï‡§æ scaling factor‡•§
- **Default**: 1.0.
- **Why**: flow‚Äëmatching models ‡§Æ‡•á‡§Ç rollout ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§¨‡§°‡§º‡•á exposure bias ‡§µ‡§æ‡§≤‡•á ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï weight ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--scheduled_sampling_reflexflow_beta1`

- **What**: ReflexFlow anti‚Äëdrift (directional) regularizer ‡§ï‡§æ weight‡•§
- **Default**: 10.0.
- **Why**: flow‚Äëmatching models ‡§Æ‡•á‡§Ç scheduled sampling ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•ã target clean sample ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§®‡•Ä predicted direction align ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§§‡§®‡§æ ‡§Æ‡§ú‡§¨‡•Ç‡§§‡•Ä ‡§∏‡•á ‡§™‡•ç‡§∞‡•ã‡§§‡•ç‡§∏‡§æ‡§π‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§è, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

### `--scheduled_sampling_reflexflow_beta2`

- **What**: ReflexFlow frequency‚Äëcompensation (loss reweighting) term ‡§ï‡§æ weight‡•§
- **Default**: 1.0.
- **Why**: reweighted flow‚Äëmatching loss ‡§ï‡•ã scale ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§ú‡•à‡§∏‡§æ ReflexFlow paper ‡§Æ‡•á‡§Ç Œ≤‚ÇÇ knob ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§

---

## üéØ CREPA (Cross-frame Representation Alignment)

CREPA ‡§è‡§ï regularization ‡§§‡§ï‡§®‡•Ä‡§ï ‡§π‡•à ‡§ú‡•ã video diffusion models ‡§ï‡•Ä fine‚Äëtuning ‡§Æ‡•á‡§Ç temporal consistency ‡§∏‡•Å‡§ß‡§æ‡§∞‡§§‡•Ä ‡§π‡•à, adjacent frames ‡§∏‡•á pretrained visual features ‡§ï‡•á ‡§∏‡§æ‡§• hidden states align ‡§ï‡§∞‡§ï‡•á‡•§ ‡§Ø‡§π ‡§™‡•á‡§™‡§∞ ["Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models"](https://arxiv.org/abs/2506.09229) ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§π‡•à‡•§

### `--crepa_enabled`

- **What**: training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® CREPA regularization ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: ‡§™‡§°‡§º‡•ã‡§∏‡•Ä frames ‡§ï‡•á DINOv2 features ‡§ï‡•á ‡§∏‡§æ‡§• DiT hidden states align ‡§ï‡§∞‡§ï‡•á ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã frames ‡§Æ‡•á‡§Ç semantic consistency ‡§¨‡§¢‡§º‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `false`
- **Note**: ‡§ï‡•á‡§µ‡§≤ Transformer-‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ diffusion models (DiT ‡§∂‡•à‡§≤‡•Ä) ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç‡•§ UNet models (SDXL, SD1.5, Kolors) ‡§ï‡•á ‡§≤‡§ø‡§è U-REPA ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§

### `--crepa_block_index`

- **What**: alignment ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡§ø‡§∏ transformer block ‡§ï‡•á hidden states ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§Ç‡•§
- **Why**: ‡§™‡•á‡§™‡§∞ CogVideoX ‡§ï‡•á ‡§≤‡§ø‡§è block 8 ‡§î‡§∞ Hunyuan Video ‡§ï‡•á ‡§≤‡§ø‡§è block 10 ‡§∏‡•Å‡§ù‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§‡•Ä blocks ‡§Ö‡§ï‡•ç‡§∏‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§µ‡•á DiT ‡§ï‡§æ "encoder" ‡§π‡§ø‡§∏‡•ç‡§∏‡§æ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Required**: ‡§π‡§æ‡§Å, ‡§ú‡§¨ CREPA enabled ‡§π‡•ã‡•§

### `--crepa_lambda`

- **What**: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø training loss ‡§ï‡•á ‡§Æ‡•Å‡§ï‡§æ‡§¨‡§≤‡•á CREPA alignment loss ‡§ï‡§æ weight‡•§
- **Why**: alignment regularization training ‡§ï‡•ã ‡§ï‡§ø‡§§‡§®‡§æ ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡§ø‡§§ ‡§ï‡§∞‡•á, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§™‡•á‡§™‡§∞ CogVideoX ‡§ï‡•á ‡§≤‡§ø‡§è 0.5 ‡§î‡§∞ Hunyuan Video ‡§ï‡•á ‡§≤‡§ø‡§è 1.0 ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0.5`

### `--crepa_adjacent_distance`

- **What**: neighbor frame alignment ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¶‡•Ç‡§∞‡•Ä `d`‡•§
- **Why**: ‡§™‡•á‡§™‡§∞ ‡§ï‡•Ä Equation 6 ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞, $K = \{f-d, f+d\}$ ‡§¨‡§§‡§æ‡§§‡§æ ‡§π‡•à ‡§ï‡§ø ‡§ï‡§ø‡§® neighboring frames ‡§∏‡•á align ‡§ï‡§∞‡§®‡§æ ‡§π‡•à‡•§ `d=1` ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§π‡§∞ frame ‡§Ö‡§™‡§®‡•á immediate neighbors ‡§∏‡•á align ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `1`

### `--crepa_adjacent_tau`

- **What**: exponential distance weighting ‡§ï‡•á ‡§≤‡§ø‡§è temperature coefficient‡•§
- **Why**: $e^{-|k-f|/\tau}$ ‡§ï‡•á ‡§ú‡§∞‡§ø‡§è alignment weight ‡§ï‡§ø‡§§‡§®‡•Ä ‡§ú‡§≤‡•ç‡§¶‡•Ä decay ‡§π‡•ã, ‡§Ø‡§π ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§ï‡§Æ ‡§Æ‡§æ‡§® immediate neighbors ‡§™‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§ú‡•ã‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `1.0`

### `--crepa_cumulative_neighbors`

- **What**: adjacent mode ‡§ï‡•Ä ‡§ú‡§ó‡§π cumulative mode ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**:
  - **Adjacent mode (‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü)**: ‡§ï‡•á‡§µ‡§≤ exact ‡§¶‡•Ç‡§∞‡•Ä `d` ‡§µ‡§æ‡§≤‡•á frames ‡§∏‡•á align ‡§ï‡§∞‡§§‡§æ ‡§π‡•à (‡§™‡•á‡§™‡§∞ ‡§ï‡•á $K = \{f-d, f+d\}$ ‡§ú‡•à‡§∏‡§æ)
  - **Cumulative mode**: ‡§¶‡•Ç‡§∞‡•Ä 1 ‡§∏‡•á `d` ‡§§‡§ï ‡§∏‡§≠‡•Ä frames ‡§∏‡•á align ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, smoother gradients ‡§¶‡•á‡§§‡§æ ‡§π‡•à
- **Default**: `false`

### `--crepa_normalize_neighbour_sum`

- **What**: neighbor‚Äësum alignment ‡§ï‡•ã per‚Äëframe weight sum ‡§∏‡•á normalize ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: `crepa_alignment_score` ‡§ï‡•ã [-1, 1] ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ loss scale ‡§ï‡•ã ‡§Ö‡§ß‡§ø‡§ï literal ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§™‡•á‡§™‡§∞ ‡§ï‡•Ä Eq. (6) ‡§∏‡•á experimental deviation ‡§π‡•à‡•§
- **Default**: `false`

### `--crepa_normalize_by_frames`

- **What**: alignment loss ‡§ï‡•ã frames ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§∏‡•á normalize ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: video length ‡§ï‡•á ‡§¨‡§æ‡§µ‡§ú‡•Ç‡§¶ loss scale consistent ‡§∞‡§π‡§§‡§æ ‡§π‡•à‡•§ Disable ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§≤‡§Ç‡§¨‡•á videos ‡§ï‡•ã stronger alignment signal ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `true`

### `--crepa_spatial_align`

- **What**: ‡§ú‡§¨ DiT ‡§î‡§∞ encoder ‡§ï‡•á token counts ‡§Ö‡§≤‡§ó ‡§π‡•ã‡§Ç ‡§§‡•ã spatial interpolation ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: DiT hidden states ‡§î‡§∞ DINOv2 features ‡§ï‡•Ä spatial resolutions ‡§Ö‡§≤‡§ó ‡§π‡•ã ‡§∏‡§ï‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ bilinear interpolation ‡§â‡§®‡•ç‡§π‡•á‡§Ç spatially align ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; disabled ‡§π‡•ã‡§®‡•á ‡§™‡§∞ global pooling fallback ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `true`

### `--crepa_model`

- **What**: feature extraction ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•å‡§®‚Äë‡§∏‡§æ pretrained encoder ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡•§
- **Why**: ‡§™‡•á‡§™‡§∞ DINOv2‚Äëg (ViT‚ÄëGiant) ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ `dinov2_vitb14` ‡§ú‡•à‡§∏‡•á ‡§õ‡•ã‡§ü‡•á variants ‡§ï‡§Æ memory ‡§≤‡•á‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Default**: `dinov2_vitg14`
- **Choices**: `dinov2_vitg14`, `dinov2_vitb14`, `dinov2_vits14`

### `--crepa_encoder_frames_batch_size`

- **What**: external feature encoder parallel ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§§‡§®‡•á frames ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞‡•á‡•§ 0 ‡§Ø‡§æ negative ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§™‡•Ç‡§∞‡•á batch ‡§ï‡•á ‡§∏‡§≠‡•Ä frames ‡§è‡§ï ‡§∏‡§æ‡§• ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§Ø‡§¶‡§ø ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ divisor ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à, ‡§§‡•ã remainder ‡§õ‡•ã‡§ü‡•á batch ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡§Ç‡§≠‡§æ‡§≤‡§æ ‡§ú‡§æ‡§è‡§ó‡§æ‡•§
- **Why**: DINO‚Äëlike encoders image models ‡§π‡•à‡§Ç, ‡§á‡§∏‡§≤‡§ø‡§è ‡§µ‡•á VRAM ‡§¨‡§ö‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è frames ‡§ï‡•ã sliced batches ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§ó‡§§‡§ø ‡§ï‡•Ä ‡§ï‡•Ä‡§Æ‡§§ ‡§™‡§∞‡•§
- **Default**: `-1`

### `--crepa_use_backbone_features`

- **What**: external encoder skip ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ diffusion model ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ student block ‡§ï‡•ã teacher block ‡§ï‡•á ‡§∏‡§æ‡§• align ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: ‡§ú‡§¨ backbone ‡§ï‡•á ‡§™‡§æ‡§∏ ‡§™‡§π‡§≤‡•á ‡§∏‡•á ‡§Æ‡§ú‡§¨‡•Ç‡§§ semantic layer ‡§π‡•ã, ‡§§‡§¨ DINOv2 ‡§≤‡•ã‡§° ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§¨‡§ö‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `false`

### `--crepa_teacher_block_index`

- **What**: backbone features ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§∏‡§Æ‡§Ø teacher block index‡•§
- **Why**: external encoder ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ, earlier student block ‡§ï‡•ã later teacher block ‡§∏‡•á align ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§ unset ‡§π‡•ã‡§®‡•á ‡§™‡§∞ student block ‡§™‡§∞ fallback ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Default**: ‡§Ø‡§¶‡§ø ‡§®‡§π‡•Ä‡§Ç ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ, ‡§§‡•ã `crepa_block_index` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§ó‡§æ‡•§

### `--crepa_encoder_image_size`

- **What**: encoder ‡§ï‡•á ‡§≤‡§ø‡§è input resolution‡•§
- **Why**: DINOv2 models ‡§Ö‡§™‡§®‡•á training resolution ‡§™‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§ giant model 518x518 ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `518`

### `--crepa_scheduler`

- **What**: training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® CREPA coefficient decay ‡§ï‡§æ schedule‡•§
- **Why**: ‡§ú‡•à‡§∏‡•á-‡§ú‡•à‡§∏‡•á training ‡§Ü‡§ó‡•á ‡§¨‡§¢‡§º‡•á, CREPA regularization strength ‡§ï‡•ã ‡§ï‡§Æ ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à, deep encoder features ‡§™‡§∞ overfitting ‡§∞‡•ã‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Options**: `constant`, `linear`, `cosine`, `polynomial`
- **Default**: `constant`

### `--crepa_warmup_steps`

- **What**: CREPA weight ‡§ï‡•ã 0 ‡§∏‡•á `crepa_lambda` ‡§§‡§ï linearly ramp ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è steps ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡•§
- **Why**: gradual warmup CREPA regularization ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á early training ‡§ï‡•ã stabilize ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0`

### `--crepa_decay_steps`

- **What**: decay ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•Å‡§≤ steps (warmup ‡§ï‡•á ‡§¨‡§æ‡§¶)‡•§ 0 ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ ‡§™‡•Ç‡§∞‡•Ä training run ‡§™‡§∞ decay ‡§π‡•ã‡§ó‡§æ‡•§
- **Why**: decay phase ‡§ï‡•Ä duration ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ warmup ‡§™‡•Ç‡§∞‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ decay ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0` (`max_train_steps` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§ó‡§æ)

### `--crepa_lambda_end`

- **What**: decay ‡§™‡•Ç‡§∞‡§æ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ final CREPA weight‡•§
- **Why**: 0 ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•á ‡§™‡§∞ training ‡§ï‡•á ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç CREPA ‡§™‡•ç‡§∞‡§≠‡§æ‡§µ‡•Ä ‡§∞‡•Ç‡§™ ‡§∏‡•á disable ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à, text2video ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä ‡§ú‡§π‡§æ‡§Å CREPA artifacts ‡§™‡•à‡§¶‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0.0`

### `--crepa_power`

- **What**: polynomial decay ‡§ï‡•á ‡§≤‡§ø‡§è power factor‡•§ 1.0 = linear, 2.0 = quadratic, ‡§Ü‡§¶‡§ø‡•§
- **Why**: higher values ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•á‡§ú decay ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§Ö‡§Ç‡§§ ‡§ï‡•Ä ‡§ì‡§∞ ‡§ß‡•Ä‡§Æ‡§æ ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `1.0`

### `--crepa_cutoff_step`

- **What**: hard cutoff step ‡§ú‡§ø‡§∏‡§ï‡•á ‡§¨‡§æ‡§¶ CREPA disable ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Why**: model temporal alignment ‡§™‡§∞ converge ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ CREPA disable ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä‡•§
- **Default**: `0` (‡§ï‡•ã‡§à step-based cutoff ‡§®‡§π‡•Ä‡§Ç)

### `--crepa_similarity_threshold`

- **What**: similarity EMA threshold ‡§ú‡§ø‡§∏ ‡§™‡§∞ CREPA cutoff trigger ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§
- **Why**: ‡§ú‡§¨ alignment score (`crepa_alignment_score`) ‡§ï‡§æ exponential moving average ‡§á‡§∏ ‡§Æ‡§æ‡§® ‡§§‡§ï ‡§™‡§π‡•Å‡§Å‡§ö‡§§‡§æ ‡§π‡•à, ‡§§‡•ã deep encoder features ‡§™‡§∞ overfitting ‡§∞‡•ã‡§ï‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è CREPA disable ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§ text2video training ‡§ï‡•á ‡§≤‡§ø‡§è ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä‡•§ `crepa_normalize_neighbour_sum` enable ‡§® ‡§π‡•ã‡§®‡•á ‡§™‡§∞ alignment score 1.0 ‡§∏‡•á ‡§ä‡§™‡§∞ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
- **Default**: None (disabled)

### `--crepa_similarity_ema_decay`

- **What**: similarity tracking ‡§ï‡•á ‡§≤‡§ø‡§è exponential moving average decay factor‡•§
- **Why**: higher values smoother tracking ‡§¶‡•á‡§§‡•á ‡§π‡•à‡§Ç (0.99 ‚âà 100-step window), lower values changes ‡§™‡§∞ ‡§§‡•á‡§ú react ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Default**: `0.99`

### `--crepa_threshold_mode`

- **What**: similarity threshold ‡§™‡§π‡•Å‡§Å‡§ö‡§®‡•á ‡§™‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡•§
- **Options**: `permanent` (threshold hit ‡§π‡•ã‡§®‡•á ‡§™‡§∞ CREPA permanently off ‡§∞‡§π‡§§‡§æ ‡§π‡•à), `recoverable` (similarity ‡§ó‡§ø‡§∞‡§®‡•á ‡§™‡§∞ CREPA ‡§´‡§ø‡§∞ ‡§∏‡•á enable ‡§π‡•ã‡§§‡§æ ‡§π‡•à)
- **Default**: `permanent`

### Example Configuration

```toml
# Enable CREPA for video fine-tuning
crepa_enabled = true
crepa_block_index = 8          # Adjust based on your model
crepa_lambda = 0.5
crepa_adjacent_distance = 1
crepa_adjacent_tau = 1.0
crepa_cumulative_neighbors = false
crepa_normalize_neighbour_sum = false
crepa_normalize_by_frames = true
crepa_spatial_align = true
crepa_model = "dinov2_vitg14"
crepa_encoder_frames_batch_size = -1
crepa_use_backbone_features = false
# crepa_teacher_block_index = 16
crepa_encoder_image_size = 518

# CREPA Scheduling (optional)
# crepa_scheduler = "cosine"           # Decay type: constant, linear, cosine, polynomial
# crepa_warmup_steps = 100             # Warmup before CREPA kicks in
# crepa_decay_steps = 1000             # Steps for decay (0 = entire training)
# crepa_lambda_end = 0.0               # Final weight after decay
# crepa_cutoff_step = 5000             # Hard cutoff step (0 = disabled)
# crepa_similarity_threshold = 0.9    # Similarity-based cutoff
# crepa_threshold_mode = "permanent"   # permanent or recoverable
```

---

## üéØ U-REPA (UNet Representation Alignment)

U-REPA UNet ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ diffusion models (SDXL, SD1.5, Kolors) ‡§ï‡•á ‡§≤‡§ø‡§è regularization ‡§§‡§ï‡§®‡•Ä‡§ï ‡§π‡•à‡•§ ‡§Ø‡§π UNet mid-block features ‡§ï‡•ã pretrained vision features ‡§ï‡•á ‡§∏‡§æ‡§• align ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ relative similarity structure ‡§∞‡§ñ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è manifold loss ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§

### `--urepa_enabled`

- **What**: training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® U-REPA enable ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: frozen vision encoder ‡§ï‡•á ‡§∏‡§æ‡§• UNet mid-block features ‡§ï‡§æ representation alignment ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `false`
- **Note**: ‡§ï‡•á‡§µ‡§≤ UNet models (SDXL, SD1.5, Kolors) ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç‡•§

### `--urepa_lambda`

- **What**: ‡§Æ‡•Å‡§ñ‡•ç‡§Ø training loss ‡§ï‡•á ‡§Æ‡•Å‡§ï‡§æ‡§¨‡§≤‡•á U-REPA alignment loss ‡§ï‡§æ weight‡•§
- **Why**: regularization ‡§ï‡•Ä strength ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0.5`

### `--urepa_manifold_weight`

- **What**: manifold loss ‡§ï‡§æ weight (alignment loss ‡§ï‡•á ‡§Æ‡•Å‡§ï‡§æ‡§¨‡§≤‡•á)‡•§
- **Why**: relative similarity structure ‡§™‡§∞ ‡§ú‡§º‡•ã‡§∞ ‡§¶‡•á‡§§‡§æ ‡§π‡•à (paper default 3.0)‡•§
- **Default**: `3.0`

### `--urepa_model`

- **What**: frozen vision encoder ‡§ï‡•á ‡§≤‡§ø‡§è torch hub identifier‡•§
- **Why**: default DINOv2 ViT-G/14; ‡§õ‡•ã‡§ü‡•á ‡§Æ‡•â‡§°‡§≤ (‡§ú‡•à‡§∏‡•á `dinov2_vits14`) ‡§§‡•á‡§ú‡§º ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Default**: `dinov2_vitg14`

### `--urepa_encoder_image_size`

- **What**: vision encoder preprocessing ‡§ï‡•á ‡§≤‡§ø‡§è input resolution‡•§
- **Why**: encoder ‡§ï‡•Ä native resolution ‡§∞‡§ñ‡•á‡§Ç (DINOv2 ViT-G/14 ‡§ï‡•á ‡§≤‡§ø‡§è 518; ViT-S/14 ‡§ï‡•á ‡§≤‡§ø‡§è 224)‡•§
- **Default**: `518`

### `--urepa_use_tae`

- **What**: full VAE ‡§ï‡•Ä ‡§ú‡§ó‡§π Tiny AutoEncoder ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- **Why**: ‡§§‡•á‡§ú‡§º ‡§î‡§∞ ‡§ï‡§Æ VRAM, ‡§≤‡•á‡§ï‡§ø‡§® decoded image quality ‡§ï‡§Æ‡•§
- **Default**: `false`

### `--urepa_scheduler`

- **What**: training ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® U-REPA coefficient decay schedule‡•§
- **Why**: training ‡§¨‡§¢‡§º‡§®‡•á ‡§ï‡•á ‡§∏‡§æ‡§• regularization strength ‡§ï‡§Æ ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Æ‡§¶‡§¶‡•§
- **Options**: `constant`, `linear`, `cosine`, `polynomial`
- **Default**: `constant`

### `--urepa_warmup_steps`

- **What**: U-REPA weight ‡§ï‡•ã 0 ‡§∏‡•á `urepa_lambda` ‡§§‡§ï linearly ‡§¨‡§¢‡§º‡§æ‡§®‡•á ‡§ï‡•á steps‡•§
- **Why**: ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§‡•Ä training ‡§ï‡•ã stabilize ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0`

### `--urepa_decay_steps`

- **What**: decay ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ï‡•Å‡§≤ steps (warmup ‡§ï‡•á ‡§¨‡§æ‡§¶)‡•§ 0 ‡§Æ‡§§‡§≤‡§¨ ‡§™‡•Ç‡§∞‡•á training ‡§Æ‡•á‡§Ç decay‡•§
- **Why**: decay phase ‡§ï‡•Ä duration ‡§®‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0` (`max_train_steps`)

### `--urepa_lambda_end`

- **What**: decay ‡§ï‡•á ‡§¨‡§æ‡§¶ final U-REPA weight‡•§
- **Why**: 0 ‡§∞‡§ñ‡§®‡•á ‡§™‡§∞ training ‡§ï‡•á ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç U-REPA effectively disable ‡§π‡•ã ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `0.0`

### `--urepa_power`

- **What**: polynomial decay power‡•§ 1.0 = linear, 2.0 = quadratic ‡§Ü‡§¶‡§ø‡•§
- **Why**: ‡§¨‡§°‡§º‡§æ ‡§Æ‡§æ‡§® ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç ‡§§‡•á‡§ú‡§º decay ‡§î‡§∞ ‡§Ö‡§Ç‡§§ ‡§Æ‡•á‡§Ç ‡§ß‡•Ä‡§Æ‡§æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `1.0`

### `--urepa_cutoff_step`

- **What**: ‡§á‡§∏ step ‡§ï‡•á ‡§¨‡§æ‡§¶ U-REPA ‡§¨‡§Ç‡§¶‡•§
- **Why**: alignment converge ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§¨‡§Ç‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è‡•§
- **Default**: `0` (no cutoff)

### `--urepa_similarity_threshold`

- **What**: similarity EMA threshold ‡§ú‡§ø‡§∏ ‡§™‡§∞ U-REPA cutoff ‡§ü‡•ç‡§∞‡§ø‡§ó‡§∞ ‡§π‡•ã‡•§
- **Why**: `urepa_similarity` ‡§ï‡§æ EMA ‡§á‡§∏ ‡§Æ‡§æ‡§® ‡§§‡§ï ‡§™‡§π‡•Å‡§Ç‡§ö‡§§‡•á ‡§π‡•Ä U-REPA disable ‡§π‡•ã‡§§‡§æ ‡§π‡•à, overfitting ‡§∞‡•ã‡§ï‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è‡•§
- **Default**: None (disabled)

### `--urepa_similarity_ema_decay`

- **What**: similarity tracking ‡§ï‡•á ‡§≤‡§ø‡§è EMA decay factor‡•§
- **Why**: ‡§¨‡§°‡§º‡§æ ‡§Æ‡§æ‡§® smooth (0.99 ‚âà 100-step window), ‡§õ‡•ã‡§ü‡§æ ‡§Æ‡§æ‡§® ‡§§‡•á‡§ú‡§º ‡§™‡•ç‡§∞‡§§‡§ø‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§
- **Default**: `0.99`

### `--urepa_threshold_mode`

- **What**: threshold ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•á ‡§™‡§∞ ‡§µ‡•ç‡§Ø‡§µ‡§π‡§æ‡§∞‡•§
- **Options**: `permanent` (‡§è‡§ï ‡§¨‡§æ‡§∞ ‡§¨‡§Ç‡§¶ ‡§§‡•ã ‡§π‡§Æ‡•á‡§∂‡§æ ‡§¨‡§Ç‡§¶), `recoverable` (similarity ‡§ó‡§ø‡§∞‡§®‡•á ‡§™‡§∞ ‡§´‡§ø‡§∞ enable)
- **Default**: `permanent`

### Example Configuration

```toml
# UNet fine-tuning ‡§ï‡•á ‡§≤‡§ø‡§è U-REPA enable ‡§ï‡§∞‡•á‡§Ç (SDXL, SD1.5, Kolors)
urepa_enabled = true
urepa_lambda = 0.5
urepa_manifold_weight = 3.0
urepa_model = "dinov2_vitg14"
urepa_encoder_image_size = 518
urepa_use_tae = false

# U-REPA Scheduling (optional)
# urepa_scheduler = "cosine"           # Decay type: constant, linear, cosine, polynomial
# urepa_warmup_steps = 100             # U-REPA ‡§∂‡•Å‡§∞‡•Ç ‡§π‡•ã‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á warmup
# urepa_decay_steps = 1000             # Decay steps (0 = ‡§™‡•Ç‡§∞‡•á training ‡§Æ‡•á‡§Ç)
# urepa_lambda_end = 0.0               # Decay ‡§ï‡•á ‡§¨‡§æ‡§¶ final weight
# urepa_cutoff_step = 5000             # Hard cutoff step (0 = disabled)
# urepa_similarity_threshold = 0.9     # Similarity-based cutoff
# urepa_threshold_mode = "permanent"   # permanent ‡§Ø‡§æ recoverable
```

---

## üîÑ Checkpointing and Resumption

### `--checkpoint_step_interval` (alias: `--checkpointing_steps`)

- **What**: training state checkpoints ‡§ï‡§ø‡§§‡§®‡•á steps ‡§™‡§∞ ‡§∏‡•á‡§µ ‡§π‡•ã‡§Ç (steps ‡§Æ‡•á‡§Ç interval)‡•§
- **Why**: training resume ‡§î‡§∞ inference ‡§ï‡•á ‡§≤‡§ø‡§è ‡§â‡§™‡§Ø‡•ã‡§ó‡•Ä‡•§ ‡§π‡§∞ *n* iterations ‡§™‡§∞ Diffusers filesystem layout ‡§Æ‡•á‡§Ç `.safetensors` format ‡§ï‡§æ partial checkpoint ‡§∏‡•á‡§µ ‡§π‡•ã‡§ó‡§æ‡•§

---

## üîÅ LayerSync (Hidden State Self-Alignment)

LayerSync ‡§è‡§ï "student" layer ‡§ï‡•ã ‡§â‡§∏‡•Ä transformer ‡§ï‡•á ‡§è‡§ï ‡§Æ‡§ú‡§¨‡•Ç‡§§ "teacher" layer ‡§∏‡•á match ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§™‡•ç‡§∞‡•ã‡§§‡•ç‡§∏‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, hidden tokens ‡§™‡§∞ cosine similarity ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á‡•§

### `--layersync_enabled`

- **What**: ‡§è‡§ï ‡§π‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§Ö‡§Ç‡§¶‡§∞ ‡§¶‡•ã transformer blocks ‡§ï‡•á ‡§¨‡•Ä‡§ö LayerSync hidden‚Äëstate alignment ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡•á‡§Ç‡•§
- **Notes**: hidden‚Äëstate buffer allocate ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; required flags missing ‡§π‡•ã‡§Ç ‡§§‡•ã startup ‡§™‡§∞ error ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Default**: `false`

### `--layersync_student_block`

- **What**: student anchor ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ transformer block index‡•§
- **Indexing**: LayerSync ‡§™‡•á‡§™‡§∞‚Äëstyle 1‚Äëbased depths ‡§Ø‡§æ 0‚Äëbased layer ids ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; implementation ‡§™‡§π‡§≤‡•á `idx-1` ‡§Ü‡§ú‡§º‡§Æ‡§æ‡§§‡§æ ‡§π‡•à, ‡§´‡§ø‡§∞ `idx`‡•§
- **Required**: ‡§π‡§æ‡§Å, ‡§ú‡§¨ LayerSync enabled ‡§π‡•ã‡•§

### `--layersync_teacher_block`

- **What**: teacher target ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ transformer block index (student ‡§∏‡•á ‡§ó‡§π‡§∞‡§æ ‡§π‡•ã ‡§∏‡§ï‡§§‡§æ ‡§π‡•à)‡•§
- **Indexing**: student block ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§π‡•Ä 1‚Äëbased‚Äëfirst, ‡§´‡§ø‡§∞ 0‚Äëbased fallback‡•§
- **Default**: omit ‡§π‡•ã‡§®‡•á ‡§™‡§∞ student block ‡§π‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡§æ ‡§π‡•à ‡§§‡§æ‡§ï‡§ø loss self‚Äësimilarity ‡§¨‡§® ‡§ú‡§æ‡§è‡•§

### `--layersync_lambda`

- **What**: student ‡§î‡§∞ teacher hidden states ‡§ï‡•á ‡§¨‡•Ä‡§ö LayerSync cosine alignment loss (negative cosine similarity) ‡§ï‡§æ weight‡•§
- **Effect**: base loss ‡§ï‡•á ‡§ä‡§™‡§∞ ‡§ú‡•ã‡§°‡§º‡§æ ‡§ó‡§Ø‡§æ auxiliary regularizer scale ‡§ï‡§∞‡§§‡§æ ‡§π‡•à; ‡§â‡§ö‡•ç‡§ö ‡§Æ‡§æ‡§® student tokens ‡§ï‡•ã teacher tokens ‡§∏‡•á ‡§Ö‡§ß‡§ø‡§ï alignment ‡§ï‡•á ‡§≤‡§ø‡§è push ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§
- **Upstream name**: ‡§Æ‡•Ç‡§≤ LayerSync codebase ‡§Æ‡•á‡§Ç `--reg-weight`.
- **Required**: LayerSync enabled ‡§π‡•ã‡§®‡•á ‡§™‡§∞ > 0 ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è (‡§Ö‡§®‡•ç‡§Ø‡§•‡§æ training abort ‡§π‡•ã‡§§‡•Ä ‡§π‡•à)‡•§
- **Default**: LayerSync enabled ‡§π‡•ã‡§®‡•á ‡§™‡§∞ `0.2` (reference repo ‡§∏‡•á ‡§Æ‡•á‡§≤), ‡§Ö‡§®‡•ç‡§Ø‡§•‡§æ `0.0`.

Upstream option mapping (LayerSync ‚Üí SimpleTuner):
- `--encoder-depth` ‚Üí `--layersync_student_block` (upstream ‡§ú‡•à‡§∏‡§æ 1‚Äëbased depth ‡§Ø‡§æ 0‚Äëbased layer index ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à)
- `--gt-encoder-depth` ‚Üí `--layersync_teacher_block` (1‚Äëbased preferred; omit ‡§π‡•ã‡§®‡•á ‡§™‡§∞ student ‡§™‡§∞ ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü)
- `--reg-weight` ‚Üí `--layersync_lambda`

> Notes: LayerSync ‡§π‡§Æ‡•á‡§∂‡§æ similarity ‡§∏‡•á ‡§™‡§π‡§≤‡•á teacher hidden state detach ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, reference implementation ‡§∏‡•á ‡§Æ‡•á‡§≤ ‡§ñ‡§æ‡§§‡•á ‡§π‡•Å‡§è‡•§ ‡§Ø‡§π ‡§â‡§® ‡§Æ‡•â‡§°‡§≤‡•ã‡§Ç ‡§™‡§∞ ‡§®‡§ø‡§∞‡•ç‡§≠‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§ú‡•ã transformer hidden states expose ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç (SimpleTuner ‡§ï‡•á ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ transformer backbones) ‡§î‡§∞ hidden‚Äëstate buffer ‡§ï‡•á ‡§≤‡§ø‡§è per‚Äëstep memory ‡§ú‡•ã‡§°‡§º‡§§‡§æ ‡§π‡•à; VRAM tight ‡§π‡•ã ‡§§‡•ã disable ‡§ï‡§∞‡•á‡§Ç‡•§

### `--checkpoint_epoch_interval`

- **What**: ‡§π‡§∞ N ‡§™‡•Ç‡§∞‡•ç‡§£ epochs ‡§™‡§∞ checkpointing ‡§ö‡§≤‡§æ‡§è‡§Å‡•§
- **Why**: step‚Äëbased checkpoints ‡§ï‡•ã ‡§™‡•Ç‡§∞‡§ï ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, ‡§§‡§æ‡§ï‡§ø multi‚Äëdataset sampling ‡§ï‡•á ‡§ï‡§æ‡§∞‡§£ step counts ‡§¨‡§¶‡§≤‡§®‡•á ‡§™‡§∞ ‡§≠‡•Ä epoch boundaries ‡§™‡§∞ state capture ‡§π‡•ã ‡§∏‡§ï‡•á‡•§

### `--resume_from_checkpoint`

- **What**: training resume ‡§ï‡§∞‡§®‡§æ ‡§π‡•à ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç ‡§î‡§∞ ‡§ï‡§π‡§æ‡§Å ‡§∏‡•á‡•§ `latest`, local checkpoint ‡§®‡§æ‡§Æ/‡§™‡§•, ‡§Ø‡§æ S3/R2 URI ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: saved state ‡§∏‡•á training ‡§ú‡§æ‡§∞‡•Ä ‡§∞‡§ñ‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à, manual ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§Ø‡§æ latest ‡§â‡§™‡§≤‡§¨‡•ç‡§ß checkpoint ‡§∏‡•á‡•§
- **Remote resume**: ‡§™‡•Ç‡§∞‡§æ URI (`s3://bucket/jobs/.../checkpoint-100`) ‡§Ø‡§æ bucket-relative key (`jobs/.../checkpoint-100`) ‡§¶‡•á‡§Ç‡•§ `latest` ‡§ï‡•á‡§µ‡§≤ local `output_dir` ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à„ÄÇ
- **Requirements**: remote resume ‡§ï‡•á ‡§≤‡§ø‡§è publishing_config ‡§Æ‡•á‡§Ç S3 entry (bucket + credentials) ‡§ö‡§æ‡§π‡§ø‡§è ‡§ú‡•ã checkpoint read ‡§ï‡§∞ ‡§∏‡§ï‡•á‡•§
- **Notes**: remote checkpoints ‡§Æ‡•á‡§Ç `checkpoint_manifest.json` ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è (‡§π‡§æ‡§≤ ‡§ï‡•Ä SimpleTuner runs ‡§∏‡•á generated)‡•§ ‡§è‡§ï checkpoint ‡§Æ‡•á‡§Ç `unet` ‡§î‡§∞ ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï ‡§∞‡•Ç‡§™ ‡§∏‡•á `unet_ema` subfolder ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ `unet` ‡§ï‡•ã ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä Diffusers layout SDXL ‡§Æ‡•â‡§°‡§≤ ‡§Æ‡•á‡§Ç ‡§∞‡§ñ‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§ú‡§ø‡§∏‡§∏‡•á ‡§á‡§∏‡•á ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§

> ‚ÑπÔ∏è PixArt, SD3, ‡§Ø‡§æ Hunyuan ‡§ú‡•à‡§∏‡•á transformer ‡§Æ‡•â‡§°‡§≤ `transformer` ‡§î‡§∞ `transformer_ema` subfolder ‡§®‡§æ‡§Æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§

### `--disk_low_threshold`

- **What**: checkpoint saves ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§®‡•ç‡§Ø‡•Ç‡§®‡§§‡§Æ ‡§ñ‡§æ‡§≤‡•Ä disk space‡•§
- **Why**: disk full errors ‡§∏‡•á training crash ‡§ï‡•ã ‡§∞‡•ã‡§ï‡§§‡§æ ‡§π‡•à, ‡§ï‡§Æ space ‡§ï‡§æ ‡§ú‡§≤‡•ç‡§¶‡•Ä ‡§™‡§§‡§æ ‡§≤‡§ó‡§æ‡§ï‡§∞ configured action ‡§≤‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Format**: size string ‡§ú‡•à‡§∏‡•á `100G`, `50M`, `1T`, `500K`, ‡§Ø‡§æ plain bytes‡•§
- **Default**: None (feature disabled)

### `--disk_low_action`

- **What**: disk space threshold ‡§∏‡•á ‡§ï‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§≤‡§ø‡§Ø‡§æ ‡§ú‡§æ‡§®‡•á ‡§µ‡§æ‡§≤‡§æ action‡•§
- **Choices**: `stop`, `wait`, `script`
- **Default**: `stop`
- **Behavior**:
  - `stop`: error message ‡§ï‡•á ‡§∏‡§æ‡§• training ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
  - `wait`: space ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•ã‡§®‡•á ‡§§‡§ï ‡§π‡§∞ 30 seconds ‡§Æ‡•á‡§Ç loop ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ ‡§Ö‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§æ‡§≤ ‡§§‡§ï ‡§™‡•ç‡§∞‡§§‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§ï‡§∞ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à‡•§
  - `script`: space ‡§ñ‡§æ‡§≤‡•Ä ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è `--disk_low_script` ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ specified script ‡§ö‡§≤‡§æ‡§§‡§æ ‡§π‡•à‡•§

### `--disk_low_script`

- **What**: disk space ‡§ï‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§ö‡§≤‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è cleanup script ‡§ï‡§æ path‡•§
- **Why**: disk space ‡§ï‡§Æ ‡§π‡•ã‡§®‡•á ‡§™‡§∞ automated cleanup (‡§ú‡•à‡§∏‡•á ‡§™‡•Å‡§∞‡§æ‡§®‡•á checkpoints ‡§π‡§ü‡§æ‡§®‡§æ, cache clear ‡§ï‡§∞‡§®‡§æ) ‡§ï‡•Ä ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§
- **Notes**: ‡§ï‡•á‡§µ‡§≤ `--disk_low_action=script` ‡§π‡•ã‡§®‡•á ‡§™‡§∞ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ script executable ‡§π‡•ã‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§Ø‡§¶‡§ø script fail ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§Ø‡§æ ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ space ‡§ñ‡§æ‡§≤‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§§‡•Ä, training error ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∞‡•Å‡§ï ‡§ú‡§æ‡§è‡§ó‡•Ä‡•§
- **Default**: None

---

## üìä Logging and Monitoring

### `--logging_dir`

- **What**: TensorBoard logs ‡§ï‡•á ‡§≤‡§ø‡§è directory‡•§
- **Why**: training progress ‡§î‡§∞ performance metrics ‡§Æ‡•â‡§®‡§ø‡§ü‡§∞ ‡§ï‡§∞‡§®‡•á ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

### `--report_to`

- **What**: results ‡§î‡§∞ logs ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è platform ‡§®‡§ø‡§∞‡•ç‡§¶‡§ø‡§∑‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
- **Why**: TensorBoard, wandb, ‡§Ø‡§æ comet_ml ‡§ú‡•à‡§∏‡•Ä platforms ‡§ï‡•á ‡§∏‡§æ‡§• integration ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§ multiple trackers ‡§™‡§∞ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è comma ‡§∏‡•á ‡§Ö‡§≤‡§ó values ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç;
- **Choices**: wandb, tensorboard, comet_ml

## Environment configuration variables

‡§ä‡§™‡§∞ ‡§¶‡§ø‡§è ‡§ó‡§è ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§Ö‡§ß‡§ø‡§ï‡§§‡§∞ `config.json` ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡•á ‡§π‡•à‡§Ç ‚Äî ‡§≤‡•á‡§ï‡§ø‡§® ‡§ï‡•Å‡§õ entries `config.env` ‡§Æ‡•á‡§Ç ‡§∏‡•á‡§ü ‡§ï‡§∞‡§®‡•Ä ‡§™‡§°‡§º‡§§‡•Ä ‡§π‡•à‡§Ç‡•§

- `TRAINING_NUM_PROCESSES` ‡§ï‡•ã ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§Æ‡•á‡§Ç GPUs ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§™‡§∞ ‡§∏‡•á‡§ü ‡§ï‡§∞‡•á‡§Ç‡•§ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§Ç‡§∂ ‡§â‡§™‡§Ø‡•ã‡§ó‚Äë‡§Æ‡§æ‡§Æ‡§≤‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∏‡•á DistributedDataParallel (DDP) training ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§π‡•ã ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§¶‡§ø ‡§Ü‡§™ `config.env` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§®‡§π‡•Ä‡§Ç ‡§ï‡§∞‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á, ‡§§‡•ã `config.json` ‡§Æ‡•á‡§Ç `num_processes` ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç‡•§
- `TRAINING_DYNAMO_BACKEND` ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á `no` ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§á‡§∏‡•á ‡§ï‡§ø‡§∏‡•Ä ‡§≠‡•Ä ‡§∏‡§Æ‡§∞‡•ç‡§•‡§ø‡§§ torch.compile backend (‡§â‡§¶‡§æ. `inductor`, `aot_eager`, `cudagraphs`) ‡§™‡§∞ ‡§∏‡•á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§î‡§∞ `--dynamo_mode`, `--dynamo_fullgraph`, ‡§Ø‡§æ `--dynamo_use_regional_compilation` ‡§ï‡•á ‡§∏‡§æ‡§• finer tuning ‡§ï‡•á ‡§≤‡§ø‡§è ‡§ú‡•ã‡§°‡§º‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à
- `SIMPLETUNER_LOG_LEVEL` ‡§°‡§ø‡§´‡§º‡•â‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á `INFO` ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® issue reports ‡§ï‡•á ‡§≤‡§ø‡§è `debug.log` ‡§Æ‡•á‡§Ç ‡§Ö‡§ß‡§ø‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ú‡•ã‡§°‡§º‡§®‡•á ‡§π‡•á‡§§‡•Å ‡§á‡§∏‡•á `DEBUG` ‡§™‡§∞ ‡§∏‡•á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à
- `VENV_PATH` ‡§ï‡•ã ‡§Ü‡§™‡§ï‡•á python virtual env ‡§ï‡•Ä ‡§≤‡•ã‡§ï‡•á‡§∂‡§® ‡§™‡§∞ ‡§∏‡•á‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à ‡§Ø‡§¶‡§ø ‡§µ‡§π ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø `.venv` ‡§≤‡•ã‡§ï‡•á‡§∂‡§® ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à
- `ACCELERATE_EXTRA_ARGS` ‡§ï‡•ã unset ‡§õ‡•ã‡§°‡§º‡§æ ‡§ú‡§æ ‡§∏‡§ï‡§§‡§æ ‡§π‡•à, ‡§Ø‡§æ ‡§á‡§∏‡§Æ‡•á‡§Ç `--multi_gpu` ‡§Ø‡§æ FSDP‚Äëspecific flags ‡§ú‡•à‡§∏‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ arguments ‡§ú‡•ã‡§°‡§º‡•á ‡§ú‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç

---

‡§Ø‡§π ‡§è‡§ï ‡§¨‡•á‡§∏‡§ø‡§ï overview ‡§π‡•à ‡§§‡§æ‡§ï‡§ø ‡§Ü‡§™ ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡§∞ ‡§∏‡§ï‡•á‡§Ç‡•§ ‡§™‡•Ç‡§∞‡•ç‡§£ options ‡§∏‡•Ç‡§ö‡•Ä ‡§î‡§∞ ‡§Ö‡§ß‡§ø‡§ï ‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü‡•Ä‡§ï‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è, ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Ç‡§∞‡•Ä specification ‡§¶‡•á‡§ñ‡•á‡§Ç:

```
usage: train.py [-h] --model_family
                {kolors,auraflow,omnigen,flux,deepfloyd,cosmos2image,sana,qwen_image,pixart_sigma,sdxl,sd1x,sd2x,wan,hidream,sd3,lumina2,ltxvideo}
                [--model_flavour MODEL_FLAVOUR] [--controlnet [CONTROLNET]]
                [--pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH]
                --output_dir OUTPUT_DIR [--logging_dir LOGGING_DIR]
                --model_type {full,lora} [--seed SEED]
                [--resolution RESOLUTION]
                [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                [--prediction_type {epsilon,v_prediction,sample,flow_matching}]
                [--pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH]
                [--vae_dtype {default,fp32,fp16,bf16}]
                [--vae_cache_ondemand [VAE_CACHE_ONDEMAND]]
                [--accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL]
                [--aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}]
                [--base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_4_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL]
                [--offload_during_startup [OFFLOAD_DURING_STARTUP]]
                [--quantize_via {cpu,accelerator,pipeline}]
                [--quantization_config QUANTIZATION_CONFIG]
                [--fuse_qkv_projections [FUSE_QKV_PROJECTIONS]]
                [--control [CONTROL]]
                [--controlnet_custom_config CONTROLNET_CUSTOM_CONFIG]
                [--controlnet_model_name_or_path CONTROLNET_MODEL_NAME_OR_PATH]
                [--tread_config TREAD_CONFIG]
                [--pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH]
                [--pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER]
                [--pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH]
                [--pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER]
                [--pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH]
                [--pretrained_gemma_model_name_or_path PRETRAINED_GEMMA_MODEL_NAME_OR_PATH]
                [--revision REVISION] [--variant VARIANT]
                [--base_model_default_dtype {bf16,fp32}]
                [--unet_attention_slice [UNET_ATTENTION_SLICE]]
                [--num_train_epochs NUM_TRAIN_EPOCHS]
                [--max_train_steps MAX_TRAIN_STEPS]
                [--train_batch_size TRAIN_BATCH_SIZE]
                [--learning_rate LEARNING_RATE] --optimizer
                {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,prodigy}
                [--optimizer_config OPTIMIZER_CONFIG]
                [--lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                [--lr_warmup_steps LR_WARMUP_STEPS]
                [--checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT]
                [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                [--train_text_encoder [TRAIN_TEXT_ENCODER]]
                [--text_encoder_lr TEXT_ENCODER_LR]
                [--lr_num_cycles LR_NUM_CYCLES] [--lr_power LR_POWER]
                [--use_soft_min_snr [USE_SOFT_MIN_SNR]] [--use_ema [USE_EMA]]
                [--ema_device {accelerator,cpu}]
                [--ema_cpu_only [EMA_CPU_ONLY]]
                [--ema_update_interval EMA_UPDATE_INTERVAL]
                [--ema_foreach_disable [EMA_FOREACH_DISABLE]]
                [--ema_decay EMA_DECAY] [--lora_rank LORA_RANK]
                [--lora_alpha LORA_ALPHA] [--lora_type {standard,lycoris}]
                [--lora_dropout LORA_DROPOUT]
                [--lora_init_type {default,gaussian,loftq,olora,pissa}]
                [--peft_lora_mode {standard,singlora}]
                [--peft_lora_target_modules PEFT_LORA_TARGET_MODULES]
                [--singlora_ramp_up_steps SINGLORA_RAMP_UP_STEPS]
                [--init_lora INIT_LORA] [--lycoris_config LYCORIS_CONFIG]
                [--init_lokr_norm INIT_LOKR_NORM]
                [--flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano,controlnet,all+ffs+embedder,all+ffs+embedder+controlnet}]
                [--use_dora [USE_DORA]]
                [--resolution_type {pixel,area,pixel_area}]
                --data_backend_config DATA_BACKEND_CONFIG
                [--caption_strategy {filename,textfile,instance_prompt,parquet}]
                [--conditioning_multidataset_sampling {combined,random}]
                [--instance_prompt INSTANCE_PROMPT]
                [--parquet_caption_column PARQUET_CAPTION_COLUMN]
                [--parquet_filename_column PARQUET_FILENAME_COLUMN]
                [--ignore_missing_files [IGNORE_MISSING_FILES]]
                [--vae_cache_scan_behaviour {recreate,sync}]
                [--vae_enable_slicing [VAE_ENABLE_SLICING]]
                [--vae_enable_tiling [VAE_ENABLE_TILING]]
                [--vae_enable_patch_conv [VAE_ENABLE_PATCH_CONV]]
                [--vae_batch_size VAE_BATCH_SIZE]
                [--caption_dropout_probability CAPTION_DROPOUT_PROBABILITY]
                [--tokenizer_max_length TOKENIZER_MAX_LENGTH]
                [--validation_step_interval VALIDATION_STEP_INTERVAL]
                [--validation_epoch_interval VALIDATION_EPOCH_INTERVAL]
                [--disable_benchmark [DISABLE_BENCHMARK]]
                [--validation_prompt VALIDATION_PROMPT]
                [--num_validation_images NUM_VALIDATION_IMAGES]
                [--num_eval_images NUM_EVAL_IMAGES]
                [--eval_steps_interval EVAL_STEPS_INTERVAL]
                [--eval_epoch_interval EVAL_EPOCH_INTERVAL]
                [--eval_timesteps EVAL_TIMESTEPS]
                [--eval_dataset_pooling [EVAL_DATASET_POOLING]]
                [--evaluation_type {none,clip}]
                [--pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH]
                [--validation_guidance VALIDATION_GUIDANCE]
                [--validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS]
                [--validation_on_startup [VALIDATION_ON_STARTUP]]
                [--validation_using_datasets [VALIDATION_USING_DATASETS]]
                [--validation_torch_compile [VALIDATION_TORCH_COMPILE]]
                [--validation_guidance_real VALIDATION_GUIDANCE_REAL]
                [--validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP]
                [--validation_negative_prompt VALIDATION_NEGATIVE_PROMPT]
                [--validation_randomize [VALIDATION_RANDOMIZE]]
                [--validation_seed VALIDATION_SEED]
                [--validation_disable [VALIDATION_DISABLE]]
                [--validation_prompt_library [VALIDATION_PROMPT_LIBRARY]]
                [--user_prompt_library USER_PROMPT_LIBRARY]
                [--eval_dataset_id EVAL_DATASET_ID]
                [--validation_stitch_input_location {left,right}]
                [--validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE]
                [--validation_disable_unconditional [VALIDATION_DISABLE_UNCONDITIONAL]]
                [--validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS]
                [--validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START]
                [--validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP]
                [--validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE]
                [--validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH]
                [--validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc,dpm++,perflow}]
                [--validation_num_video_frames VALIDATION_NUM_VIDEO_FRAMES]
                [--validation_audio_only [VALIDATION_AUDIO_ONLY]]
                [--validation_resolution VALIDATION_RESOLUTION]
                [--validation_seed_source {cpu,gpu}]
                [--i_know_what_i_am_doing [I_KNOW_WHAT_I_AM_DOING]]
                [--flow_sigmoid_scale FLOW_SIGMOID_SCALE]
                [--flux_fast_schedule [FLUX_FAST_SCHEDULE]]
                [--flow_use_uniform_schedule [FLOW_USE_UNIFORM_SCHEDULE]]
                [--flow_use_beta_schedule [FLOW_USE_BETA_SCHEDULE]]
                [--flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA]
                [--flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA]
                [--flow_schedule_shift FLOW_SCHEDULE_SHIFT]
                [--flow_schedule_auto_shift [FLOW_SCHEDULE_AUTO_SHIFT]]
                [--flux_guidance_mode {constant,random-range}]
                [--flux_attention_masked_training [FLUX_ATTENTION_MASKED_TRAINING]]
                [--flux_guidance_value FLUX_GUIDANCE_VALUE]
                [--flux_guidance_min FLUX_GUIDANCE_MIN]
                [--flux_guidance_max FLUX_GUIDANCE_MAX]
                [--t5_padding {zero,unmodified}]
                [--sd3_clip_uncond_behaviour {empty_string,zero}]
                [--sd3_t5_uncond_behaviour {empty_string,zero}]
                [--soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA]
                [--mixed_precision {no,fp16,bf16,fp8}]
                [--attention_mechanism {diffusers,xformers,flash-attn,flash-attn-2,flash-attn-3,flash-attn-3-varlen,flex,cudnn,native-efficient,native-flash,native-math,native-npu,native-xla,sla,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}]
                [--sageattention_usage {training,inference,training+inference}]
                [--disable_tf32 [DISABLE_TF32]]
                [--set_grads_to_none [SET_GRADS_TO_NONE]]
                [--noise_offset NOISE_OFFSET]
                [--noise_offset_probability NOISE_OFFSET_PROBABILITY]
                [--input_perturbation INPUT_PERTURBATION]
                [--input_perturbation_steps INPUT_PERTURBATION_STEPS]
                [--lr_end LR_END] [--lr_scale [LR_SCALE]]
                [--lr_scale_sqrt [LR_SCALE_SQRT]]
                [--ignore_final_epochs [IGNORE_FINAL_EPOCHS]]
                [--freeze_encoder_before FREEZE_ENCODER_BEFORE]
                [--freeze_encoder_after FREEZE_ENCODER_AFTER]
                [--freeze_encoder_strategy {before,between,after}]
                [--layer_freeze_strategy {none,bitfit}]
                [--fully_unload_text_encoder [FULLY_UNLOAD_TEXT_ENCODER]]
                [--save_text_encoder [SAVE_TEXT_ENCODER]]
                [--text_encoder_limit TEXT_ENCODER_LIMIT]
                [--prepend_instance_prompt [PREPEND_INSTANCE_PROMPT]]
                [--only_instance_prompt [ONLY_INSTANCE_PROMPT]]
                [--data_aesthetic_score DATA_AESTHETIC_SCORE]
                [--delete_unwanted_images [DELETE_UNWANTED_IMAGES]]
                [--delete_problematic_images [DELETE_PROBLEMATIC_IMAGES]]
                [--disable_bucket_pruning [DISABLE_BUCKET_PRUNING]]
                [--disable_segmented_timestep_sampling [DISABLE_SEGMENTED_TIMESTEP_SAMPLING]]
                [--preserve_data_backend_cache [PRESERVE_DATA_BACKEND_CACHE]]
                [--override_dataset_config [OVERRIDE_DATASET_CONFIG]]
                [--cache_dir CACHE_DIR] [--cache_dir_text CACHE_DIR_TEXT]
                [--cache_dir_vae CACHE_DIR_VAE]
                [--compress_disk_cache [COMPRESS_DISK_CACHE]]
                [--aspect_bucket_disable_rebuild [ASPECT_BUCKET_DISABLE_REBUILD]]
                [--keep_vae_loaded [KEEP_VAE_LOADED]]
                [--skip_file_discovery SKIP_FILE_DISCOVERY]
                [--data_backend_sampling {uniform,auto-weighting}]
                [--image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE]
                [--write_batch_size WRITE_BATCH_SIZE]
                [--read_batch_size READ_BATCH_SIZE]
                [--enable_multiprocessing [ENABLE_MULTIPROCESSING]]
                [--max_workers MAX_WORKERS]
                [--aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS]
                [--torch_num_threads TORCH_NUM_THREADS]
                [--dataloader_prefetch [DATALOADER_PREFETCH]]
                [--dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN]
                [--aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT]
                [--aspect_bucket_alignment {8,16,24,32,64}]
                [--minimum_image_size MINIMUM_IMAGE_SIZE]
                [--maximum_image_size MAXIMUM_IMAGE_SIZE]
                [--target_downsample_size TARGET_DOWNSAMPLE_SIZE]
                [--max_upscale_threshold MAX_UPSCALE_THRESHOLD]
                [--metadata_update_interval METADATA_UPDATE_INTERVAL]
                [--debug_aspect_buckets [DEBUG_ASPECT_BUCKETS]]
                [--debug_dataset_loader [DEBUG_DATASET_LOADER]]
                [--print_filenames [PRINT_FILENAMES]]
                [--print_sampler_statistics [PRINT_SAMPLER_STATISTICS]]
                [--timestep_bias_strategy {earlier,later,range,none}]
                [--timestep_bias_begin TIMESTEP_BIAS_BEGIN]
                [--timestep_bias_end TIMESTEP_BIAS_END]
                [--timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER]
                [--timestep_bias_portion TIMESTEP_BIAS_PORTION]
                [--training_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--inference_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--loss_type {l2,huber,smooth_l1}]
                [--huber_schedule {snr,exponential,constant}]
                [--huber_c HUBER_C] [--snr_gamma SNR_GAMMA]
                [--masked_loss_probability MASKED_LOSS_PROBABILITY]
                [--hidream_use_load_balancing_loss [HIDREAM_USE_LOAD_BALANCING_LOSS]]
                [--hidream_load_balancing_loss_weight HIDREAM_LOAD_BALANCING_LOSS_WEIGHT]
                [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]
                [--optimizer_beta1 OPTIMIZER_BETA1]
                [--optimizer_beta2 OPTIMIZER_BETA2]
                [--optimizer_cpu_offload_method {none}]
                [--gradient_precision {unmodified,fp32}]
                [--adam_weight_decay ADAM_WEIGHT_DECAY]
                [--adam_epsilon ADAM_EPSILON] [--prodigy_steps PRODIGY_STEPS]
                [--max_grad_norm MAX_GRAD_NORM]
                [--grad_clip_method {value,norm}]
                [--optimizer_offload_gradients [OPTIMIZER_OFFLOAD_GRADIENTS]]
                [--fuse_optimizer [FUSE_OPTIMIZER]]
                [--optimizer_release_gradients [OPTIMIZER_RELEASE_GRADIENTS]]
                [--push_to_hub [PUSH_TO_HUB]]
                [--push_to_hub_background [PUSH_TO_HUB_BACKGROUND]]
                [--push_checkpoints_to_hub [PUSH_CHECKPOINTS_TO_HUB]]
                [--publishing_config PUBLISHING_CONFIG]
                [--hub_model_id HUB_MODEL_ID]
                [--model_card_private [MODEL_CARD_PRIVATE]]
                [--model_card_safe_for_work [MODEL_CARD_SAFE_FOR_WORK]]
                [--model_card_note MODEL_CARD_NOTE]
                [--modelspec_comment MODELSPEC_COMMENT]
                [--report_to {tensorboard,wandb,comet_ml,all,none}]
                [--checkpoint_step_interval CHECKPOINT_STEP_INTERVAL]
                [--checkpoint_epoch_interval CHECKPOINT_EPOCH_INTERVAL]
                [--checkpointing_rolling_steps CHECKPOINTING_ROLLING_STEPS]
                [--checkpointing_use_tempdir [CHECKPOINTING_USE_TEMPDIR]]
                [--checkpoints_rolling_total_limit CHECKPOINTS_ROLLING_TOTAL_LIMIT]
                [--tracker_run_name TRACKER_RUN_NAME]
                [--tracker_project_name TRACKER_PROJECT_NAME]
                [--tracker_image_layout {gallery,table}]
                [--enable_watermark [ENABLE_WATERMARK]]
                [--framerate FRAMERATE]
                [--seed_for_each_device [SEED_FOR_EACH_DEVICE]]
                [--snr_weight SNR_WEIGHT]
                [--rescale_betas_zero_snr [RESCALE_BETAS_ZERO_SNR]]
                [--webhook_config WEBHOOK_CONFIG]
                [--webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL]
                [--distillation_method {lcm,dcm,dmd,perflow}]
                [--distillation_config DISTILLATION_CONFIG]
                [--ema_validation {none,ema_only,comparison}]
                [--local_rank LOCAL_RANK] [--ltx_train_mode {t2v,i2v}]
                [--ltx_i2v_prob LTX_I2V_PROB]
                [--ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION]
                [--ltx_protect_first_frame [LTX_PROTECT_FIRST_FRAME]]
                [--offload_param_path OFFLOAD_PARAM_PATH]
                [--offset_noise [OFFSET_NOISE]]
                [--quantize_activations [QUANTIZE_ACTIVATIONS]]
                [--refiner_training [REFINER_TRAINING]]
                [--refiner_training_invert_schedule [REFINER_TRAINING_INVERT_SCHEDULE]]
                [--refiner_training_strength REFINER_TRAINING_STRENGTH]
                [--sdxl_refiner_uses_full_range [SDXL_REFINER_USES_FULL_RANGE]]
                [--sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION]

The following SimpleTuner command-line options are available:

options:
  -h, --help            show this help message and exit
  --model_family {kolors,auraflow,omnigen,flux,deepfloyd,cosmos2image,sana,qwen_image,pixart_sigma,sdxl,sd1x,sd2x,wan,hidream,sd3,lumina2,ltxvideo}
                        The base model architecture family to train
  --model_flavour MODEL_FLAVOUR
                        Specific variant of the selected model family
  --controlnet [CONTROLNET]
                        Train ControlNet (full or LoRA) branches alongside the
                        primary network.
  --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH
                        Optional override of the model checkpoint. Leave blank
                        to use the default path for the selected model
                        flavour.
  --output_dir OUTPUT_DIR
                        Directory where model checkpoints and logs will be
                        saved
  --logging_dir LOGGING_DIR
                        Directory for TensorBoard logs
  --model_type {full,lora}
                        Choose between full model training or LoRA adapter
                        training
  --seed SEED           Seed used for deterministic training behaviour
  --resolution RESOLUTION
                        Resolution for training images
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        Select checkpoint to resume training from
  --prediction_type {epsilon,v_prediction,sample,flow_matching}
                        The parameterization type for the diffusion model
  --pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH
                        Path to pretrained VAE model
  --vae_dtype {default,fp32,fp16,bf16}
                        Precision for VAE encoding/decoding. Lower precision
                        saves memory.
  --vae_cache_ondemand [VAE_CACHE_ONDEMAND]
                        Process VAE latents during training instead of
                        precomputing them
  --vae_cache_disable [VAE_CACHE_DISABLE]
                        Implicitly enables on-demand caching and disables
                        writing embeddings to disk.
  --accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL
                        Clear the cache from VRAM every X steps to prevent
                        memory leaks
  --aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}
                        Number of decimal places to round aspect ratios to for
                        bucket creation
  --base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for loading the base model. Lower precision
                        saves memory.
  --text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_4_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL
                        Checkpoint every N transformer blocks
  --offload_during_startup [OFFLOAD_DURING_STARTUP]
                        Offload text encoders to CPU during VAE caching
  --quantize_via {cpu,accelerator,pipeline}
                        Where to perform model quantization
  --quantization_config QUANTIZATION_CONFIG
                        JSON or file path describing Diffusers quantization
                        config for pipeline quantization
  --fuse_qkv_projections [FUSE_QKV_PROJECTIONS]
                        Enables Flash Attention 3 when supported; otherwise
                        falls back to PyTorch SDPA.
  --control [CONTROL]   Enable channel-wise control style training
  --controlnet_custom_config CONTROLNET_CUSTOM_CONFIG
                        Custom configuration for ControlNet models
  --controlnet_model_name_or_path CONTROLNET_MODEL_NAME_OR_PATH
                        Path to ControlNet model weights to preload
  --tread_config TREAD_CONFIG
                        Configuration for TREAD training method
  --pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH
                        Path to pretrained transformer model
  --pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER
                        Subfolder containing transformer model weights
  --pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH
                        Path to pretrained UNet model
  --pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER
                        Subfolder containing UNet model weights
  --pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH
                        Path to pretrained T5 model
  --pretrained_gemma_model_name_or_path PRETRAINED_GEMMA_MODEL_NAME_OR_PATH
                        Path to pretrained Gemma model
  --revision REVISION   Git branch/tag/commit for model version
  --variant VARIANT     Model variant (e.g., fp16, bf16)
  --base_model_default_dtype {bf16,fp32}
                        Default precision for quantized base model weights
  --unet_attention_slice [UNET_ATTENTION_SLICE]
                        Enable attention slicing for SDXL UNet
  --num_train_epochs NUM_TRAIN_EPOCHS
                        Number of times to iterate through the entire dataset
  --max_train_steps MAX_TRAIN_STEPS
                        Maximum number of training steps (0 = use epochs
                        instead)
  --train_batch_size TRAIN_BATCH_SIZE
                        Number of samples processed per forward/backward pass
                        (per device).
  --learning_rate LEARNING_RATE
                        Base learning rate for training
  --optimizer {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,prodigy}
                        Optimization algorithm for training
  --optimizer_config OPTIMIZER_CONFIG
                        Comma-separated key=value pairs forwarded to the
                        selected optimizer
  --lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}
                        How learning rate changes during training
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of steps to accumulate gradients
  --lr_warmup_steps LR_WARMUP_STEPS
                        Number of steps to gradually increase LR from 0
  --checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT
                        Maximum number of checkpoints to keep on disk
  --gradient_checkpointing [GRADIENT_CHECKPOINTING]
                        Trade compute for memory during training
  --train_text_encoder [TRAIN_TEXT_ENCODER]
                        Also train the text encoder (CLIP) model
  --text_encoder_lr TEXT_ENCODER_LR
                        Separate learning rate for text encoder
  --lr_num_cycles LR_NUM_CYCLES
                        Number of cosine annealing cycles
  --lr_power LR_POWER   Power for polynomial decay scheduler
  --use_soft_min_snr [USE_SOFT_MIN_SNR]
                        Use soft clamping instead of hard clamping for Min-SNR
  --use_ema [USE_EMA]   Maintain an exponential moving average copy of the
                        model during training.
  --ema_device {accelerator,cpu}
                        Where to keep the EMA weights in-between updates.
  --ema_cpu_only [EMA_CPU_ONLY]
                        Keep EMA weights exclusively on CPU even when
                        ema_device would normally move them.
  --ema_update_interval EMA_UPDATE_INTERVAL
                        Update EMA weights every N optimizer steps
  --ema_foreach_disable [EMA_FOREACH_DISABLE]
                        Fallback to standard tensor ops instead of
                        torch.foreach updates.
  --ema_decay EMA_DECAY
                        Smoothing factor for EMA updates (closer to 1.0 =
                        slower drift).
  --lora_rank LORA_RANK
                        Dimension of LoRA update matrices
  --lora_alpha LORA_ALPHA
                        Scaling factor for LoRA updates
  --lora_type {standard,lycoris}
                        LoRA implementation type
  --lora_dropout LORA_DROPOUT
                        LoRA dropout randomly ignores neurons during training.
                        This can help prevent overfitting.
  --lora_init_type {default,gaussian,loftq,olora,pissa}
                        The initialization type for the LoRA model
  --peft_lora_mode {standard,singlora}
                        PEFT LoRA training mode
  --peft_lora_target_modules PEFT_LORA_TARGET_MODULES
                        JSON array (or path to a JSON file) listing PEFT
                        LoRA target module names. Overrides preset targets.
  --singlora_ramp_up_steps SINGLORA_RAMP_UP_STEPS
                        Number of ramp-up steps for SingLoRA
  --slider_lora_target [SLIDER_LORA_TARGET]
                        Route LoRA training to slider-friendly targets
                        (self-attn + conv/time embeddings). Only affects
                        standard PEFT LoRA.
  --init_lora INIT_LORA
                        Specify an existing LoRA or LyCORIS safetensors file
                        to initialize the adapter
  --lycoris_config LYCORIS_CONFIG
                        Path to LyCORIS configuration JSON file
  --init_lokr_norm INIT_LOKR_NORM
                        Perturbed normal initialization for LyCORIS LoKr
                        layers
  --flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano,controlnet,all+ffs+embedder,all+ffs+embedder+controlnet}
                        Which layers to train in Flux models
  --use_dora [USE_DORA]
                        Enable DoRA (Weight-Decomposed LoRA)
  --resolution_type {pixel,area,pixel_area}
                        How to interpret the resolution value
  --data_backend_config DATA_BACKEND_CONFIG
                        Select a saved dataset configuration (managed in
                        Datasets & Environments tabs)
  --caption_strategy {filename,textfile,instance_prompt,parquet}
                        How to load captions for images
  --conditioning_multidataset_sampling {combined,random}
                        How to sample from multiple conditioning datasets
  --instance_prompt INSTANCE_PROMPT
                        Instance prompt for training
  --parquet_caption_column PARQUET_CAPTION_COLUMN
                        Column name containing captions in parquet files
  --parquet_filename_column PARQUET_FILENAME_COLUMN
                        Column name containing image paths in parquet files
  --ignore_missing_files [IGNORE_MISSING_FILES]
                        Continue training even if some files are missing
  --vae_cache_scan_behaviour {recreate,sync}
                        How to scan VAE cache for missing files
  --vae_enable_slicing [VAE_ENABLE_SLICING]
                        Enable VAE attention slicing for memory efficiency
  --vae_enable_tiling [VAE_ENABLE_TILING]
                        Enable VAE tiling for large images
  --vae_enable_patch_conv [VAE_ENABLE_PATCH_CONV]
                        Enable patch-based 3D conv for HunyuanVideo VAE to
                        reduce peak VRAM (slight slowdown)
  --vae_batch_size VAE_BATCH_SIZE
                        Batch size for VAE encoding during caching
  --caption_dropout_probability CAPTION_DROPOUT_PROBABILITY
                        Caption dropout will randomly drop captions and, for
                        SDXL, size conditioning inputs based on this
                        probability
  --tokenizer_max_length TOKENIZER_MAX_LENGTH
                        Override the tokenizer sequence length (advanced).
  --validation_step_interval VALIDATION_STEP_INTERVAL
                        Run validation every N training steps (deprecated alias: --validation_steps)
  --validation_epoch_interval VALIDATION_EPOCH_INTERVAL
                        Run validation every N training epochs
  --disable_benchmark [DISABLE_BENCHMARK]
                        Skip generating baseline comparison images before
                        training starts
  --validation_prompt VALIDATION_PROMPT
                        Prompt to use for validation images
  --num_validation_images NUM_VALIDATION_IMAGES
                        Number of images to generate per validation
  --num_eval_images NUM_EVAL_IMAGES
                        Number of images to generate for evaluation metrics
  --eval_steps_interval EVAL_STEPS_INTERVAL
                        Run evaluation every N training steps
  --eval_epoch_interval EVAL_EPOCH_INTERVAL
                        Run evaluation every N training epochs (decimals run
                        multiple times per epoch)
  --eval_timesteps EVAL_TIMESTEPS
                        Number of timesteps for evaluation
  --eval_dataset_pooling [EVAL_DATASET_POOLING]
                        Combine evaluation metrics from all datasets into a
                        single chart
  --evaluation_type {none,clip}
                        Type of evaluation metrics to compute
  --pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH
                        Path to pretrained model for evaluation metrics
  --validation_guidance VALIDATION_GUIDANCE
                        CFG guidance scale for validation images
  --validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS
                        Number of diffusion steps for validation renders
  --validation_on_startup [VALIDATION_ON_STARTUP]
                        Run validation on the base model before training
                        starts
  --validation_using_datasets [VALIDATION_USING_DATASETS]
                        Use random images from training datasets for
                        validation
  --validation_torch_compile [VALIDATION_TORCH_COMPILE]
                        Use torch.compile() on validation pipeline for speed
  --validation_guidance_real VALIDATION_GUIDANCE_REAL
                        CFG value for distilled models (e.g., FLUX schnell)
  --validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP
                        Skip CFG for initial timesteps (Flux only)
  --validation_negative_prompt VALIDATION_NEGATIVE_PROMPT
                        Negative prompt for validation images
  --validation_randomize [VALIDATION_RANDOMIZE]
                        Use random seeds for each validation
  --validation_seed VALIDATION_SEED
                        Fixed seed for reproducible validation images
  --validation_disable [VALIDATION_DISABLE]
                        Completely disable validation image generation
  --validation_prompt_library [VALIDATION_PROMPT_LIBRARY]
                        Use SimpleTuner's built-in prompt library
  --user_prompt_library USER_PROMPT_LIBRARY
                        Path to custom JSON prompt library
  --eval_dataset_id EVAL_DATASET_ID
                        Specific dataset to use for evaluation metrics
  --validation_stitch_input_location {left,right}
                        Where to place input image in img2img validations
  --validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE
                        CFG rescale value for validation
  --validation_disable_unconditional [VALIDATION_DISABLE_UNCONDITIONAL]
                        Disable unconditional image generation during
                        validation
  --validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS
                        JSON list of transformer layers to skip during
                        classifier-free guidance
  --validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START
                        Starting layer index to skip guidance
  --validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP
                        Ending layer index to skip guidance
  --validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE
                        Scale guidance strength when applying layer skipping
  --validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH
                        Strength multiplier for LyCORIS validation
  --validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc,dpm++,perflow}
                        Noise scheduler for validation
  --validation_num_video_frames VALIDATION_NUM_VIDEO_FRAMES
                        Number of frames for video validation
  --validation_audio_only [VALIDATION_AUDIO_ONLY]
                        Disable video generation during validation and emit
                        audio only
  --validation_resolution VALIDATION_RESOLUTION
                        Override resolution for validation images (pixels or
                        megapixels)
  --validation_seed_source {cpu,gpu}
                        Source device used to generate validation seeds
  --i_know_what_i_am_doing [I_KNOW_WHAT_I_AM_DOING]
                        Unlock experimental overrides and bypass built-in
                        safety limits.
  --flow_sigmoid_scale FLOW_SIGMOID_SCALE
                        Scale factor for sigmoid timestep sampling for flow-
                        matching models.
  --flux_fast_schedule [FLUX_FAST_SCHEDULE]
                        Use experimental fast schedule for Flux training
  --flow_use_uniform_schedule [FLOW_USE_UNIFORM_SCHEDULE]
                        Use uniform schedule instead of sigmoid for flow-
                        matching
  --flow_use_beta_schedule [FLOW_USE_BETA_SCHEDULE]
                        Use beta schedule instead of sigmoid for flow-matching
  --flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA
                        Alpha value for beta schedule (default: 2.0)
  --flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA
                        Beta value for beta schedule (default: 2.0)
  --flow_schedule_shift FLOW_SCHEDULE_SHIFT
                        Shift the noise schedule for flow-matching models
  --flow_schedule_auto_shift [FLOW_SCHEDULE_AUTO_SHIFT]
                        Auto-adjust schedule shift based on image resolution
  --flux_guidance_mode {constant,random-range}
                        Guidance mode for Flux training
  --flux_attention_masked_training [FLUX_ATTENTION_MASKED_TRAINING]
                        Enable attention masked training for Flux models
  --flux_guidance_value FLUX_GUIDANCE_VALUE
                        Guidance value for constant mode
  --flux_guidance_min FLUX_GUIDANCE_MIN
                        Minimum guidance value for random-range mode
  --flux_guidance_max FLUX_GUIDANCE_MAX
                        Maximum guidance value for random-range mode
  --t5_padding {zero,unmodified}
                        Padding behavior for T5 text encoder
  --sd3_clip_uncond_behaviour {empty_string,zero}
                        How SD3 handles unconditional prompts
  --sd3_t5_uncond_behaviour {empty_string,zero}
                        How SD3 T5 handles unconditional prompts
  --soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA
                        Sigma data for soft min SNR weighting
  --mixed_precision {no,fp16,bf16,fp8}
                        Precision for training computations
  --attention_mechanism {diffusers,xformers,flash-attn,flash-attn-2,flash-attn-3,flash-attn-3-varlen,flex,cudnn,native-efficient,native-flash,native-math,native-npu,native-xla,sla,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}
                        Attention computation backend
  --sageattention_usage {training,inference,training+inference}
                        When to use SageAttention
  --disable_tf32 [DISABLE_TF32]
                        Force IEEE FP32 precision (disables TF32) using
                        PyTorch's fp32_precision controls when available
  --set_grads_to_none [SET_GRADS_TO_NONE]
                        Set gradients to None instead of zero
  --noise_offset NOISE_OFFSET
                        Add noise offset to training
  --noise_offset_probability NOISE_OFFSET_PROBABILITY
                        Probability of applying noise offset
  --input_perturbation INPUT_PERTURBATION
                        Add additional noise only to the inputs fed to the
                        model during training
  --input_perturbation_steps INPUT_PERTURBATION_STEPS
                        Only apply input perturbation over the first N steps
                        with linear decay
  --lr_end LR_END       A polynomial learning rate will end up at this value
                        after the specified number of warmup steps
  --lr_scale [LR_SCALE]
                        Scale the learning rate by the number of GPUs,
                        gradient accumulation steps, and batch size
  --lr_scale_sqrt [LR_SCALE_SQRT]
                        If using --lr_scale, use the square root of (number of
                        GPUs * gradient accumulation steps * batch size)
  --ignore_final_epochs [IGNORE_FINAL_EPOCHS]
                        When provided, the max epoch counter will not
                        determine the end of the training run
  --freeze_encoder_before FREEZE_ENCODER_BEFORE
                        When using 'before' strategy, we will freeze layers
                        earlier than this
  --freeze_encoder_after FREEZE_ENCODER_AFTER
                        When using 'after' strategy, we will freeze layers
                        later than this
  --freeze_encoder_strategy {before,between,after}
                        When freezing the text encoder, we can use the
                        'before', 'between', or 'after' strategy
  --layer_freeze_strategy {none,bitfit}
                        When freezing parameters, we can use the 'none' or
                        'bitfit' strategy
  --fully_unload_text_encoder [FULLY_UNLOAD_TEXT_ENCODER]
                        If set, will fully unload the text_encoder from memory
                        when not in use
  --save_text_encoder [SAVE_TEXT_ENCODER]
                        If set, will save the text encoder after training
  --text_encoder_limit TEXT_ENCODER_LIMIT
                        When training the text encoder, we want to limit how
                        long it trains for to avoid catastrophic loss
  --prepend_instance_prompt [PREPEND_INSTANCE_PROMPT]
                        When determining the captions from the filename,
                        prepend the instance prompt as an enforced keyword
  --only_instance_prompt [ONLY_INSTANCE_PROMPT]
                        Use the instance prompt instead of the caption from
                        filename
  --data_aesthetic_score DATA_AESTHETIC_SCORE
                        Since currently we do not calculate aesthetic scores
                        for data, we will statically set it to one value. This
                        is only used by the SDXL Refiner
  --delete_unwanted_images [DELETE_UNWANTED_IMAGES]
                        If set, will delete images that are not of a minimum
                        size to save on disk space for large training runs
  --delete_problematic_images [DELETE_PROBLEMATIC_IMAGES]
                        If set, any images that error out during load will be
                        removed from the underlying storage medium
  --disable_bucket_pruning [DISABLE_BUCKET_PRUNING]
                        When training on very small datasets, you might not
                        care that the batch sizes will outpace your image
                        count. Setting this option will prevent SimpleTuner
                        from deleting your bucket lists that do not meet the
                        minimum image count requirements. Use at your own
                        risk, it may end up throwing off your statistics or
                        epoch tracking
  --disable_segmented_timestep_sampling [DISABLE_SEGMENTED_TIMESTEP_SAMPLING]
                        By default, the timestep schedule is divided into
                        roughly `train_batch_size` number of segments, and
                        then each of those are sampled from separately. This
                        improves the selection distribution, but may not be
                        desired in certain training scenarios, eg. when
                        limiting the timestep selection range
  --preserve_data_backend_cache [PRESERVE_DATA_BACKEND_CACHE]
                        For very large cloud storage buckets that will never
                        change, enabling this option will prevent the trainer
                        from scanning it at startup, by preserving the cache
                        files that we generate. Be careful when using this,
                        as, switching datasets can result in the preserved
                        cache being used, which would be problematic.
                        Currently, cache is not stored in the dataset itself
                        but rather, locally. This may change in a future
                        release
  --override_dataset_config [OVERRIDE_DATASET_CONFIG]
                        When provided, the dataset's config will not be
                        checked against the live backend config
  --cache_dir CACHE_DIR
                        The directory where the downloaded models and datasets
                        will be stored
  --cache_dir_text CACHE_DIR_TEXT
                        This is the path to a local directory that will
                        contain your text embed cache
  --cache_dir_vae CACHE_DIR_VAE
                        This is the path to a local directory that will
                        contain your VAE outputs
  --compress_disk_cache [COMPRESS_DISK_CACHE]
                        If set, will gzip-compress the disk cache for Pytorch
                        files. This will save substantial disk space, but may
                        slow down the training process
  --aspect_bucket_disable_rebuild [ASPECT_BUCKET_DISABLE_REBUILD]
                        When using a randomised aspect bucket list, the VAE
                        and aspect cache are rebuilt on each epoch. With a
                        large and diverse enough dataset, rebuilding the
                        aspect list may take a long time, and this may be
                        undesirable. This option will not override
                        vae_cache_clear_each_epoch. If both options are
                        provided, only the VAE cache will be rebuilt
  --keep_vae_loaded [KEEP_VAE_LOADED]
                        If set, will keep the VAE loaded in memory. This can
                        reduce disk churn, but consumes VRAM during the
                        forward pass
  --skip_file_discovery SKIP_FILE_DISCOVERY
                        Comma-separated values of which stages to skip
                        discovery for. Skipping any stage will speed up
                        resumption, but will increase the risk of errors, as
                        missing images or incorrectly bucketed images may not
                        be caught. Valid options: aspect, vae, text, metadata
  --data_backend_sampling {uniform,auto-weighting}
                        When using multiple data backends, the sampling
                        weighting can be set to 'uniform' or 'auto-weighting'
  --image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE
                        When resizing and cropping images, we do it in
                        parallel using processes or threads. This defines how
                        many images will be read into the queue before they
                        are processed
  --write_batch_size WRITE_BATCH_SIZE
                        When using certain storage backends, it is better to
                        batch smaller writes rather than continuous
                        dispatching. In SimpleTuner, write batching is
                        currently applied during VAE caching, when many small
                        objects are written. This mostly applies to S3, but
                        some shared server filesystems may benefit as well.
                        Default: 64
  --read_batch_size READ_BATCH_SIZE
                        Used by the VAE cache to prefetch image data. This is
                        the number of images to read ahead
  --enable_multiprocessing [ENABLE_MULTIPROCESSING]
                        If set, will use processes instead of threads during
                        metadata caching operations
  --max_workers MAX_WORKERS
                        How many active threads or processes to run during VAE
                        caching
  --aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS
                        When using AWS backends, the maximum number of
                        connections to keep open to the S3 bucket at a single
                        time
  --torch_num_threads TORCH_NUM_THREADS
                        The number of threads to use for PyTorch operations.
                        This is not the same as the number of workers
  --dataloader_prefetch [DATALOADER_PREFETCH]
                        When provided, the dataloader will read-ahead and
                        attempt to retrieve latents, text embeds, and other
                        metadata ahead of the time when the batch is required,
                        so that it can be immediately available
  --dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN
                        Set the number of prefetched batches
  --aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT
                        The number of workers to use for aspect bucketing.
                        This is a CPU-bound task, so the number of workers
                        should be set to the number of CPU threads available.
                        If you use an I/O bound backend, an even higher value
                        may make sense. Default: 12
  --aspect_bucket_alignment {8,16,24,32,64}
                        When training diffusion models, the image sizes
                        generally must align to a 64 pixel interval
  --minimum_image_size MINIMUM_IMAGE_SIZE
                        The minimum resolution for both sides of input images
  --maximum_image_size MAXIMUM_IMAGE_SIZE
                        When cropping images that are excessively large, the
                        entire scene context may be lost, eg. the crop might
                        just end up being a portion of the background. To
                        avoid this, a maximum image size may be provided,
                        which will result in very-large images being
                        downsampled before cropping them. This value uses
                        --resolution_type to determine whether it is a pixel
                        edge or megapixel value
  --target_downsample_size TARGET_DOWNSAMPLE_SIZE
                        When using --maximum_image_size, very-large images
                        exceeding that value will be downsampled to this
                        target size before cropping
  --max_upscale_threshold MAX_UPSCALE_THRESHOLD
                        Limit upscaling of small images to prevent quality
                        degradation (opt-in). When set, filters out aspect
                        buckets requiring upscaling beyond this threshold.
                        For example, 0.2 allows up to 20% upscaling. Default
                        (None) allows unlimited upscaling. Must be between 0
                        and 1.
  --metadata_update_interval METADATA_UPDATE_INTERVAL
                        When generating the aspect bucket indicies, we want to
                        save it every X seconds
  --debug_aspect_buckets [DEBUG_ASPECT_BUCKETS]
                        If set, will print excessive debugging for aspect
                        bucket operations
  --debug_dataset_loader [DEBUG_DATASET_LOADER]
                        If set, will print excessive debugging for data loader
                        operations
  --print_filenames [PRINT_FILENAMES]
                        If any image files are stopping the process eg. due to
                        corruption or truncation, this will help identify
                        which is at fault
  --print_sampler_statistics [PRINT_SAMPLER_STATISTICS]
                        If provided, will print statistics about the dataset
                        sampler. This is useful for debugging
  --timestep_bias_strategy {earlier,later,range,none}
                        Strategy for biasing timestep sampling
  --timestep_bias_begin TIMESTEP_BIAS_BEGIN
                        Beginning of timestep bias range
  --timestep_bias_end TIMESTEP_BIAS_END
                        End of timestep bias range
  --timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER
                        Multiplier for timestep bias probability
  --timestep_bias_portion TIMESTEP_BIAS_PORTION
                        Portion of training steps to apply timestep bias
  --training_scheduler_timestep_spacing {leading,linspace,trailing}
                        Timestep spacing for training scheduler
  --inference_scheduler_timestep_spacing {leading,linspace,trailing}
                        Timestep spacing for inference scheduler
  --loss_type {l2,huber,smooth_l1}
                        Loss function for training
  --huber_schedule {snr,exponential,constant}
                        Schedule for Huber loss transition threshold
  --huber_c HUBER_C     Transition point between L2 and L1 regions for Huber
                        loss
  --snr_gamma SNR_GAMMA
                        SNR weighting gamma value (0 = disabled)
  --masked_loss_probability MASKED_LOSS_PROBABILITY
                        Probability of applying masked loss weighting per
                        batch
  --hidream_use_load_balancing_loss [HIDREAM_USE_LOAD_BALANCING_LOSS]
                        Apply experimental load balancing loss when training
                        HiDream models.
  --hidream_load_balancing_loss_weight HIDREAM_LOAD_BALANCING_LOSS_WEIGHT
                        Strength multiplier for HiDream load balancing loss.
  --adam_beta1 ADAM_BETA1
                        First moment decay rate for Adam optimizers
  --adam_beta2 ADAM_BETA2
                        Second moment decay rate for Adam optimizers
  --optimizer_beta1 OPTIMIZER_BETA1
                        First moment decay rate for optimizers
  --optimizer_beta2 OPTIMIZER_BETA2
                        Second moment decay rate for optimizers
  --optimizer_cpu_offload_method {none}
                        Method for CPU offloading optimizer states
  --gradient_precision {unmodified,fp32}
                        Precision for gradient computation
  --adam_weight_decay ADAM_WEIGHT_DECAY
                        L2 regularisation strength for Adam-family optimizers.
  --adam_epsilon ADAM_EPSILON
                        Small constant added for numerical stability.
  --prodigy_steps PRODIGY_STEPS
                        Number of steps Prodigy should spend adapting its
                        learning rate.
  --max_grad_norm MAX_GRAD_NORM
                        Gradient clipping threshold to prevent exploding
                        gradients.
  --grad_clip_method {value,norm}
                        Strategy for applying max_grad_norm during clipping.
  --optimizer_offload_gradients [OPTIMIZER_OFFLOAD_GRADIENTS]
                        Move optimizer gradients to CPU to save GPU memory.
  --fuse_optimizer [FUSE_OPTIMIZER]
                        Enable fused kernels when offloading to reduce memory
                        overhead.
  --optimizer_release_gradients [OPTIMIZER_RELEASE_GRADIENTS]
                        Free gradient tensors immediately after optimizer step
                        when using Optimi optimizers.
  --push_to_hub [PUSH_TO_HUB]
                        Automatically upload the trained model to your Hugging
                        Face Hub repository.
  --push_to_hub_background [PUSH_TO_HUB_BACKGROUND]
                        Run Hub uploads in a background worker so training is
                        not blocked while pushing.
  --push_checkpoints_to_hub [PUSH_CHECKPOINTS_TO_HUB]
                        Upload intermediate checkpoints to the same Hugging
                        Face repository during training.
  --publishing_config PUBLISHING_CONFIG
                        Optional JSON/file path describing additional
                        publishing targets (S3/Backblaze B2/Azure Blob/Dropbox).
  --hub_model_id HUB_MODEL_ID
                        If left blank, SimpleTuner derives a name from the
                        project settings when pushing to Hub.
  --model_card_private [MODEL_CARD_PRIVATE]
                        Create the Hugging Face repository as private instead
                        of public.
  --model_card_safe_for_work [MODEL_CARD_SAFE_FOR_WORK]
                        Remove the default NSFW warning from the generated
                        model card on Hugging Face Hub.
  --model_card_note MODEL_CARD_NOTE
                        Optional note that appears at the top of the generated
                        model card.
  --modelspec_comment MODELSPEC_COMMENT
                        Text embedded in safetensors file metadata as
                        modelspec.comment, visible in external model viewers.
  --report_to {tensorboard,wandb,comet_ml,all,none}
                        Where to log training metrics
  --checkpoint_step_interval CHECKPOINT_STEP_INTERVAL
                        Save model checkpoint every N steps (deprecated alias: --checkpointing_steps)
  --checkpoint_epoch_interval CHECKPOINT_EPOCH_INTERVAL
                        Save model checkpoint every N epochs
  --checkpointing_rolling_steps CHECKPOINTING_ROLLING_STEPS
                        Rolling checkpoint window size for continuous
                        checkpointing
  --checkpointing_use_tempdir [CHECKPOINTING_USE_TEMPDIR]
                        Use temporary directory for checkpoint files before
                        final save
  --checkpoints_rolling_total_limit CHECKPOINTS_ROLLING_TOTAL_LIMIT
                        Maximum number of rolling checkpoints to keep
  --tracker_run_name TRACKER_RUN_NAME
                        Name for this training run in tracking platforms
  --tracker_project_name TRACKER_PROJECT_NAME
                        Project name in tracking platforms
  --tracker_image_layout {gallery,table}
                        How validation images are displayed in trackers
  --enable_watermark [ENABLE_WATERMARK]
                        Add invisible watermark to generated images
  --framerate FRAMERATE
                        Framerate for video model training
  --seed_for_each_device [SEED_FOR_EACH_DEVICE]
                        Use a unique deterministic seed per GPU instead of
                        sharing one seed across devices.
  --snr_weight SNR_WEIGHT
                        Weight factor for SNR-based loss scaling
  --rescale_betas_zero_snr [RESCALE_BETAS_ZERO_SNR]
                        Rescale betas for zero terminal SNR
  --webhook_config WEBHOOK_CONFIG
                        Path to webhook configuration file
  --webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL
                        Interval for webhook reports (seconds)
  --distillation_method {lcm,dcm,dmd,perflow}
                        Method for model distillation
  --distillation_config DISTILLATION_CONFIG
                        Path to distillation configuration file
  --ema_validation {none,ema_only,comparison}
                        Control how EMA weights are used during validation
                        runs.
  --local_rank LOCAL_RANK
                        Local rank for distributed training
  --ltx_train_mode {t2v,i2v}
                        Training mode for LTX models
  --ltx_i2v_prob LTX_I2V_PROB
                        Probability of using image-to-video training for LTX
  --ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION
                        Fraction of noise to add for LTX partial training
  --ltx_protect_first_frame [LTX_PROTECT_FIRST_FRAME]
                        Protect the first frame from noise in LTX training
  --offload_param_path OFFLOAD_PARAM_PATH
                        Path to offloaded parameter files
  --offset_noise [OFFSET_NOISE]
                        Enable offset-noise training
  --quantize_activations [QUANTIZE_ACTIVATIONS]
                        Quantize model activations during training
  --refiner_training [REFINER_TRAINING]
                        Enable refiner model training mode
  --refiner_training_invert_schedule [REFINER_TRAINING_INVERT_SCHEDULE]
                        Invert the noise schedule for refiner training
  --refiner_training_strength REFINER_TRAINING_STRENGTH
                        Strength of refiner training
  --sdxl_refiner_uses_full_range [SDXL_REFINER_USES_FULL_RANGE]
                        Use full timestep range for SDXL refiner
  --sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION
                        Complex human instruction for Sana model training
```
