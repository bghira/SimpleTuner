# CLIP 分数跟踪

CLIP 分数与模型遵循提示词的能力有一定相关性，但与图像质量/保真度无关。

模型的 `clip/mean` 分数表示图像提取特征与提示词提取特征的对齐程度。它目前常用于衡量整体提示词遵循度，但通常需要在大量（约 5,000）测试提示词（如 Parti Prompts）上评估。

在模型预训练期间生成 CLIP 分数有助于证明模型正在接近目标，但当 `clip/mean` 达到 `.30` 到 `.39` 左右时，比较意义会变弱。平均 CLIP 分数 `.33` 的模型可能在人工评测中优于 `.36` 的模型。不过，平均分仅有 `0.18` 到 `0.22` 的模型通常表现很差。

在单次测试中，有些提示词可能得到很低的 CLIP 分数（约 `0.14`，对应跟踪图中的 `clip/min`），即便图像质量高且与提示词匹配；相反，质量可疑的图像也可能出现 `0.39` 的高分（`clip/max`）。这是因为该测试并不衡量图像质量。因此通常需要大量提示词来评估模型性能——_即便如此_ 也不完美。

单独计算 CLIP 分数并不耗时，但为了获得有意义的评价所需的提示词数量会让耗时变得非常长。

在小规模训练中加入 CLIP 评估通常无妨。你可能会发现某些输出模式，进而决定停止训练或调整其他超参（如学习率）。

若要使用标准提示词库进行评估，可提供 `--validation_prompt_library`，这样可在训练运行间建立相对基准。

在 `config.json` 中：

```json
{
  ...
  "evaluation_type": "clip",
  "pretrained_evaluation_model_name_or_path": "openai/clip-vit-large-patch14-336",
  "report_to": "tensorboard", # or wandb
  ...
}
```

## 兼容性

SageAttention 目前与 CLIP 分数跟踪不兼容。必须禁用其中一个。
