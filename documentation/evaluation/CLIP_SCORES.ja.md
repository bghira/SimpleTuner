# CLIP スコア追跡

CLIP スコアはプロンプトに従う能力の測定と弱く関連しますが、画像品質/忠実度とは関係ありません。

モデルの `clip/mean` スコアは、画像から抽出した特徴とプロンプトから抽出した特徴の一致度を示します。一般的なプロンプト追従性の指標として広く使われますが、通常は非常に多く（約 5,000）のテストプロンプト（例: Parti Prompts）で評価します。

事前学習中の CLIP スコア生成は、モデルが目的に近づいていることの指標になりますが、`clip/mean` が `.30`〜`.39` 程度に達すると比較の意味合いは薄れてきます。平均 CLIP スコアが `.33` のモデルが、人間評価では `.36` のモデルを上回ることもあります。一方、平均 CLIP スコアが `0.18`〜`0.22` と非常に低いモデルは、性能がかなり低い可能性があります。

単一のテストランでも、ユーザープロンプトと十分一致して高品質な画像であっても `0.14` 程度の低い CLIP スコア（トラッカーチャートの `clip/min`）になることがあります。逆に、品質が疑わしい画像でも `0.39`（`clip/max`）の高スコアになる場合があります。このテストは品質情報を捉えるものではないためです。これが、モデル性能の測定に多数のプロンプトが必要とされる理由です。_それでも_ 完全とは言えません。

CLIP スコア自体の算出は時間がかかりませんが、意味のある評価には大量のプロンプトが必要なため、非常に時間がかかります。

小規模な学習では CLIP 評価を含めても大きな負担ではありません。出力パターンを把握して、学習を中断するか、学習率などのハイパーパラメータを調整する判断材料になることもあります。

標準のプロンプトライブラリで評価するには、`--validation_prompt_library` を指定します。これにより学習ラン間の相対的なベンチマークが得られます。

`config.json`:

```json
{
  ...
  "evaluation_type": "clip",
  "pretrained_evaluation_model_name_or_path": "openai/clip-vit-large-patch14-336",
  "report_to": "tensorboard", # or wandb
  ...
}
```

## 互換性

SageAttention は現在 CLIP スコア追跡と互換性がありません。どちらか一方を無効にする必要があります。
