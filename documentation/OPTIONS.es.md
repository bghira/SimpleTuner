# Opciones del script de entrenamiento de SimpleTuner

## Visi√≥n general

Esta gu√≠a ofrece un desglose amigable de las opciones de l√≠nea de comandos disponibles en el script `train.py` de SimpleTuner. Estas opciones ofrecen un alto grado de personalizaci√≥n, permiti√©ndote entrenar tu modelo para ajustarlo a tus requisitos espec√≠ficos.

### Formato del archivo de configuraci√≥n JSON

El nombre de archivo JSON esperado es `config.json` y los nombres de clave son los mismos que los `--argumentos` de abajo. El prefijo `--` no es obligatorio en el archivo JSON, pero tambi√©n puede dejarse.

¬øBuscas ejemplos listos para ejecutar? Consulta los presets curados en [simpletuner/examples/README.md](/simpletuner/examples/README.md).

### Script de configuraci√≥n f√°cil (***RECOMENDADO***)

El comando `simpletuner configure` puede usarse para configurar un archivo `config.json` con valores predeterminados casi ideales.

#### Modificar configuraciones existentes

El comando `configure` es capaz de aceptar un √∫nico argumento, un `config.json` compatible, lo que permite la modificaci√≥n interactiva de tu configuraci√≥n de entrenamiento:

```bash
simpletuner configure config/foo/config.json
```

Donde `foo` es tu entorno de configuraci√≥n; o simplemente usa `config/config.json` si no est√°s usando entornos de configuraci√≥n.

<img width="1484" height="560" alt="image" src="https://github.com/user-attachments/assets/67dec8d8-3e41-42df-96e6-f95892d2814c" />

> ‚ö†Ô∏è Para usuarios ubicados en pa√≠ses donde Hugging Face Hub no es f√°cilmente accesible, deber√≠as a√±adir `HF_ENDPOINT=https://hf-mirror.com` a tu `~/.bashrc` o `~/.zshrc` dependiendo de qu√© `$SHELL` use tu sistema.

---

## üåü Configuraci√≥n central del modelo

### `--model_type`

- **Qu√©**: Selecciona si se crea una LoRA o un fine-tune completo.
- **Opciones**: lora, full.
- **Predeterminado**: lora
  - Si se usa lora, `--lora_type` dicta si se usa PEFT o LyCORIS. Algunos modelos (PixArt) funcionan solo con adaptadores LyCORIS.

### `--model_family`

- **Qu√©**: Determina qu√© arquitectura de modelo se est√° entrenando.
- **Opciones**: pixart_sigma, flux, sd3, sdxl, kolors, legacy

### `--lora_format`

- **Qu√©**: Selecciona el formato de claves del checkpoint LoRA para cargar/guardar.
- **Opciones**: `diffusers` (predeterminado), `comfyui`
- **Notas**:
  - `diffusers` es el esquema est√°ndar de PEFT/Diffusers.
  - `comfyui` convierte hacia/desde claves estilo ComfyUI (`diffusion_model.*` con tensores `lora_A/lora_B` y `.alpha`). Flux, Flux2, Lumina2 y Z-Image detectar√°n autom√°ticamente entradas ComfyUI incluso si esto se deja en `diffusers`, pero c√°mbialo a `comfyui` para forzar salida ComfyUI al guardar.

### `--fuse_qkv_projections`

- **Qu√©**: Fusiona las proyecciones QKV en los bloques de atenci√≥n del modelo para un uso m√°s eficiente del hardware.
- **Nota**: Solo disponible con NVIDIA H100 o H200 con Flash Attention 3 instalado manualmente.

### `--offload_during_startup`

- **Qu√©**: Descarga los pesos del codificador de texto a CPU mientras se hace el cach√© del VAE.
- **Por qu√©**: Esto es √∫til para modelos grandes como HiDream y Wan 2.1, que pueden quedarse sin memoria al cargar el cach√© del VAE. Esta opci√≥n no afecta la calidad del entrenamiento, pero para codificadores de texto muy grandes o CPUs lentas, puede extender sustancialmente el tiempo de arranque con muchos datasets. Est√° desactivada por defecto por esta raz√≥n.
- **Consejo**: Complementa la funci√≥n de offload en grupo de abajo para sistemas con memoria especialmente limitada.

### `--offload_during_save`

- **Qu√©**: Mueve temporalmente toda la canalizaci√≥n a CPU mientras `save_hooks.py` prepara los checkpoints para que todos los pesos FP8/cuantizados se escriban fuera del dispositivo.
- **Por qu√©**: Guardar pesos fp8-quanto puede disparar el uso de VRAM (por ejemplo, durante la serializaci√≥n de `state_dict()`). Esta opci√≥n mantiene el modelo en el acelerador para entrenar, pero lo descarga brevemente cuando se dispara un guardado para evitar OOM de CUDA.
- **Consejo**: Act√≠valo solo cuando el guardado falle con errores de OOM; el cargador devuelve el modelo despu√©s y el entrenamiento se reanuda sin problemas.

### `--delete_model_after_load`

- **Qu√©**: Elimina los archivos de modelo del cach√© de HuggingFace despu√©s de cargarlos en memoria.
- **Por qu√©**: Reduce el uso de disco para configuraciones con presupuestos ajustados que cobran por gigabyte usado. Despu√©s de cargar los modelos en VRAM/RAM, el cach√© en disco ya no es necesario hasta la siguiente ejecuci√≥n. Esto traslada la carga de almacenamiento al ancho de banda de red en ejecuciones posteriores.
- **Notas**:
  - El VAE **no** se elimina si la validaci√≥n est√° habilitada, ya que se necesita para generar im√°genes de validaci√≥n.
  - Los codificadores de texto se eliminan despu√©s de que la f√°brica de backend de datos completa el arranque (despu√©s del cach√© de embeddings).
  - Los modelos Transformer/UNet se eliminan inmediatamente despu√©s de la carga.
  - En configuraciones multinodo, solo el rank local 0 en cada nodo realiza la eliminaci√≥n. Los fallos de eliminaci√≥n se ignoran silenciosamente para manejar condiciones de carrera en almacenamiento de red compartido.
  - Esto **no** afecta a los checkpoints de entrenamiento guardados; solo al cach√© del modelo base preentrenado.

### `--enable_group_offload`

- **Qu√©**: Habilita el offload de m√≥dulos agrupados de diffusers para que los bloques del modelo se puedan preparar en CPU (o disco) entre pasadas hacia delante.
- **Por qu√©**: Reduce dr√°sticamente el uso m√°ximo de VRAM en transformadores grandes (Flux, Wan, Auraflow, LTXVideo, Cosmos2Image) con un impacto m√≠nimo en rendimiento cuando se usa con streams CUDA.
- **Notas**:
  - Es mutuamente excluyente con `--enable_model_cpu_offload`; elige una estrategia por ejecuci√≥n.
  - Requiere diffusers **v0.33.0** o m√°s reciente.

### `--group_offload_type`

- **Opciones**: `block_level` (predeterminado), `leaf_level`
- **Qu√©**: Controla c√≥mo se agrupan las capas. `block_level` equilibra el ahorro de VRAM con el rendimiento, mientras que `leaf_level` maximiza el ahorro a costa de m√°s transferencias de CPU.

### `--group_offload_blocks_per_group`

- **Qu√©**: Al usar `block_level`, el n√∫mero de bloques de transformer que se agrupan en un solo grupo de offload.
- **Predeterminado**: `1`
- **Por qu√©**: Aumentar este n√∫mero reduce la frecuencia de transferencias (m√°s r√°pido) pero mantiene m√°s par√°metros en el acelerador (usa m√°s VRAM).

### `--group_offload_use_stream`

- **Qu√©**: Usa un stream CUDA dedicado para solapar transferencias host/dispositivo con c√≥mputo.
- **Predeterminado**: `False`
- **Notas**:
  - Recurre autom√°ticamente a transferencias estilo CPU en backends no CUDA (Apple MPS, ROCm, CPU).
  - Recomendado al entrenar en GPUs NVIDIA con capacidad de motor de copia disponible.

### `--group_offload_to_disk_path`

- **Qu√©**: Ruta de directorio usada para volcar par√°metros agrupados a disco en lugar de RAM.
- **Por qu√©**: √ötil para presupuestos de RAM de CPU extremadamente ajustados (p. ej., estaci√≥n de trabajo con gran NVMe).
- **Consejo**: Usa un SSD local r√°pido; los sistemas de archivos de red ralentizar√°n significativamente el entrenamiento.

### `--musubi_blocks_to_swap`

- **Qu√©**: Intercambio de bloques Musubi para LongCat-Video, Wan, LTXVideo, Kandinsky5-Video, Qwen-Image, Flux, Flux.2, Cosmos2Image y HunyuanVideo: mantiene los √∫ltimos N bloques del transformer en CPU y transmite pesos por bloque durante el forward.
- **Predeterminado**: `0` (desactivado)
- **Notas**: Offload de pesos estilo Musubi; reduce VRAM con coste en rendimiento y se omite cuando los gradientes est√°n habilitados.

### `--musubi_block_swap_device`

- **Qu√©**: Cadena de dispositivo para almacenar bloques de transformer intercambiados (p. ej., `cpu`, `cuda:0`).
- **Predeterminado**: `cpu`
- **Notas**: Solo se usa cuando `--musubi_blocks_to_swap` > 0.

### `--ramtorch`

- **Qu√©**: Reemplaza capas `nn.Linear` con implementaciones RamTorch transmitidas desde CPU.
- **Por qu√©**: Comparte los pesos Linear en memoria de CPU y los transmite al acelerador para reducir la presi√≥n de VRAM.
- **Notas**:
  - Requiere CUDA o ROCm (no compatible con Apple/MPS).
  - Es mutuamente excluyente con `--enable_group_offload`.
  - Habilita autom√°ticamente `--set_grads_to_none`.

### `--ramtorch_target_modules`

- **Qu√©**: Patrones glob separados por comas que limitan qu√© m√≥dulos Linear se convierten a RamTorch.
- **Predeterminado**: Todas las capas Linear se convierten cuando no se proporciona un patr√≥n.
- **Notas**: Coincide con nombres de m√≥dulos completamente calificados o nombres de clase (se permiten comodines).

### `--ramtorch_text_encoder`

- **Qu√©**: Aplica reemplazos RamTorch a todas las capas Linear del codificador de texto.
- **Predeterminado**: `False`

### `--ramtorch_vae`

- **Qu√©**: Conversi√≥n RamTorch experimental solo para las capas Linear del bloque medio del VAE.
- **Predeterminado**: `False`
- **Notas**: Los bloques de convoluci√≥n up/down del VAE se dejan sin cambios.

### `--ramtorch_controlnet`

- **Qu√©**: Aplica reemplazos RamTorch a las capas Linear de ControlNet cuando se entrena un ControlNet.
- **Predeterminado**: `False`

### `--ramtorch_transformer_percent`

- **Qu√©**: Porcentaje (0-100) de capas Linear del transformer a descargar con RamTorch.
- **Predeterminado**: `100` (todas las capas elegibles)
- **Por qu√©**: Permite una descarga parcial para equilibrar el ahorro de VRAM con el rendimiento. Valores m√°s bajos mantienen m√°s capas en la GPU para un entrenamiento m√°s r√°pido, mientras se reduce el uso de memoria.
- **Notas**: Las capas se seleccionan desde el inicio del orden de recorrido del m√≥dulo. Se puede combinar con `--ramtorch_target_modules`.

### `--ramtorch_text_encoder_percent`

- **Qu√©**: Porcentaje (0-100) de capas Linear del codificador de texto a descargar con RamTorch.
- **Predeterminado**: `100` (todas las capas elegibles)
- **Por qu√©**: Permite la descarga parcial de codificadores de texto cuando `--ramtorch_text_encoder` est√° habilitado.
- **Notas**: Solo aplica cuando `--ramtorch_text_encoder` est√° habilitado.

### `--pretrained_model_name_or_path`

- **Qu√©**: Ruta al modelo preentrenado o su identificador en <https://huggingface.co/models>.
- **Por qu√©**: Para especificar el modelo base desde el que comenzar√°s a entrenar. Usa `--revision` y `--variant` para especificar versiones concretas desde un repositorio. Tambi√©n admite rutas de un solo archivo `.safetensors` para SDXL, Flux y SD3.x.

### `--pretrained_t5_model_name_or_path`

- **Qu√©**: Ruta al modelo T5 preentrenado o su identificador en <https://huggingface.co/models>.
- **Por qu√©**: Al entrenar PixArt, quiz√° quieras usar una fuente espec√≠fica para tus pesos T5 para evitar descargarlos varias veces al cambiar el modelo base desde el que entrenas.

### `--pretrained_gemma_model_name_or_path`

- **Qu√©**: Ruta al modelo Gemma preentrenado o su identificador en <https://huggingface.co/models>.
- **Por qu√©**: Al entrenar modelos basados en Gemma (por ejemplo LTX-2, Sana o Lumina2), puedes apuntar a un checkpoint Gemma compartido sin cambiar la ruta del modelo base de difusi√≥n.

### `--custom_text_encoder_intermediary_layers`

- **Qu√©**: Sobrescribe qu√© capas de estado oculto extraer del encoder de texto para modelos FLUX.2.
- **Formato**: Array JSON de √≠ndices de capas, ej: `[10, 20, 30]`
- **Por defecto**: Se usan los valores por defecto espec√≠ficos del modelo cuando no se establece:
  - FLUX.2-dev (Mistral-3): `[10, 20, 30]`
  - FLUX.2-klein (Qwen3): `[9, 18, 27]`
- **Por qu√©**: Permite experimentar con diferentes combinaciones de estados ocultos del encoder de texto para alineaci√≥n personalizada o prop√≥sitos de investigaci√≥n.
- **Nota**: Esta opci√≥n es experimental y solo aplica a modelos FLUX.2. Cambiar los √≠ndices de capas invalidar√° los embeddings de texto en cach√© y requerir√° regenerarlos. El n√∫mero de capas debe coincidir con la entrada esperada por el modelo (3 capas).

### `--gradient_checkpointing`

- **Qu√©**: Durante el entrenamiento, los gradientes se calcular√°n por capa y se acumular√°n para ahorrar en los requisitos m√°ximos de VRAM a costa de un entrenamiento m√°s lento.

### `--gradient_checkpointing_interval`

- **Qu√©**: Hace checkpoint cada *n* bloques, donde *n* es un valor mayor que cero. Un valor de 1 es equivalente a dejar `--gradient_checkpointing` habilitado, y un valor de 2 har√° checkpoint en bloques alternos.
- **Nota**: SDXL y Flux son actualmente los √∫nicos modelos que soportan esta opci√≥n. SDXL usa una implementaci√≥n algo improvisada.

### `--refiner_training`

- **Qu√©**: Habilita el entrenamiento de una serie de modelos de mezcla de expertos personalizada. Consulta [Mixture-of-Experts](MIXTURE_OF_EXPERTS.md) para m√°s informaci√≥n sobre estas opciones.

## Precisi√≥n

### `--quantize_via`

- **Opciones**: `cpu`, `accelerator`, `pipeline`
  - En `accelerator`, puede funcionar moderadamente m√°s r√°pido con el riesgo de posibles OOM en tarjetas de 24G para un modelo tan grande como Flux.
  - En `cpu`, la cuantizaci√≥n tarda unos 30 segundos. (**Predeterminado**)
  - `pipeline` delega la cuantizaci√≥n a Diffusers usando `--quantization_config` o presets compatibles con pipeline (p. ej., `nf4-bnb`, `int8-torchao`, `fp8-torchao`, `int8-quanto`, o checkpoints `.gguf`).

### `--base_model_precision`

- **Qu√©**: Reduce la precisi√≥n del modelo y entrena usando menos memoria. Hay tres backends de cuantizaci√≥n compatibles: BitsAndBytes (pipeline), TorchAO (pipeline o manual) y Optimum Quanto (pipeline o manual).

#### Presets de pipeline de Diffusers

- `nf4-bnb` se carga a trav√©s de Diffusers con una configuraci√≥n BitsAndBytes NF4 de 4 bits (solo CUDA). Requiere `bitsandbytes` y una build de diffusers con soporte BnB.
- `int4-torchao`, `int8-torchao` y `fp8-torchao` usan TorchAoConfig v√≠a Diffusers (CUDA). Requiere `torchao` y diffusers/transformers recientes.
- `int8-quanto`, `int4-quanto`, `int2-quanto`, `fp8-quanto` y `fp8uz-quanto` usan QuantoConfig v√≠a Diffusers. Diffusers asigna FP8-NUZ a pesos float8; usa cuantizaci√≥n manual de quanto si necesitas la variante NUZ.
- `.gguf` checkpoints se detectan autom√°ticamente y se cargan con `GGUFQuantizationConfig` cuando est√° disponible. Instala versiones recientes de diffusers/transformers para soporte GGUF.

#### Optimum Quanto

Proporcionada por Hugging Face, la librer√≠a optimum-quanto tiene un soporte robusto en todas las plataformas compatibles.

- `int8-quanto` es la m√°s compatible y probablemente produce los mejores resultados
  - entrenamiento m√°s r√°pido para RTX4090 y probablemente otras GPUs
  - usa matmul acelerado por hardware en dispositivos CUDA para int8, int4
    - int4 sigue siendo terriblemente lento
  - funciona con `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`)
- `fp8uz-quanto` es una variante fp8 experimental para dispositivos CUDA y ROCm.
  - mejor soportado en silicio AMD como Instinct o arquitecturas m√°s nuevas
  - puede ser ligeramente m√°s r√°pido que `int8-quanto` en una 4090 para entrenamiento, pero no para inferencia (1 segundo m√°s lento)
  - funciona con `TRAINING_DYNAMO_BACKEND=inductor` (`torch.compile()`)
- `fp8-quanto` no usar√° (por ahora) matmul fp8, no funciona en sistemas Apple.
  - a√∫n no hay matmul fp8 por hardware en dispositivos CUDA o ROCm, por lo que posiblemente sea notablemente m√°s lento que int8
    - usa el kernel MARLIN para GEMM fp8
  - incompatible con dynamo, deshabilitar√° autom√°ticamente dynamo si se intenta esa combinaci√≥n.

#### TorchAO

Una biblioteca m√°s nueva de PyTorch; AO nos permite reemplazar los linears y las convoluciones 2D (p. ej., modelos tipo unet) con contrapartes cuantizadas.
<!-- Additionally, it provides an experimental CPU offload optimiser that essentially provides a simpler reimplementation of DeepSpeed. -->

- `int8-torchao` reducir√° el consumo de memoria al mismo nivel que cualquiera de los niveles de precisi√≥n de Quanto
  - al momento de escribir, corre ligeramente m√°s lento (11s/iter) que Quanto (9s/iter) en Apple MPS
  - cuando no se usa `torch.compile`, misma velocidad y uso de memoria que `int8-quanto` en dispositivos CUDA, perfil de velocidad desconocido en ROCm
  - al usar `torch.compile`, m√°s lento que `int8-quanto`
- `fp8-torchao` solo est√° disponible para aceleradores Hopper (H100, H200) o m√°s nuevos (Blackwell B200)

##### Optimizadores

TorchAO incluye optimizadores 4bit y 8bit de uso general: `ao-adamw8bit`, `ao-adamw4bit`

Tambi√©n proporciona dos optimizadores dirigidos a usuarios de Hopper (H100 o mejor): `ao-adamfp8` y `ao-adamwfp8`

#### SDNQ (SD.Next Quantization Engine)

[SDNQ](https://github.com/disty0/sdnq) es una librer√≠a de cuantizaci√≥n optimizada para entrenamiento que funciona en todas las plataformas: AMD (ROCm), Apple (MPS) y NVIDIA (CUDA). Proporciona entrenamiento cuantizado con redondeo estoc√°stico y estados de optimizador cuantizados para eficiencia de memoria.

##### Niveles de precisi√≥n recomendados

**Para fine-tuning completo** (se actualizan los pesos del modelo):
- `uint8-sdnq` - Mejor equilibrio entre ahorro de memoria y calidad de entrenamiento
- `uint16-sdnq` - Mayor precisi√≥n para m√°xima calidad (p. ej., Stable Cascade)
- `int16-sdnq` - Alternativa signed de 16 bits
- `fp16-sdnq` - FP16 cuantizado, m√°xima precisi√≥n con los beneficios de SDNQ

**Para entrenamiento LoRA** (pesos base congelados):
- `int8-sdnq` - 8 bits con signo, buena opci√≥n general
- `int6-sdnq`, `int5-sdnq` - Menor precisi√≥n, menos memoria
- `uint5-sdnq`, `uint4-sdnq`, `uint3-sdnq`, `uint2-sdnq` - Compresi√≥n agresiva

**Nota:** `int7-sdnq` est√° disponible pero no se recomienda (lento y no mucho m√°s peque√±o que int8).

**Importante:** Por debajo de 5 bits de precisi√≥n, SDNQ habilita autom√°ticamente SVD (Singular Value Decomposition) con 8 pasos para mantener la calidad. SVD tarda m√°s en cuantizar y es no determinista, por lo que Disty0 proporciona modelos SVD pre-cuantizados en HuggingFace. SVD a√±ade overhead de c√≥mputo durante el entrenamiento, as√≠ que ev√≠talo para fine-tuning completo donde los pesos se actualizan activamente.

**Caracter√≠sticas clave:**
- Multiplataforma: Funciona de forma id√©ntica en hardware AMD, Apple y NVIDIA
- Optimizado para entrenamiento: Usa redondeo estoc√°stico para reducir acumulaci√≥n de errores de cuantizaci√≥n
- Eficiente en memoria: Soporta buffers de estado del optimizador cuantizados
- Matmul desacoplado: La precisi√≥n de los pesos y la precisi√≥n de matmul son independientes (matmul INT8/FP8/FP16 disponible)

##### Optimizadores SDNQ

SDNQ incluye optimizadores con buffers de estado cuantizados opcionales para ahorro adicional de memoria:

- `sdnq-adamw` - AdamW con buffers de estado cuantizados (uint8, group_size=32)
- `sdnq-adamw+no_quant` - AdamW sin estados cuantizados (para comparaci√≥n)
- `sdnq-adafactor` - Adafactor con buffers de estado cuantizados
- `sdnq-came` - Optimizador CAME con buffers de estado cuantizados
- `sdnq-lion` - Optimizador Lion con buffers de estado cuantizados
- `sdnq-muon` - Optimizador Muon con buffers de estado cuantizados
- `sdnq-muon+quantized_matmul` - Muon con matmul INT8 en el c√°lculo de zeropower

Todos los optimizadores SDNQ usan redondeo estoc√°stico por defecto y pueden configurarse con `--optimizer_config` para ajustes personalizados como `use_quantized_buffers=false` para desactivar la cuantizaci√≥n de estado.

**Opciones espec√≠ficas de Muon:**
- `use_quantized_matmul` - Habilita matmul INT8/FP8/FP16 en zeropower_via_newtonschulz5
- `quantized_matmul_dtype` - Precisi√≥n de matmul: `int8` (GPUs de consumo), `fp8` (datacenter), `fp16`
- `zeropower_dtype` - Precisi√≥n para el c√°lculo de zeropower (se ignora cuando `use_quantized_matmul=True`)
- Prefija argumentos con `muon_` o `adamw_` para establecer valores diferentes para Muon vs fallback AdamW

**Modelos pre-cuantizados:** Disty0 proporciona modelos SVD uint4 pre-cuantizados en [huggingface.co/collections/Disty0/sdnq](https://huggingface.co/collections/Disty0/sdnq). C√°rgalos normalmente y luego convierte con `convert_sdnq_model_to_training()` despu√©s de importar SDNQ (SDNQ debe importarse antes de cargar para registrarse con Diffusers).

**Nota sobre checkpointing:** Los modelos de entrenamiento SDNQ se guardan tanto en formato nativo PyTorch (`.pt`) para reanudar entrenamiento como en formato safetensors para inferencia. El formato nativo es obligatorio para una reanudaci√≥n correcta, ya que la clase `SDNQTensor` de SDNQ usa serializaci√≥n personalizada.

**Consejo de espacio en disco:** Para ahorrar espacio en disco, puedes mantener solo los pesos cuantizados y usar el script `dequantize_sdnq_training.py` de SDNQ ([enlace](https://github.com/Disty0/sdnq/blob/main/scripts/dequantize_sdnq_training.py)) para de-cuantizar cuando sea necesario para inferencia.

### `--quantization_config`

- **Qu√©**: Objeto JSON o ruta de archivo que describe overrides de `quantization_config` de Diffusers cuando se usa `--quantize_via=pipeline`.
- **C√≥mo**: Acepta JSON inline (o un archivo) con entradas por componente. Las claves pueden incluir `unet`, `transformer`, `text_encoder` o `default`.
- **Ejemplos**:

```json
{
  "unet": {"load_in_4bit": true, "bnb_4bit_quant_type": "nf4", "bnb_4bit_compute_dtype": "bfloat16"},
  "text_encoder": {"quant_type": {"group_size": 128}}
}
```

Este ejemplo habilita BnB NF4 de 4 bits en el UNet y TorchAO int4 en el codificador de texto.

#### Torch Dynamo

Habilita `torch.compile()` desde la WebUI visitando **Hardware ‚Üí Accelerate (advanced)** y configurando **Torch Dynamo Backend** con tu compilador preferido (por ejemplo, *inductor*). Los toggles adicionales te permiten elegir un **modo** de optimizaci√≥n, habilitar guardas de **forma din√°mica** u optar por la **compilaci√≥n regional** para acelerar arranques en fr√≠o en modelos transformer muy profundos.

La misma configuraci√≥n puede expresarse en `config/config.env`:

```bash
TRAINING_DYNAMO_BACKEND=inductor
```

Opcionalmente puedes combinar esto con `--dynamo_mode=max-autotune` u otras flags de Dynamo expuestas en la UI para un control m√°s fino.

Ten en cuenta que los primeros pasos del entrenamiento ser√°n m√°s lentos de lo normal debido a la compilaci√≥n en segundo plano.

Para persistir los ajustes en `config.json`, a√±ade las claves equivalentes:

```json
{
  "dynamo_backend": "inductor",
  "dynamo_mode": "max-autotune",
  "dynamo_fullgraph": false,
  "dynamo_dynamic": false,
  "dynamo_use_regional_compilation": true
}
```

Omite cualquier entrada que quieras heredar de los valores predeterminados de Accelerate (por ejemplo, deja fuera `dynamo_mode` para usar la selecci√≥n autom√°tica).

### `--attention_mechanism`

Se soportan mecanismos de atenci√≥n alternativos, con distintos niveles de compatibilidad u otros compromisos:

- `diffusers` usa los kernels SDPA nativos de PyTorch y es el predeterminado.
- `xformers` habilita el kernel de atenci√≥n [xformers](https://github.com/facebookresearch/xformers) de Meta (entrenamiento + inferencia) cuando el modelo subyacente expone `enable_xformers_memory_efficient_attention`.
- `flash-attn`, `flash-attn-2`, `flash-attn-3` y `flash-attn-3-varlen` se enganchan al helper `attention_backend` de Diffusers para enrutar la atenci√≥n a trav√©s de los kernels FlashAttention v1/2/3. Instala las wheels correspondientes de `flash-attn` / `flash-attn-interface` y ten en cuenta que FA3 actualmente requiere GPUs Hopper.
- `flex` selecciona el backend FlexAttention de PyTorch 2.5 (FP16/BF16 en CUDA). Debes compilar/instalar los kernels Flex por separado; consulta [documentation/attention/FLEX.md](attention/FLEX.md).
- `cudnn`, `native-efficient`, `native-flash`, `native-math`, `native-npu` y `native-xla` seleccionan el backend SDPA correspondiente expuesto por `torch.nn.attention.sdpa_kernel`. Son √∫tiles cuando quieres determinismo (`native-math`), el kernel SDPA de CuDNN o los aceleradores nativos del proveedor (NPU/XLA).
- `sla` habilita [Sparse‚ÄìLinear Attention (SLA)](https://github.com/thu-ml/SLA), proporcionando un kernel h√≠brido disperso/lineal afinable que puede usarse tanto para entrenamiento como para validaci√≥n sin gating adicional.
  - Instala el paquete SLA (por ejemplo con `pip install -e ~/src/SLA`) antes de seleccionar este backend.
  - SimpleTuner guarda los pesos de proyecci√≥n aprendidos de SLA en `sla_attention.pt` dentro de cada checkpoint; conserva este archivo con el resto del checkpoint para que las reanudaciones e inferencia mantengan el estado SLA entrenado.
  - Como el backbone se ajusta alrededor del comportamiento mixto disperso/lineal de SLA, SLA ser√° necesario tambi√©n en inferencia. Consulta `documentation/attention/SLA.md` para una gu√≠a enfocada.
  - Usa `--sla_config '{"topk":0.15,"blkq":32,"tie_feature_map_qk":false}'` (JSON o sintaxis de dict de Python) para sobrescribir los valores predeterminados de runtime de SLA si necesitas experimentar.
- `sageattention`, `sageattention-int8-fp16-triton`, `sageattention-int8-fp16-cuda` y `sageattention-int8-fp8-cuda` envuelven los kernels correspondientes de [SageAttention](https://github.com/thu-ml/SageAttention). Est√°n orientados a inferencia y deben usarse con `--sageattention_usage` para protegerse de entrenamiento accidental.
  - En t√©rminos simples, SageAttention reduce el requisito de c√≥mputo para inferencia

> ‚ÑπÔ∏è Los selectores de backend Flash/Flex/PyTorch dependen del despachador `attention_backend` de Diffusers, por lo que actualmente benefician a modelos estilo transformer que ya optan por esa ruta de c√≥digo (Flux, Wan 2.x, LTXVideo, QwenImage, etc.). Los UNet cl√°sicos de SD/SDXL siguen usando SDPA de PyTorch directamente.

Usar `--sageattention_usage` para habilitar entrenamiento con SageAttention debe hacerse con cuidado, ya que no rastrea ni propaga gradientes desde sus implementaciones CUDA personalizadas para los linears QKV.

- Esto hace que estas capas queden completamente sin entrenar, lo que podr√≠a causar colapso del modelo o mejoras leves en entrenamientos cortos.

---

## üì∞ Publicaci√≥n

### `--push_to_hub`

- **Qu√©**: Si se proporciona, tu modelo se subir√° a [Huggingface Hub](https://huggingface.co) cuando finalice el entrenamiento. Usar `--push_checkpoints_to_hub` adem√°s subir√° cada checkpoint intermedio.

### `--push_to_hub_background`

- **Qu√©**: Sube a Hugging Face Hub desde un worker en segundo plano para que los pushes de checkpoints no pausen el bucle de entrenamiento.
- **Por qu√©**: Mantiene el entrenamiento y la validaci√≥n en marcha mientras las subidas al Hub proceden de forma as√≠ncrona. Las subidas finales a√∫n se esperan antes de que la ejecuci√≥n termine para que los fallos se expongan.

### `--webhook_config`

- **Qu√©**: Configuraci√≥n para destinos de webhook (p. ej., Discord, endpoints personalizados) para recibir eventos de entrenamiento en tiempo real.
- **Por qu√©**: Te permite monitorear ejecuciones de entrenamiento con herramientas y paneles externos, recibiendo notificaciones en etapas clave del entrenamiento.
- **Notas**: El campo `job_id` en las cargas √∫tiles del webhook puede poblarse configurando la variable de entorno `SIMPLETUNER_JOB_ID` antes del entrenamiento:
  ```bash
  export SIMPLETUNER_JOB_ID="my-training-run-name"
  python train.py
  ```
Esto es √∫til para herramientas de monitoreo que reciben webhooks de m√∫ltiples ejecuciones de entrenamiento para identificar qu√© configuraci√≥n envi√≥ cada evento. Si SIMPLETUNER_JOB_ID no est√° configurado, job_id ser√° null en las cargas √∫tiles del webhook.

### `--publishing_config`

- **Qu√©**: JSON/dict/ruta de archivo opcional que describe destinos de publicaci√≥n que no son de Hugging Face (almacenamiento compatible con S3, Backblaze B2, Azure Blob Storage, Dropbox).
- **Por qu√©**: Refleja el parseo de `--webhook_config` para que puedas distribuir artefactos m√°s all√° del Hub. La publicaci√≥n se ejecuta en el proceso principal despu√©s de la validaci√≥n usando el `output_dir` actual.
- **Notas**: Los proveedores se suman a `--push_to_hub`. Instala los SDK de cada proveedor (p. ej., `boto3`, `azure-storage-blob`, `dropbox`) dentro de tu `.venv` cuando los habilites. Consulta `documentation/publishing/README.md` para ejemplos completos.

### `--hub_model_id`

- **Qu√©**: El nombre del modelo en Huggingface Hub y el directorio de resultados locales.
- **Por qu√©**: Este valor se usa como nombre de directorio bajo la ubicaci√≥n especificada como `--output_dir`. Si se proporciona `--push_to_hub`, este se convertir√° en el nombre del modelo en Huggingface Hub.

### `--modelspec_comment`

- **Qu√©**: Texto incorporado en los metadatos del archivo safetensors como `modelspec.comment`
- **Por defecto**: None (deshabilitado)
- **Notas**:
  - Visible en visualizadores de modelos externos (ComfyUI, herramientas de info de modelos)
  - Acepta una cadena o un array de cadenas (unidas con saltos de l√≠nea)
  - Soporta marcadores `{env:VAR_NAME}` para sustituci√≥n de variables de entorno
  - Cada checkpoint usa el valor de configuraci√≥n actual en el momento del guardado

**Ejemplo (cadena)**:
```json
"modelspec_comment": "Entrenado en mi dataset personalizado v2.1"
```

**Ejemplo (array para m√∫ltiples l√≠neas)**:
```json
"modelspec_comment": [
  "Ejecuci√≥n de entrenamiento: experiment-42",
  "Dataset: custom-portraits-v2",
  "Notas: {env:TRAINING_NOTES}"
]
```

### `--disable_benchmark`

- **Qu√©**: Desactiva la validaci√≥n/benchmark de arranque que ocurre en el paso 0 sobre el modelo base. Estas salidas se concatenan al lado izquierdo de tus im√°genes de validaci√≥n del modelo entrenado.

## üìÇ Almacenamiento y gesti√≥n de datos

### `--data_backend_config`

- **Qu√©**: Ruta a tu configuraci√≥n de dataset de SimpleTuner.
- **Por qu√©**: Se pueden combinar m√∫ltiples datasets en distintos medios de almacenamiento en una sola sesi√≥n de entrenamiento.
- **Ejemplo**: Consulta [multidatabackend.json.example](/multidatabackend.json.example) para un ejemplo de configuraci√≥n y [este documento](DATALOADER.md) para m√°s informaci√≥n sobre la configuraci√≥n del data loader.

### `--override_dataset_config`

- **Qu√©**: Cuando se proporciona, permite que SimpleTuner ignore las diferencias entre la configuraci√≥n en cach√© dentro del dataset y los valores actuales.
- **Por qu√©**: Cuando SimpleTuner se ejecuta por primera vez sobre un dataset, crear√° un documento de cach√© que contiene informaci√≥n sobre todo en ese dataset. Esto incluye la configuraci√≥n del dataset, incluyendo sus valores de configuraci√≥n de "crop" y "resolution". Cambiarlos arbitrariamente o por accidente podr√≠a hacer que tus trabajos de entrenamiento fallen aleatoriamente, por lo que se recomienda encarecidamente no usar este par√°metro y, en su lugar, resolver las diferencias que quieras aplicar en tu dataset por otro medio.

### `--data_backend_sampling`

- **Qu√©**: Al usar m√∫ltiples backends de datos, el muestreo puede hacerse con distintas estrategias.
- **Opciones**:
  - `uniform` - el comportamiento anterior de v0.9.8.1 y versiones previas donde la longitud del dataset no se consideraba, solo los pesos de probabilidad manuales.
  - `auto-weighting` - el comportamiento predeterminado donde se usa la longitud del dataset para muestrear equitativamente todos los datasets, manteniendo un muestreo uniforme de toda la distribuci√≥n de datos.
    - Esto es necesario si tienes datasets de distintos tama√±os y quieres que el modelo aprenda por igual.
    - Pero ajustar `repeats` manualmente es **necesario** para muestrear correctamente im√°genes Dreambooth frente a tu conjunto de regularizaci√≥n

### `--vae_cache_scan_behaviour`

- **Qu√©**: Configura el comportamiento de la comprobaci√≥n de escaneo de integridad.
- **Por qu√©**: Un dataset podr√≠a tener ajustes incorrectos aplicados en m√∫ltiples puntos del entrenamiento, p. ej., si eliminas accidentalmente los archivos de cach√© `.json` de tu dataset y cambias la configuraci√≥n del backend de datos para usar im√°genes cuadradas en lugar de crops por aspecto. Esto dar√° como resultado un cach√© de datos inconsistente, que puede corregirse configurando `scan_for_errors` en `true` en tu archivo de configuraci√≥n `multidatabackend.json`. Cuando se ejecuta este escaneo, se basa en el ajuste de `--vae_cache_scan_behaviour` para determinar c√≥mo resolver la inconsistencia: `recreate` (predeterminado) eliminar√° la entrada de cach√© problem√°tica para que pueda recrearse, y `sync` actualizar√° los metadatos del bucket para reflejar la realidad de la muestra real de entrenamiento. Valor recomendado: `recreate`.

### `--dataloader_prefetch`

- **Qu√©**: Recupera lotes por adelantado.
- **Por qu√©**: Especialmente al usar lotes grandes, el entrenamiento se "pausar√°" mientras las muestras se recuperan del disco (incluso NVMe), lo que afecta las m√©tricas de utilizaci√≥n de la GPU. Al habilitar el prefetch del dataloader se mantendr√° un buffer lleno de lotes completos, de modo que puedan cargarse instant√°neamente.

> ‚ö†Ô∏è Esto solo es realmente relevante para H100 o mejor a baja resoluci√≥n donde la E/S se convierte en el cuello de botella. Para la mayor√≠a de los otros casos, es una complejidad innecesaria.

### `--dataloader_prefetch_qlen`

- **Qu√©**: Aumenta o reduce el n√∫mero de lotes mantenidos en memoria.
- **Por qu√©**: Al usar prefetch del dataloader, se mantienen 10 entradas en memoria por GPU/proceso. Esto puede ser demasiado o muy poco. Este valor puede ajustarse para incrementar el n√∫mero de lotes preparados por adelantado.

### `--compress_disk_cache`

- **Qu√©**: Comprime los cach√©s en disco del VAE y de embeddings de texto.
- **Por qu√©**: El codificador T5 usado por DeepFloyd, SD3 y PixArt produce embeddings de texto muy grandes que terminan siendo en su mayor√≠a espacio vac√≠o para captions cortas o redundantes. Habilitar `--compress_disk_cache` puede reducir el espacio consumido hasta en un 75%, con ahorros promedio del 40%.

> ‚ö†Ô∏è Deber√°s eliminar manualmente los directorios de cach√© existentes para que puedan recrearse con compresi√≥n por el trainer.

---

## üåà Procesamiento de imagen y texto

Muchas configuraciones se establecen a trav√©s del [dataloader config](DATALOADER.md), pero estas se aplicar√°n de forma global.

### `--resolution_type`

- **Qu√©**: Esto le dice a SimpleTuner si debe usar c√°lculos de tama√±o `area` o c√°lculos de borde `pixel`. Tambi√©n se admite un enfoque h√≠brido de `pixel_area`, que permite usar p√≠xeles en lugar de megap√≠xeles para medidas de `area`.
- **Opciones**:
  - `resolution_type=pixel_area`
    - Un valor de `resolution` de 1024 se mapear√° internamente a una medici√≥n de √°rea precisa para un bucketizado eficiente por aspecto.
    - Tama√±os resultantes de ejemplo para `1024`: 1024x1024, 1216x832, 832x1216
  - `resolution_type=pixel`
    - Todas las im√°genes del dataset tendr√°n su lado menor redimensionado a esta resoluci√≥n para el entrenamiento, lo que podr√≠a resultar en un gran uso de VRAM por el tama√±o de las im√°genes resultantes.
    - Tama√±os resultantes de ejemplo para `1024`: 1024x1024, 1766x1024, 1024x1766
  - `resolution_type=area`
    - **Obsoleto**. Usa `pixel_area` en su lugar.

### `--resolution`

- **Qu√©**: Resoluci√≥n de imagen de entrada expresada en longitud de borde en p√≠xeles
- **Predeterminado**: 1024
- **Nota**: Este es el valor global por defecto si un dataset no tiene una resoluci√≥n definida.

### `--validation_resolution`

- **Qu√©**: Resoluci√≥n de imagen de salida, medida en p√≠xeles o en formato `widthxheight`, como `1024x1024`. Se pueden definir m√∫ltiples resoluciones separadas por comas.
- **Por qu√©**: Todas las im√°genes generadas durante la validaci√≥n tendr√°n esta resoluci√≥n. √ötil si el modelo se entrena con una resoluci√≥n distinta.

### `--validation_method`

- **Qu√©**: Elige c√≥mo se ejecutan las validaciones.
- **Opciones**: `simpletuner-local` (predeterminado) ejecuta la canalizaci√≥n incorporada; `external-script` ejecuta un ejecutable proporcionado por el usuario.
- **Por qu√©**: Te permite delegar la validaci√≥n a un sistema externo sin pausar el entrenamiento por el trabajo de la canalizaci√≥n local.

### `--validation_external_script`

- **Qu√©**: Ejecutable que se ejecuta cuando `--validation_method=external-script`. Usa separaci√≥n estilo shell, por lo que debes entrecomillar la cadena de comando adecuadamente.
- **Marcadores**: Puedes incrustar estos tokens (formateados con `.format`) para pasar contexto de entrenamiento. Los valores faltantes se reemplazan por una cadena vac√≠a salvo que se indique:
  - `{local_checkpoint_path}` ‚Üí directorio del √∫ltimo checkpoint bajo `output_dir` (requiere al menos un checkpoint).
  - `{global_step}` ‚Üí paso global actual.
  - `{tracker_run_name}` ‚Üí valor de `--tracker_run_name`.
  - `{tracker_project_name}` ‚Üí valor de `--tracker_project_name`.
  - `{model_family}` ‚Üí valor de `--model_family`.
  - `{model_type}` / `{lora_type}` ‚Üí tipo de modelo y variante LoRA.
  - `{huggingface_path}` ‚Üí valor de `--hub_model_id` (si est√° configurado).
  - `{remote_checkpoint_path}` ‚Üí URL remota de tu √∫ltima subida (vac√≠o para el hook de validaci√≥n).
  - Cualquier valor de configuraci√≥n `validation_*` (p. ej., `validation_num_inference_steps`, `validation_guidance`, `validation_noise_scheduler`).
- **Ejemplo**: `--validation_external_script="/opt/tools/validate.sh {local_checkpoint_path} {global_step}"`

### `--validation_external_background`

- **Qu√©**: Cuando est√° configurado, `--validation_external_script` se lanza en segundo plano (fire-and-forget).
- **Por qu√©**: Mant√©n el entrenamiento en marcha sin esperar al script externo; en este modo no se comprueban c√≥digos de salida.

### `--post_upload_script`

- **Qu√©**: Ejecutable opcional que se ejecuta despu√©s de que cada proveedor de publicaci√≥n y la subida a Hugging Face Hub termina (subidas finales del modelo y de checkpoints). Se ejecuta de forma as√≠ncrona para que el entrenamiento no se bloquee.
- **Marcadores**: Mismas sustituciones que `--validation_external_script`, adem√°s de `{remote_checkpoint_path}` (URI devuelta por el proveedor) para que puedas reenviar la URL publicada a sistemas downstream.
- **Notas**:
  - Los scripts se ejecutan por proveedor/subida; los errores se registran pero no detienen el entrenamiento.
  - Los scripts tambi√©n se invocan cuando no ocurre ninguna subida remota, por lo que puedes usarlos para automatizaci√≥n local (p. ej., ejecutar inferencia en otra GPU).
  - SimpleTuner no ingiere resultados de tu script; registra directamente en tu tracker si quieres m√©tricas o im√°genes.
- **Ejemplo**:
  ```bash
  --post_upload_script='/opt/hooks/notify.sh {remote_checkpoint_path} {tracker_project_name} {tracker_run_name}'
  ```
  Donde `/opt/hooks/notify.sh` podr√≠a publicar en tu sistema de tracking:
  ```bash
  #!/usr/bin/env bash
  REMOTE="$1"
  PROJECT="$2"
  RUN="$3"
  curl -X POST "https://tracker.internal/api/runs/${PROJECT}/${RUN}/artifacts" \
       -H "Content-Type: application/json" \
       -d "{\"remote_uri\":\"${REMOTE}\"}"
  ```
- **Ejemplos funcionales**:
  - `simpletuner/examples/external-validation/replicate_post_upload.py` muestra un hook de Replicate que consume `{remote_checkpoint_path}`, `{model_family}`, `{model_type}`, `{lora_type}` y `{huggingface_path}` para disparar inferencia despu√©s de las subidas.
  - `simpletuner/examples/external-validation/wavespeed_post_upload.py` muestra un hook de WaveSpeed usando los mismos marcadores m√°s el polling as√≠ncrono de WaveSpeed.
  - `simpletuner/examples/external-validation/fal_post_upload.py` muestra un hook de Flux LoRA en fal.ai (requiere `FAL_KEY`).
  - `simpletuner/examples/external-validation/use_second_gpu.py` ejecuta inferencia Flux LoRA en una GPU secundaria y funciona incluso sin subidas remotas.

### `--post_checkpoint_script`

- **Qu√©**: Ejecutable que se ejecuta inmediatamente despu√©s de que cada directorio de checkpoint se escribe en disco (antes de que comiencen las subidas). Se ejecuta de forma as√≠ncrona en el proceso principal.
- **Marcadores**: Mismas sustituciones que `--validation_external_script`, incluyendo `{local_checkpoint_path}`, `{global_step}`, `{tracker_run_name}`, `{tracker_project_name}`, `{model_family}`, `{model_type}`, `{lora_type}`, `{huggingface_path}` y cualquier valor de configuraci√≥n `validation_*`. `{remote_checkpoint_path}` se resuelve a vac√≠o para este hook.
- **Notas**:
  - Se dispara para checkpoints programados, manuales y de rolling tan pronto como terminan de guardarse localmente.
  - √ötil para lanzar automatizaciones locales (copiar a otro volumen, ejecutar jobs de evaluaci√≥n) sin esperar a que las subidas terminen.
- **Ejemplo**:
  ```bash
  --post_checkpoint_script='/opt/hooks/run_eval.sh {local_checkpoint_path} {global_step}'
  ```


### `--validation_adapter_path`

- **Qu√©**: Carga temporalmente un √∫nico adaptador LoRA al ejecutar validaciones programadas.
- **Formatos**:
  - Repo de Hugging Face: `org/repo` o `org/repo:weight_name.safetensors` (por defecto `pytorch_lora_weights.safetensors`).
  - Ruta de archivo local o directorio que apunte a un adaptador safetensors.
- **Notas**:
  - Mutuamente excluyente con `--validation_adapter_config`; proporcionar ambos lanza un error.
  - El adaptador solo se adjunta para validaciones (los pesos base del entrenamiento permanecen intactos).

### `--validation_adapter_name`

- **Qu√©**: Identificador opcional que se aplica al adaptador temporal cargado v√≠a `--validation_adapter_path`.
- **Por qu√©**: Controla c√≥mo se etiqueta la ejecuci√≥n del adaptador en logs/Web UI y garantiza nombres de adaptador predecibles cuando se prueban varios adaptadores en secuencia.

### `--validation_adapter_strength`

- **Qu√©**: Multiplicador de fuerza aplicado al habilitar el adaptador temporal (predeterminado `1.0`).
- **Por qu√©**: Te permite barrer escalas LoRA m√°s ligeras/pesadas durante la validaci√≥n sin alterar el estado de entrenamiento; acepta cualquier valor mayor que cero.

### `--validation_adapter_mode`

- **Opciones**: `adapter_only`, `comparison`, `none`
- **Qu√©**:
  - `adapter_only`: ejecuta validaciones solo con el adaptador temporal adjunto.
  - `comparison`: genera muestras tanto del modelo base como con el adaptador habilitado para revisi√≥n lado a lado.
  - `none`: omite adjuntar el adaptador (√∫til para desactivar la funci√≥n sin borrar flags de CLI).

### `--validation_adapter_config`

- **Qu√©**: Archivo JSON o JSON inline que describe m√∫ltiples combinaciones de adaptadores de validaci√≥n para iterar.
- **Formato**: Ya sea un array de entradas o un objeto con un array `runs`. Cada entrada puede incluir:
  - `label`: Nombre amigable mostrado en logs/UI.
  - `path`: ID de repo de Hugging Face o ruta local (mismos formatos que `--validation_adapter_path`).
  - `adapter_name`: Identificador opcional por adaptador.
  - `strength`: Override escalar opcional.
  - `adapters`/`paths`: Array de objetos/cadenas para cargar m√∫ltiples adaptadores en una sola ejecuci√≥n.
- **Notas**:
  - Cuando se proporciona, las opciones de adaptador √∫nico (`--validation_adapter_path`, `--validation_adapter_name`, `--validation_adapter_strength`, `--validation_adapter_mode`) se ignoran/deshabilitan en la UI.
  - Cada ejecuci√≥n se carga una a la vez y se desmonta completamente antes de comenzar la siguiente.

### `--validation_preview`

- **Qu√©**: Transmite vistas previas intermedias de validaci√≥n durante el muestreo de difusi√≥n usando Tiny AutoEncoders
- **Predeterminado**: False
- **Por qu√©**: Habilita la vista previa en tiempo real de im√°genes de validaci√≥n a medida que se generan, decodificadas mediante modelos Tiny AutoEncoder livianos y enviadas a trav√©s de callbacks de webhook. Esto permite monitorear la progresi√≥n de las muestras de validaci√≥n paso a paso en lugar de esperar a la generaci√≥n completa.
- **Notas**:
  - Solo disponible en familias de modelos con soporte de Tiny AutoEncoder (p. ej., Flux, SDXL, SD3)
  - Requiere configuraci√≥n de webhook para recibir im√°genes de vista previa
  - Usa `--validation_preview_steps` para controlar con qu√© frecuencia se decodifican las vistas previas

### `--validation_preview_steps`

- **Qu√©**: Intervalo para decodificar y transmitir vistas previas de validaci√≥n
- **Predeterminado**: 1
- **Por qu√©**: Controla con qu√© frecuencia se decodifican los latentes intermedios durante el muestreo de validaci√≥n. Establecerlo en un valor m√°s alto (p. ej., 3) reduce el overhead de ejecutar el Tiny AutoEncoder al decodificar solo cada N pasos de muestreo.
- **Ejemplo**: Con `--validation_num_inference_steps=20` y `--validation_preview_steps=5`, recibir√°s 4 im√°genes de vista previa durante el proceso de generaci√≥n (en los pasos 5, 10, 15, 20).

### `--evaluation_type`

- **Qu√©**: Habilita la evaluaci√≥n CLIP de im√°genes generadas durante las validaciones.
- **Por qu√©**: Los puntajes CLIP calculan la distancia de las caracter√≠sticas de la imagen generada al prompt de validaci√≥n proporcionado. Esto puede dar una idea de si la adherencia al prompt mejora, aunque requiere un gran n√∫mero de prompts de validaci√≥n para tener valor significativo.
- **Opciones**: "none" o "clip"
- **Programaci√≥n**: Usa `--eval_steps_interval` para programaci√≥n por pasos o `--eval_epoch_interval` para programaci√≥n por √©pocas (fracciones como `0.5` se ejecutan varias veces por √©poca). Si ambos est√°n configurados, el trainer registra una advertencia y ejecuta ambos calendarios.

- **Programaci√≥n**: Usa `--eval_steps_interval` para programaci√≥n por pasos o `--eval_epoch_interval` para programaci√≥n por √©pocas (fracciones como `0.5` se ejecutan varias veces por √©poca). Si ambos est√°n configurados, el trainer registra una advertencia y ejecuta ambos calendarios.

### `--eval_loss_disable`

- **Qu√©**: Desactiva el c√°lculo de p√©rdida de evaluaci√≥n durante la validaci√≥n.
- **Por qu√©**: Cuando se configura un dataset de eval, la p√©rdida se calcula autom√°ticamente. Si la evaluaci√≥n CLIP tambi√©n est√° habilitada, ambas se ejecutar√°n. Este flag te permite desactivar selectivamente la p√©rdida de eval manteniendo la evaluaci√≥n CLIP habilitada.

### `--validation_using_datasets`

- **Qu√©**: Usa im√°genes de datasets de entrenamiento para validaci√≥n en lugar de generaci√≥n pura de texto a imagen.
- **Por qu√©**: Habilita el modo de validaci√≥n imagen-a-imagen (img2img) donde el modelo des-ruido parcialmente im√°genes de entrenamiento en lugar de generar desde ruido puro. √ötil para:
  - Probar modelos de edici√≥n/inpainting que requieren im√°genes de entrada
  - Evaluar qu√© tan bien el modelo preserva la estructura de imagen
  - Modelos que soportan flujos duales texto-a-imagen E imagen-a-imagen (ej., Flux2, LTXVideo2)
- **Notas**:
  - Requiere que el modelo tenga un pipeline `IMG2IMG` registrado
  - Puede combinarse con `--eval_dataset_id` para obtener im√°genes de un dataset espec√≠fico
  - La fuerza de des-ruido se controla con los ajustes normales de timestep de validaci√≥n

### `--eval_dataset_id`

- **Qu√©**: ID de dataset espec√≠fico a usar para obtener im√°genes de evaluaci√≥n/validaci√≥n.
- **Por qu√©**: Al usar `--validation_using_datasets` o validaci√≥n basada en conditioning, controla qu√© dataset provee las im√°genes de entrada:
  - Sin esta opci√≥n, las im√°genes se seleccionan aleatoriamente de todos los datasets de entrenamiento
  - Con esta opci√≥n, solo se usa el dataset especificado para entradas de validaci√≥n
- **Notas**:
  - El ID de dataset debe coincidir con un dataset configurado en tu config de dataloader
  - √ötil para mantener evaluaci√≥n consistente usando un dataset de eval dedicado
  - Para modelos de conditioning, los datos de conditioning del dataset (si existen) tambi√©n se usar√°n

---

## Entendiendo Modos de Conditioning y Validaci√≥n

SimpleTuner soporta tres paradigmas principales para modelos que usan entradas de conditioning (im√°genes de referencia, se√±ales de control, etc.):

### 1. Modelos que REQUIEREN Conditioning

Algunos modelos no pueden funcionar sin entradas de conditioning:

- **Flux Kontext**: Siempre necesita im√°genes de referencia para entrenamiento estilo edici√≥n
- **Entrenamiento ControlNet**: Requiere im√°genes de se√±al de control

Para estos modelos, un dataset de conditioning es obligatorio. La WebUI mostrar√° opciones de conditioning como requeridas, y el entrenamiento fallar√° sin ellas.

### 2. Modelos que SOPORTAN Conditioning Opcional

Algunos modelos pueden operar en modos texto-a-imagen E imagen-a-imagen:

- **Flux2**: Soporta entrenamiento dual T2I/I2I con im√°genes de referencia opcionales
- **LTXVideo2**: Soporta T2V e I2V (imagen-a-video) con conditioning de primer frame opcional
- **LongCat-Video**: Soporta conditioning de frames opcional

Para estos modelos, PUEDES agregar datasets de conditioning pero no es obligatorio. La WebUI mostrar√° opciones de conditioning como opcionales.

### 3. Modos de Validaci√≥n

| Modo | Flag | Comportamiento |
|------|------|----------------|
| **Texto-a-Imagen** | (por defecto) | Genera solo desde prompts de texto |
| **Basado en Dataset** | `--validation_using_datasets` | Des-ruido parcial de im√°genes de datasets (img2img) |
| **Basado en Conditioning** | (auto cuando se configura conditioning) | Usa entradas de conditioning durante validaci√≥n |

**Combinando modos**: Cuando un modelo soporta conditioning Y `--validation_using_datasets` est√° habilitado:
- El sistema de validaci√≥n obtiene im√°genes de datasets
- Si esos datasets tienen datos de conditioning, se usan autom√°ticamente
- Usa `--eval_dataset_id` para controlar qu√© dataset provee entradas

### Tipos de Datos de Conditioning

Diferentes modelos esperan diferentes datos de conditioning:

| Tipo | Modelos | Configuraci√≥n de Dataset |
|------|---------|-------------------------|
| `conditioning` | ControlNet, Control | `type: conditioning` en config de dataset |
| `image` | Flux Kontext | `type: image` (dataset de imagen est√°ndar) |
| `latents` | Flux, Flux2 | Conditioning se codifica con VAE autom√°ticamente |

---

### `--caption_strategy`

- **Qu√©**: Estrategia para derivar captions de imagen. **Opciones**: `textfile`, `filename`, `parquet`, `instanceprompt`
- **Por qu√©**: Determina c√≥mo se generan los captions para im√°genes de entrenamiento.
  - `textfile` usar√° el contenido de un archivo `.txt` con el mismo nombre que la imagen
  - `filename` aplicar√° una limpieza al nombre del archivo antes de usarlo como caption.
  - `parquet` requiere un archivo parquet presente en el dataset y usar√° la columna `caption` como caption salvo que se proporcione `parquet_caption_column`. Todas las captions deben estar presentes a menos que se proporcione `parquet_fallback_caption_column`.
  - `instanceprompt` usar√° el valor de `instance_prompt` en la configuraci√≥n del dataset como prompt para cada imagen del dataset.

### `--conditioning_multidataset_sampling` {#--conditioning_multidataset_sampling}

- **Qu√©**: C√≥mo muestrear desde m√∫ltiples datasets de condicionamiento. **Opciones**: `combined`, `random`
- **Por qu√©**: Al entrenar con m√∫ltiples datasets de condicionamiento (p. ej., m√∫ltiples im√°genes de referencia o se√±ales de control), esto determina c√≥mo se usan:
  - `combined` une las entradas de condicionamiento, mostr√°ndolas simult√°neamente durante el entrenamiento. √ötil para tareas de composici√≥n multi-imagen.
  - `random` selecciona aleatoriamente un dataset de condicionamiento por muestra, alternando condiciones durante el entrenamiento.
- **Nota**: Al usar `combined`, no puedes definir `captions` separados en datasets de condicionamiento; se usan las captions del dataset fuente.
- **Ver tambi√©n**: [DATALOADER.md](DATALOADER.md#conditioning_data) para configurar m√∫ltiples datasets de condicionamiento.

---

## üéõ Par√°metros de entrenamiento

### `--num_train_epochs`

- **Qu√©**: N√∫mero de √©pocas de entrenamiento (el n√∫mero de veces que se ven todas las im√°genes). Configurar esto en 0 permitir√° que `--max_train_steps` tenga prioridad.
- **Por qu√©**: Determina el n√∫mero de repeticiones de imagen, lo que impacta la duraci√≥n del proceso de entrenamiento. M√°s √©pocas tienden a resultar en sobreajuste, pero podr√≠an ser necesarias para aprender los conceptos que deseas entrenar. Un valor razonable podr√≠a estar entre 5 y 50.

### `--max_train_steps`

- **Qu√©**: N√∫mero de pasos de entrenamiento tras los cuales salir del entrenamiento. Si se establece en 0, permitir√° que `--num_train_epochs` tenga prioridad.
- **Por qu√©**: √ötil para acortar la duraci√≥n del entrenamiento.

### `--ignore_final_epochs`

- **Qu√©**: Ignora las √∫ltimas √©pocas contadas en favor de `--max_train_steps`.
- **Por qu√©**: Al cambiar la longitud del dataloader, el entrenamiento puede terminar antes de lo que quieres porque el c√°lculo de √©pocas cambia. Esta opci√≥n ignorar√° las √©pocas finales y en su lugar continuar√° entrenando hasta que se alcance `--max_train_steps`.

### `--learning_rate`

- **Qu√©**: Tasa de aprendizaje inicial tras el posible warmup.
- **Por qu√©**: La tasa de aprendizaje se comporta como un "tama√±o de paso" para las actualizaciones de gradiente: demasiado alta y nos pasamos de la soluci√≥n; demasiado baja y nunca llegamos a la soluci√≥n ideal. Un valor m√≠nimo para un ajuste `full` puede ser tan bajo como `1e-7` hasta un m√°ximo de `1e-6`, mientras que para ajuste `lora` un valor m√≠nimo podr√≠a ser `1e-5` con un m√°ximo tan alto como `1e-3`. Cuando se usa una tasa de aprendizaje m√°s alta, es ventajoso usar una red EMA con warmup de tasa de aprendizaje; consulta `--use_ema`, `--lr_warmup_steps` y `--lr_scheduler`.

### `--lr_scheduler`

- **Qu√©**: C√≥mo escalar la tasa de aprendizaje en el tiempo.
- **Opciones**: constant, constant_with_warmup, cosine, cosine_with_restarts, **polynomial** (recomendado), linear
- **Por qu√©**: Los modelos se benefician de ajustes continuos de la tasa de aprendizaje para explorar mejor el paisaje de p√©rdida. Un calendario cosine se usa como predeterminado; permite que el entrenamiento transicione suavemente entre dos extremos. Si se usa una tasa constante, es com√∫n seleccionar un valor demasiado alto o demasiado bajo, causando divergencia (demasiado alto) o qued√°ndose atrapado en un m√≠nimo local (demasiado bajo). Un calendario polynomial se combina mejor con un warmup, donde se acercar√° gradualmente al valor `learning_rate` antes de ralentizarse y aproximarse a `--lr_end` al final.

### `--optimizer`

- **Qu√©**: El optimizador a usar para el entrenamiento.
- **Opciones**: adamw_bf16, ao-adamw8bit, ao-adamw4bit, ao-adamfp8, ao-adamwfp8, adamw_schedulefree, adamw_schedulefree+aggressive, adamw_schedulefree+no_kahan, optimi-stableadamw, optimi-adamw, optimi-lion, optimi-radam, optimi-ranger, optimi-adan, optimi-adam, optimi-sgd, soap, bnb-adagrad, bnb-adagrad8bit, bnb-adam, bnb-adam8bit, bnb-adamw, bnb-adamw8bit, bnb-adamw-paged, bnb-adamw8bit-paged, bnb-lion, bnb-lion8bit, bnb-lion-paged, bnb-lion8bit-paged, bnb-ademamix, bnb-ademamix8bit, bnb-ademamix-paged, bnb-ademamix8bit-paged, prodigy

> Nota: Algunos optimizadores pueden no estar disponibles en hardware no NVIDIA.

### `--optimizer_config`

- **Qu√©**: Ajusta la configuraci√≥n del optimizador.
- **Por qu√©**: Como los optimizadores tienen tantas configuraciones diferentes, no es viable proporcionar un argumento de l√≠nea de comandos para cada una. En su lugar, puedes proporcionar una lista separada por comas de valores para sobrescribir cualquiera de los valores predeterminados.
- **Ejemplo**: Tal vez quieras establecer `d_coef` para el optimizador **prodigy**: `--optimizer_config=d_coef=0.1`

> Nota: Las betas del optimizador se sobrescriben usando par√°metros dedicados, `--optimizer_beta1`, `--optimizer_beta2`.

### `--train_batch_size`

- **Qu√©**: Tama√±o de batch para el data loader de entrenamiento.
- **Por qu√©**: Afecta el consumo de memoria del modelo, la calidad de convergencia y la velocidad de entrenamiento. Cuanto mayor sea el batch size, mejores ser√°n los resultados, pero un batch muy alto puede resultar en sobreajuste o entrenamiento desestabilizado, adem√°s de aumentar innecesariamente la duraci√≥n de la sesi√≥n. La experimentaci√≥n est√° justificada, pero en general, quieres maximizar la memoria de video sin reducir la velocidad de entrenamiento.

### `--gradient_accumulation_steps`

- **Qu√©**: N√∫mero de pasos de actualizaci√≥n a acumular antes de realizar un pase hacia atr√°s/actualizaci√≥n, esencialmente dividiendo el trabajo en m√∫ltiples lotes para ahorrar memoria a costa de mayor tiempo de entrenamiento.
- **Por qu√©**: √ötil para manejar modelos o datasets m√°s grandes.

> Nota: No habilites el pase backward fusionado para ning√∫n optimizador al usar pasos de acumulaci√≥n de gradiente.

### `--allow_dataset_oversubscription` {#--allow_dataset_oversubscription}

- **Qu√©**: Ajusta autom√°ticamente los `repeats` del dataset cuando el dataset es m√°s peque√±o que el tama√±o de batch efectivo.
- **Por qu√©**: Evita fallos de entrenamiento cuando el tama√±o de tu dataset no cumple los requisitos m√≠nimos de tu configuraci√≥n multi-GPU.
- **C√≥mo funciona**:
  - Calcula el **tama√±o de batch efectivo**: `train_batch_size √ó num_gpus √ó gradient_accumulation_steps`
  - Si cualquier bucket de aspecto tiene menos muestras que el tama√±o de batch efectivo, incrementa autom√°ticamente `repeats`
  - Solo aplica cuando `repeats` no est√° configurado expl√≠citamente en tu configuraci√≥n de dataset
  - Registra una advertencia mostrando el ajuste y el razonamiento
- **Casos de uso**:
  - Datasets peque√±os (< 100 im√°genes) con m√∫ltiples GPUs
  - Experimentar con distintos tama√±os de batch sin reconfigurar datasets
  - Prototipado antes de recopilar un dataset completo
- **Ejemplo**: Con 25 im√°genes, 8 GPUs y `train_batch_size=4`, el tama√±o de batch efectivo es 32. Este flag establecer√≠a autom√°ticamente `repeats=1` para proporcionar 50 muestras (25 √ó 2).
- **Nota**: Esto **no** sobrescribir√° valores `repeats` configurados manualmente en tu configuraci√≥n de dataloader. Similar a `--disable_bucket_pruning`, este flag ofrece conveniencia sin comportamiento sorprendente.

Consulta la gu√≠a [DATALOADER.md](DATALOADER.md#automatic-dataset-oversubscription) para m√°s detalles sobre el tama√±o de dataset para entrenamiento multi-GPU.

---

## üõ† Optimizaciones avanzadas

### `--use_ema`

- **Qu√©**: Mantener una media m√≥vil exponencial de tus pesos durante la vida de entrenamiento del modelo es como fusionar peri√≥dicamente el modelo en s√≠ mismo.
- **Por qu√©**: Puede mejorar la estabilidad del entrenamiento a costa de m√°s recursos del sistema y un ligero incremento en el tiempo de entrenamiento.

### `--ema_device`

- **Opciones**: `cpu`, `accelerator`; predeterminado: `cpu`
- **Qu√©**: Elige d√≥nde viven los pesos EMA entre actualizaciones.
- **Por qu√©**: Mantener la EMA en el acelerador da las actualizaciones m√°s r√°pidas pero consume VRAM. Mantenerla en CPU reduce la presi√≥n de memoria pero requiere mover pesos a menos que se configure `--ema_cpu_only`.

### `--ema_cpu_only`

- **Qu√©**: Evita que los pesos EMA se muevan de vuelta al acelerador para actualizaciones cuando `--ema_device=cpu`.
- **Por qu√©**: Ahorra el tiempo de transferencia host-dispositivo y el uso de VRAM para EMAs grandes. No tiene efecto si `--ema_device=accelerator` porque los pesos ya residen en el acelerador.

### `--ema_foreach_disable`

- **Qu√©**: Desactiva el uso de kernels `torch._foreach_*` para actualizaciones EMA.
- **Por qu√©**: Algunos backends o combinaciones de hardware tienen problemas con ops foreach. Deshabilitarlos vuelve a la implementaci√≥n escalar a costa de actualizaciones ligeramente m√°s lentas.

### `--ema_update_interval`

- **Qu√©**: Reduce la frecuencia con la que se actualizan los par√°metros sombra EMA.
- **Por qu√©**: Actualizar en cada paso es innecesario para muchos flujos de trabajo. Por ejemplo, `--ema_update_interval=100` solo realiza una actualizaci√≥n EMA cada 100 pasos de optimizador, reduciendo overhead cuando `--ema_device=cpu` o `--ema_cpu_only` est√° habilitado.

### `--ema_decay`

- **Qu√©**: Controla el factor de suavizado usado al aplicar actualizaciones EMA.
- **Por qu√©**: Valores m√°s altos (p. ej., `0.999`) hacen que la EMA responda lentamente pero produce pesos muy estables. Valores m√°s bajos (p. ej., `0.99`) se adaptan m√°s r√°pido a nuevas se√±ales de entrenamiento.

### `--snr_gamma`

- **Qu√©**: Utiliza un factor de p√©rdida ponderado por min-SNR.
- **Por qu√©**: La gamma de SNR m√≠nima pondera el factor de p√©rdida de un timestep por su posici√≥n en el calendario. Los timesteps muy ruidosos reducen su contribuci√≥n y los menos ruidosos la aumentan. El valor recomendado por el paper original es **5**, pero puedes usar valores tan bajos como **1** o tan altos como **20**, t√≠picamente el m√°ximo; m√°s all√° de 20, las matem√°ticas no cambian mucho. Un valor de **1** es el m√°s fuerte.

### `--use_soft_min_snr`

- **Qu√©**: Entrena un modelo usando un ponderado m√°s gradual en el paisaje de p√©rdida.
- **Por qu√©**: Al entrenar modelos de difusi√≥n en p√≠xeles, simplemente se degradar√°n sin usar una programaci√≥n espec√≠fica de ponderaci√≥n de p√©rdida. Este es el caso con DeepFloyd, donde se encontr√≥ que soft-min-snr-gamma era esencialmente obligatorio para buenos resultados. Puedes tener √©xito con entrenamiento de modelos de difusi√≥n latente, pero en experimentos peque√±os se encontr√≥ que potencialmente produce resultados borrosos.

### `--diff2flow_enabled`

- **Qu√©**: Habilita el puente Diffusion-to-Flow para modelos epsilon o v-prediction.
- **Por qu√©**: Permite que los modelos entrenados con objetivos de difusi√≥n est√°ndar usen objetivos de flow-matching (ruido - latentes) sin cambiar la arquitectura del modelo.
- **Nota**: Funci√≥n experimental.

### `--diff2flow_loss`

- **Qu√©**: Entrena con p√©rdida de Flow Matching en lugar de la p√©rdida nativa de predicci√≥n.
- **Por qu√©**: Cuando se habilita junto con `--diff2flow_enabled`, calcula la p√©rdida contra el objetivo de flujo (ruido - latentes) en lugar del objetivo nativo del modelo (epsilon o velocidad).
- **Nota**: Requiere `--diff2flow_enabled`.

### `--scheduled_sampling_max_step_offset`

- **Qu√©**: N√∫mero m√°ximo de pasos para hacer "rollout" durante el entrenamiento.
- **Por qu√©**: Habilita Scheduled Sampling (Rollout), donde el modelo genera sus propias entradas durante algunos pasos en el entrenamiento. Esto ayuda al modelo a aprender a corregir sus propios errores y reduce el sesgo de exposici√≥n.
- **Predeterminado**: 0 (desactivado). Configura un entero positivo (p. ej., 5 o 10) para habilitarlo.

### `--scheduled_sampling_strategy`

- **Qu√©**: Estrategia para elegir el offset de rollout.
- **Opciones**: `uniform`, `biased_early`, `biased_late`.
- **Predeterminado**: `uniform`.
- **Por qu√©**: Controla la distribuci√≥n de longitudes de rollout. `uniform` muestrea uniformemente; `biased_early` favorece rollouts m√°s cortos; `biased_late` favorece rollouts m√°s largos.

### `--scheduled_sampling_probability`

- **Qu√©**: Probabilidad de aplicar un offset de rollout no cero para una muestra dada.
- **Predeterminado**: 0.0.
- **Por qu√©**: Controla con qu√© frecuencia se aplica scheduled sampling. Un valor de 0.0 lo desactiva incluso si `max_step_offset` > 0. Un valor de 1.0 lo aplica a cada muestra.

### `--scheduled_sampling_prob_start`

- **Qu√©**: Probabilidad inicial para scheduled sampling al inicio del ramp.
- **Predeterminado**: 0.0.

### `--scheduled_sampling_prob_end`

- **Qu√©**: Probabilidad final para scheduled sampling al final del ramp.
- **Predeterminado**: 0.5.

### `--scheduled_sampling_ramp_steps`

- **Qu√©**: N√∫mero de pasos para aumentar la probabilidad de `prob_start` a `prob_end`.
- **Predeterminado**: 0 (sin ramp).

### `--scheduled_sampling_start_step`

- **Qu√©**: Paso global para iniciar el ramp de scheduled sampling.
- **Predeterminado**: 0.0.

### `--scheduled_sampling_ramp_shape`

- **Qu√©**: Forma del ramp de probabilidad.
- **Opciones**: `linear`, `cosine`.
- **Predeterminado**: `linear`.

### `--scheduled_sampling_sampler`

- **Qu√©**: El solver usado para los pasos de generaci√≥n de rollout.
- **Opciones**: `unipc`, `euler`, `dpm`, `rk4`.
- **Predeterminado**: `unipc`.

### `--scheduled_sampling_order`

- **Qu√©**: El orden del solver usado para el rollout.
- **Predeterminado**: 2.

### `--scheduled_sampling_reflexflow`

- **Qu√©**: Habilita mejoras estilo ReflexFlow (anti-drift + ponderaci√≥n compensada por frecuencia) durante el scheduled sampling para modelos de flow-matching.
- **Por qu√©**: Reduce el sesgo de exposici√≥n al hacer rollout en modelos de flow-matching a√±adiendo regularizaci√≥n direccional y ponderaci√≥n de p√©rdida consciente del sesgo.
- **Predeterminado**: Se habilita autom√°ticamente para modelos de flow-matching cuando `--scheduled_sampling_max_step_offset` > 0; sobrescribe con `--scheduled_sampling_reflexflow=false`.

### `--scheduled_sampling_reflexflow_alpha`

- **Qu√©**: Factor de escala para el peso de compensaci√≥n de frecuencia derivado del sesgo de exposici√≥n.
- **Predeterminado**: 1.0.
- **Por qu√©**: Valores m√°s altos aumentan el peso de las regiones con mayor sesgo de exposici√≥n durante el rollout en modelos de flow-matching.

### `--scheduled_sampling_reflexflow_beta1`

- **Qu√©**: Peso para el regularizador anti-drift (direccional) de ReflexFlow.
- **Predeterminado**: 10.0.
- **Por qu√©**: Controla cu√°n fuertemente se incentiva al modelo a alinear su direcci√≥n predicha con la muestra limpia objetivo al usar scheduled sampling en modelos de flow-matching.

### `--scheduled_sampling_reflexflow_beta2`

- **Qu√©**: Peso para el t√©rmino de compensaci√≥n de frecuencia (reponderaci√≥n de p√©rdida) de ReflexFlow.
- **Predeterminado**: 1.0.
- **Por qu√©**: Escala la p√©rdida de flow-matching reponderada, coincidiendo con el control Œ≤‚ÇÇ descrito en el paper de ReflexFlow.

---

## üéØ CREPA (Cross-frame Representation Alignment)

CREPA es una t√©cnica de regularizaci√≥n para fine-tuning de modelos de difusi√≥n de video que mejora la consistencia temporal al alinear estados ocultos con caracter√≠sticas visuales preentrenadas de fotogramas adyacentes. Basado en el paper ["Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models"](https://arxiv.org/abs/2506.09229).

### `--crepa_enabled`

- **Qu√©**: Habilita la regularizaci√≥n CREPA durante el entrenamiento.
- **Por qu√©**: Mejora la consistencia sem√°ntica entre fotogramas de video al alinear estados ocultos DiT con caracter√≠sticas DINOv2 de fotogramas vecinos.
- **Predeterminado**: `false`
- **Nota**: Solo aplica a modelos de video (Wan, LTXVideo, SanaVideo, Kandinsky5).

### `--crepa_block_index`

- **Qu√©**: Qu√© bloque del transformer usar para estados ocultos para el alineamiento.
- **Por qu√©**: El paper recomienda el bloque 8 para CogVideoX y el bloque 10 para Hunyuan Video. Los bloques m√°s tempranos tienden a funcionar mejor ya que act√∫an como la porci√≥n "encoder" del DiT.
- **Requerido**: S√≠, cuando CREPA est√° habilitado.

### `--crepa_lambda`

- **Qu√©**: Peso de la p√©rdida de alineamiento CREPA relativo a la p√©rdida principal de entrenamiento.
- **Por qu√©**: Controla cu√°n fuertemente la regularizaci√≥n de alineamiento influye en el entrenamiento. El paper usa 0.5 para CogVideoX y 1.0 para Hunyuan Video.
- **Predeterminado**: `0.5`

### `--crepa_adjacent_distance`

- **Qu√©**: Distancia `d` para el alineamiento con fotogramas vecinos.
- **Por qu√©**: Seg√∫n la ecuaci√≥n 6 del paper, $K = \{f-d, f+d\}$ define qu√© fotogramas vecinos alinear. Con `d=1`, cada fotograma se alinea con sus vecinos inmediatos.
- **Predeterminado**: `1`

### `--crepa_adjacent_tau`

- **Qu√©**: Coeficiente de temperatura para la ponderaci√≥n exponencial por distancia.
- **Por qu√©**: Controla qu√© tan r√°pido decae el peso de alineamiento con la distancia entre fotogramas v√≠a $e^{-|k-f|/\tau}$. Valores m√°s bajos se enfocan m√°s en vecinos inmediatos.
- **Predeterminado**: `1.0`

### `--crepa_cumulative_neighbors`

- **Qu√©**: Usa modo acumulativo en lugar de modo adyacente.
- **Por qu√©**:
  - **Modo adyacente (predeterminado)**: Solo alinea con fotogramas a distancia exacta `d` (coincide con $K = \{f-d, f+d\}$ del paper)
  - **Modo acumulativo**: Alinea con todos los fotogramas desde distancia 1 hasta `d`, proporcionando gradientes m√°s suaves
- **Predeterminado**: `false`

### `--crepa_normalize_by_frames`

- **Qu√©**: Normaliza la p√©rdida de alineamiento por el n√∫mero de fotogramas.
- **Por qu√©**: Garantiza una escala de p√©rdida consistente sin importar la longitud del video. Deshabil√≠talo para dar a videos m√°s largos una se√±al de alineamiento m√°s fuerte.
- **Predeterminado**: `true`

### `--crepa_spatial_align`

- **Qu√©**: Usa interpolaci√≥n espacial cuando los recuentos de tokens difieren entre DiT y el encoder.
- **Por qu√©**: Los estados ocultos DiT y las caracter√≠sticas DINOv2 pueden tener diferentes resoluciones espaciales. Cuando est√° habilitado, la interpolaci√≥n bilineal los alinea espacialmente. Cuando est√° deshabilitado, recurre a un pooling global.
- **Predeterminado**: `true`

### `--crepa_model`

- **Qu√©**: Qu√© encoder preentrenado usar para extracci√≥n de caracter√≠sticas.
- **Por qu√©**: El paper usa DINOv2-g (ViT-Giant). Variantes m√°s peque√±as como `dinov2_vitb14` usan menos memoria.
- **Predeterminado**: `dinov2_vitg14`
- **Opciones**: `dinov2_vitg14`, `dinov2_vitb14`, `dinov2_vits14`

### `--crepa_encoder_frames_batch_size`

- **Qu√©**: Cu√°ntos fotogramas procesa en paralelo el encoder de caracter√≠sticas externo. Cero o negativo para todos los fotogramas del batch completo simult√°neamente. Si el n√∫mero no es divisor, el resto se manejar√° como un batch m√°s peque√±o.
- **Por qu√©**: Dado que los encoders tipo DINO son modelos de imagen, pueden procesar fotogramas en batches troceados para menor uso de VRAM a costa de velocidad.
- **Predeterminado**: `-1`

### `--crepa_use_backbone_features`

- **Qu√©**: Omite el encoder externo y alinea un bloque estudiante con un bloque maestro dentro del modelo de difusi√≥n.
- **Por qu√©**: Evita cargar DINOv2 cuando el backbone ya tiene una capa sem√°ntica m√°s fuerte para supervisar.
- **Predeterminado**: `false`

### `--crepa_teacher_block_index`

- **Qu√©**: √çndice del bloque maestro al usar caracter√≠sticas del backbone.
- **Por qu√©**: Te permite alinear un bloque estudiante temprano con un bloque maestro m√°s profundo sin un encoder externo. Si no se establece, cae en el bloque estudiante.
- **Predeterminado**: Usa `crepa_block_index` si no se proporciona.

### `--crepa_encoder_image_size`

- **Qu√©**: Resoluci√≥n de entrada para el encoder.
- **Por qu√©**: Los modelos DINOv2 funcionan mejor a su resoluci√≥n de entrenamiento. El modelo gigante usa 518x518.
- **Predeterminado**: `518`

### `--crepa_scheduler`

- **Qu√©**: Programa para el decaimiento del coeficiente CREPA durante el entrenamiento.
- **Por qu√©**: Permite reducir la fuerza de regularizaci√≥n CREPA a medida que avanza el entrenamiento, previniendo el sobreajuste en caracter√≠sticas profundas del encoder.
- **Opciones**: `constant`, `linear`, `cosine`, `polynomial`
- **Predeterminado**: `constant`

### `--crepa_warmup_steps`

- **Qu√©**: N√∫mero de pasos para incrementar linealmente el peso CREPA desde 0 hasta `crepa_lambda`.
- **Por qu√©**: Un calentamiento gradual puede ayudar a estabilizar el entrenamiento temprano antes de que la regularizaci√≥n CREPA entre en efecto.
- **Predeterminado**: `0`

### `--crepa_decay_steps`

- **Qu√©**: Pasos totales para el decaimiento (despu√©s del calentamiento). Establece a 0 para decaer durante todo el entrenamiento.
- **Por qu√©**: Controla la duraci√≥n de la fase de decaimiento. El decaimiento comienza despu√©s de que se completa el calentamiento.
- **Predeterminado**: `0` (usa `max_train_steps`)

### `--crepa_lambda_end`

- **Qu√©**: Peso CREPA final despu√©s de que se completa el decaimiento.
- **Por qu√©**: Establecerlo a 0 desactiva efectivamente CREPA al final del entrenamiento, √∫til para text2video donde CREPA puede causar artefactos.
- **Predeterminado**: `0.0`

### `--crepa_power`

- **Qu√©**: Factor de potencia para el decaimiento polinomial. 1.0 = lineal, 2.0 = cuadr√°tico, etc.
- **Por qu√©**: Valores m√°s altos causan un decaimiento inicial m√°s r√°pido que se ralentiza hacia el final.
- **Predeterminado**: `1.0`

### `--crepa_cutoff_step`

- **Qu√©**: Paso de corte duro despu√©s del cual CREPA se desactiva.
- **Por qu√©**: √ötil para desactivar CREPA despu√©s de que el modelo ha convergido en el alineamiento temporal.
- **Predeterminado**: `0` (sin corte basado en pasos)

### `--crepa_similarity_threshold`

- **Qu√©**: Umbral de EMA de similitud en el cual se activa el corte de CREPA.
- **Por qu√©**: Cuando el promedio m√≥vil exponencial de similitud alcanza este valor, CREPA se desactiva para prevenir el sobreajuste en caracter√≠sticas profundas del encoder. Esto es particularmente √∫til para entrenamiento text2video.
- **Predeterminado**: None (desactivado)

### `--crepa_similarity_ema_decay`

- **Qu√©**: Factor de decaimiento del promedio m√≥vil exponencial para el seguimiento de similitud.
- **Por qu√©**: Valores m√°s altos proporcionan un seguimiento m√°s suave (0.99 ‚âà ventana de 100 pasos), valores m√°s bajos reaccionan m√°s r√°pido a los cambios.
- **Predeterminado**: `0.99`

### `--crepa_threshold_mode`

- **Qu√©**: Comportamiento cuando se alcanza el umbral de similitud.
- **Opciones**: `permanent` (CREPA permanece desactivado una vez que se alcanza el umbral), `recoverable` (CREPA se reactiva si la similitud cae)
- **Predeterminado**: `permanent`

### Ejemplo de configuraci√≥n

```toml
# Habilitar CREPA para fine-tuning de video
crepa_enabled = true
crepa_block_index = 8          # Ajusta seg√∫n tu modelo
crepa_lambda = 0.5
crepa_adjacent_distance = 1
crepa_adjacent_tau = 1.0
crepa_cumulative_neighbors = false
crepa_normalize_by_frames = true
crepa_spatial_align = true
crepa_model = "dinov2_vitg14"
crepa_encoder_frames_batch_size = -1
crepa_use_backbone_features = false
# crepa_teacher_block_index = 16
crepa_encoder_image_size = 518

# Programaci√≥n CREPA (opcional)
# crepa_scheduler = "cosine"           # Tipo de decaimiento: constant, linear, cosine, polynomial
# crepa_warmup_steps = 100             # Calentamiento antes de que CREPA entre en efecto
# crepa_decay_steps = 1000             # Pasos para el decaimiento (0 = todo el entrenamiento)
# crepa_lambda_end = 0.0               # Peso final despu√©s del decaimiento
# crepa_cutoff_step = 5000             # Paso de corte duro (0 = desactivado)
# crepa_similarity_threshold = 0.9    # Corte basado en similitud
# crepa_threshold_mode = "permanent"   # permanent o recoverable
```

---

## üîÑ Checkpointing y reanudaci√≥n

### `--checkpoint_step_interval` (alias: `--checkpointing_steps`)

- **Qu√©**: Intervalo en el que se guardan los checkpoints del estado de entrenamiento (en pasos).
- **Por qu√©**: √ötil para reanudar entrenamiento y para inferencia. Cada *n* iteraciones, se guardar√° un checkpoint parcial en formato `.safetensors`, v√≠a el layout de sistema de archivos de Diffusers.

---

## üîÅ LayerSync (Autoalineaci√≥n de estados ocultos)

LayerSync anima a una capa "estudiante" a igualar una capa "maestra" m√°s fuerte dentro del mismo transformer, usando similitud coseno sobre tokens ocultos.

### `--layersync_enabled`

- **Qu√©**: Habilita el alineamiento de estados ocultos LayerSync entre dos bloques transformer dentro del mismo modelo.
- **Notas**: Asigna un buffer de estado oculto; falla al iniciar si faltan flags requeridos.
- **Predeterminado**: `false`

### `--layersync_student_block`

- **Qu√©**: √çndice del bloque transformer a tratar como ancla estudiante.
- **Indexaci√≥n**: Acepta profundidades estilo paper de LayerSync basadas en 1 o IDs de capa basados en 0; la implementaci√≥n prueba `idx-1` primero y luego `idx`.
- **Requerido**: S√≠ cuando LayerSync est√° habilitado.

### `--layersync_teacher_block`

- **Qu√©**: √çndice del bloque transformer a tratar como objetivo maestro (puede ser m√°s profundo que el estudiante).
- **Indexaci√≥n**: Misma preferencia 1-based, luego fallback 0-based que el bloque estudiante.
- **Predeterminado**: Usa el bloque estudiante cuando se omite para que la p√©rdida sea auto-similitud.

### `--layersync_lambda`

- **Qu√©**: Peso de la p√©rdida de alineamiento coseno de LayerSync entre los estados ocultos del estudiante y del maestro (similitud coseno negativa).
- **Efecto**: Escala el regularizador auxiliar a√±adido sobre la p√©rdida base; valores m√°s altos empujan a los tokens del estudiante a alinearse m√°s fuertemente con los tokens del maestro.
- **Nombre upstream**: `--reg-weight` en el codebase original de LayerSync.
- **Requerido**: Debe ser > 0 cuando LayerSync est√° habilitado (de lo contrario el entrenamiento aborta).
- **Predeterminado**: `0.2` cuando LayerSync est√° habilitado (coincide con el repo de referencia), `0.0` en otro caso.

Mapeo de opciones upstream (LayerSync ‚Üí SimpleTuner):
- `--encoder-depth` ‚Üí `--layersync_student_block` (acepta profundidad 1-based como en upstream, o √≠ndice de capa 0-based)
- `--gt-encoder-depth` ‚Üí `--layersync_teacher_block` (se prefiere 1-based; por defecto usa el estudiante cuando se omite)
- `--reg-weight` ‚Üí `--layersync_lambda`

> Notas: LayerSync siempre separa el estado oculto del maestro antes de la similitud, igual que la implementaci√≥n de referencia. Depende de modelos que expongan estados ocultos del transformer (la mayor√≠a de backbones transformer en SimpleTuner) y a√±ade memoria por paso para el buffer de estados ocultos; desact√≠valo si la VRAM es limitada.

### `--checkpoint_epoch_interval`

- **Qu√©**: Ejecuta checkpointing cada N √©pocas completadas.
- **Por qu√©**: Complementa los checkpoints por paso asegurando que siempre captures el estado en los l√≠mites de √©poca, incluso cuando los conteos de pasos var√≠an con el muestreo multi-dataset.

### `--resume_from_checkpoint`

- **Qu√©**: Especifica si y desde d√≥nde reanudar el entrenamiento. Acepta `latest`, un nombre/ruta local de checkpoint o un URI S3/R2.
- **Por qu√©**: Permite continuar entrenando desde un estado guardado, ya sea especificado manualmente o el m√°s reciente disponible.
- **Reanudaci√≥n remota**: Proporciona un URI completo (`s3://bucket/jobs/.../checkpoint-100`) o una clave relativa al bucket (`jobs/.../checkpoint-100`). `latest` solo funciona con `output_dir` local.
- **Requisitos**: La reanudaci√≥n remota necesita una entrada S3 en publishing_config (bucket + credenciales) que pueda leer el checkpoint.
- **Notas**: Los checkpoints remotos deben incluir `checkpoint_manifest.json` (generado por ejecuciones recientes de SimpleTuner). Un checkpoint se compone de un subdirectorio `unet` y opcionalmente `unet_ema`. El `unet` puede colocarse en cualquier layout de Diffusers para SDXL, permitiendo usarlo como lo har√≠as con un modelo normal.

> ‚ÑπÔ∏è Los modelos transformer como PixArt, SD3 o Hunyuan usan los nombres de subcarpeta `transformer` y `transformer_ema`.

### `--disk_low_threshold`

- **Qu√©**: Espacio m√≠nimo libre en disco requerido antes de guardar checkpoints.
- **Por qu√©**: Previene que el entrenamiento falle por errores de disco lleno al detectar espacio bajo tempranamente y tomar una acci√≥n configurada.
- **Formato**: Cadena de tama√±o como `100G`, `50M`, `1T`, `500K`, o bytes simples.
- **Por defecto**: Ninguno (funci√≥n desactivada)

### `--disk_low_action`

- **Qu√©**: Acci√≥n a tomar cuando el espacio en disco est√° por debajo del umbral.
- **Opciones**: `stop`, `wait`, `script`
- **Por defecto**: `stop`
- **Comportamiento**:
  - `stop`: Termina el entrenamiento inmediatamente con un mensaje de error.
  - `wait`: Hace bucle cada 30 segundos hasta que el espacio est√© disponible. Puede esperar indefinidamente.
  - `script`: Ejecuta el script especificado por `--disk_low_script` para liberar espacio.

### `--disk_low_script`

- **Qu√©**: Ruta a un script de limpieza para ejecutar cuando el espacio en disco es bajo.
- **Por qu√©**: Permite limpieza automatizada (ej: eliminar checkpoints antiguos, limpiar cach√©) cuando el espacio en disco es bajo.
- **Notas**: Solo se usa cuando `--disk_low_action=script`. El script debe ser ejecutable. Si el script falla o no libera suficiente espacio, el entrenamiento se detendr√° con un error.
- **Por defecto**: Ninguno

---

## üìä Registro y monitoreo

### `--logging_dir`

- **Qu√©**: Directorio para logs de TensorBoard.
- **Por qu√©**: Te permite monitorear el progreso de entrenamiento y m√©tricas de rendimiento.

### `--report_to`

- **Qu√©**: Especifica la plataforma para reportar resultados y logs.
- **Por qu√©**: Habilita integraci√≥n con plataformas como TensorBoard, wandb o comet_ml para monitoreo. Usa m√∫ltiples valores separados por comas para reportar a m√∫ltiples trackers;
- **Opciones**: wandb, tensorboard, comet_ml

## Variables de configuraci√≥n del entorno

Las opciones anteriores aplican en su mayor parte a `config.json`, pero algunas entradas deben configurarse en `config.env`.

- `TRAINING_NUM_PROCESSES` debe configurarse al n√∫mero de GPUs del sistema. Para la mayor√≠a de los casos de uso, esto es suficiente para habilitar entrenamiento DistributedDataParallel (DDP). Usa `num_processes` dentro de `config.json` si prefieres no usar `config.env`.
- `TRAINING_DYNAMO_BACKEND` por defecto es `no` pero puede configurarse a cualquier backend de torch.compile soportado (p. ej., `inductor`, `aot_eager`, `cudagraphs`) y combinarse con `--dynamo_mode`, `--dynamo_fullgraph` o `--dynamo_use_regional_compilation` para un ajuste m√°s fino
- `SIMPLETUNER_LOG_LEVEL` por defecto es `INFO` pero puede configurarse a `DEBUG` para a√±adir m√°s informaci√≥n para reportes de problemas en `debug.log`
- `VENV_PATH` puede configurarse a la ubicaci√≥n de tu entorno virtual de python si no est√° en la ubicaci√≥n t√≠pica `.venv`
- `ACCELERATE_EXTRA_ARGS` puede dejarse sin configurar o contener argumentos extra como `--multi_gpu` o flags espec√≠ficos de FSDP

---

Este es un resumen b√°sico pensado para ayudarte a empezar. Para una lista completa de opciones y explicaciones m√°s detalladas, consulta la especificaci√≥n completa:

```
usage: train.py [-h] --model_family
                {kolors,auraflow,omnigen,flux,deepfloyd,cosmos2image,sana,qwen_image,pixart_sigma,sdxl,sd1x,sd2x,wan,hidream,sd3,lumina2,ltxvideo}
                [--model_flavour MODEL_FLAVOUR] [--controlnet [CONTROLNET]]
                [--pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH]
                --output_dir OUTPUT_DIR [--logging_dir LOGGING_DIR]
                --model_type {full,lora} [--seed SEED]
                [--resolution RESOLUTION]
                [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]
                [--prediction_type {epsilon,v_prediction,sample,flow_matching}]
                [--pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH]
                [--vae_dtype {default,fp32,fp16,bf16}]
                [--vae_cache_ondemand [VAE_CACHE_ONDEMAND]]
                [--accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL]
                [--aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}]
                [--base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--text_encoder_4_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}]
                [--gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL]
                [--offload_during_startup [OFFLOAD_DURING_STARTUP]]
                [--quantize_via {cpu,accelerator,pipeline}]
                [--quantization_config QUANTIZATION_CONFIG]
                [--fuse_qkv_projections [FUSE_QKV_PROJECTIONS]]
                [--control [CONTROL]]
                [--controlnet_custom_config CONTROLNET_CUSTOM_CONFIG]
                [--controlnet_model_name_or_path CONTROLNET_MODEL_NAME_OR_PATH]
                [--tread_config TREAD_CONFIG]
                [--pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH]
                [--pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER]
                [--pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH]
                [--pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER]
                [--pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH]
                [--pretrained_gemma_model_name_or_path PRETRAINED_GEMMA_MODEL_NAME_OR_PATH]
                [--revision REVISION] [--variant VARIANT]
                [--base_model_default_dtype {bf16,fp32}]
                [--unet_attention_slice [UNET_ATTENTION_SLICE]]
                [--num_train_epochs NUM_TRAIN_EPOCHS]
                [--max_train_steps MAX_TRAIN_STEPS]
                [--train_batch_size TRAIN_BATCH_SIZE]
                [--learning_rate LEARNING_RATE] --optimizer
                {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,prodigy}
                [--optimizer_config OPTIMIZER_CONFIG]
                [--lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]
                [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                [--lr_warmup_steps LR_WARMUP_STEPS]
                [--checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT]
                [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]
                [--train_text_encoder [TRAIN_TEXT_ENCODER]]
                [--text_encoder_lr TEXT_ENCODER_LR]
                [--lr_num_cycles LR_NUM_CYCLES] [--lr_power LR_POWER]
                [--use_soft_min_snr [USE_SOFT_MIN_SNR]] [--use_ema [USE_EMA]]
                [--ema_device {accelerator,cpu}]
                [--ema_cpu_only [EMA_CPU_ONLY]]
                [--ema_update_interval EMA_UPDATE_INTERVAL]
                [--ema_foreach_disable [EMA_FOREACH_DISABLE]]
                [--ema_decay EMA_DECAY] [--lora_rank LORA_RANK]
                [--lora_alpha LORA_ALPHA] [--lora_type {standard,lycoris}]
                [--lora_dropout LORA_DROPOUT]
                [--lora_init_type {default,gaussian,loftq,olora,pissa}]
                [--peft_lora_mode {standard,singlora}]
                [--peft_lora_target_modules PEFT_LORA_TARGET_MODULES]
                [--singlora_ramp_up_steps SINGLORA_RAMP_UP_STEPS]
                [--init_lora INIT_LORA] [--lycoris_config LYCORIS_CONFIG]
                [--init_lokr_norm INIT_LOKR_NORM]
                [--flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano,controlnet,all+ffs+embedder,all+ffs+embedder+controlnet}]
                [--use_dora [USE_DORA]]
                [--resolution_type {pixel,area,pixel_area}]
                --data_backend_config DATA_BACKEND_CONFIG
                [--caption_strategy {filename,textfile,instance_prompt,parquet}]
                [--conditioning_multidataset_sampling {combined,random}]
                [--instance_prompt INSTANCE_PROMPT]
                [--parquet_caption_column PARQUET_CAPTION_COLUMN]
                [--parquet_filename_column PARQUET_FILENAME_COLUMN]
                [--ignore_missing_files [IGNORE_MISSING_FILES]]
                [--vae_cache_scan_behaviour {recreate,sync}]
                [--vae_enable_slicing [VAE_ENABLE_SLICING]]
                [--vae_enable_tiling [VAE_ENABLE_TILING]]
                [--vae_enable_patch_conv [VAE_ENABLE_PATCH_CONV]]
                [--vae_batch_size VAE_BATCH_SIZE]
                [--caption_dropout_probability CAPTION_DROPOUT_PROBABILITY]
                [--tokenizer_max_length TOKENIZER_MAX_LENGTH]
                [--validation_step_interval VALIDATION_STEP_INTERVAL]
                [--validation_epoch_interval VALIDATION_EPOCH_INTERVAL]
                [--disable_benchmark [DISABLE_BENCHMARK]]
                [--validation_prompt VALIDATION_PROMPT]
                [--num_validation_images NUM_VALIDATION_IMAGES]
                [--num_eval_images NUM_EVAL_IMAGES]
                [--eval_steps_interval EVAL_STEPS_INTERVAL]
                [--eval_epoch_interval EVAL_EPOCH_INTERVAL]
                [--eval_timesteps EVAL_TIMESTEPS]
                [--eval_dataset_pooling [EVAL_DATASET_POOLING]]
                [--evaluation_type {none,clip}]
                [--pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH]
                [--validation_guidance VALIDATION_GUIDANCE]
                [--validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS]
                [--validation_on_startup [VALIDATION_ON_STARTUP]]
                [--validation_using_datasets [VALIDATION_USING_DATASETS]]
                [--validation_torch_compile [VALIDATION_TORCH_COMPILE]]
                [--validation_guidance_real VALIDATION_GUIDANCE_REAL]
                [--validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP]
                [--validation_negative_prompt VALIDATION_NEGATIVE_PROMPT]
                [--validation_randomize [VALIDATION_RANDOMIZE]]
                [--validation_seed VALIDATION_SEED]
                [--validation_disable [VALIDATION_DISABLE]]
                [--validation_prompt_library [VALIDATION_PROMPT_LIBRARY]]
                [--user_prompt_library USER_PROMPT_LIBRARY]
                [--eval_dataset_id EVAL_DATASET_ID]
                [--validation_stitch_input_location {left,right}]
                [--validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE]
                [--validation_disable_unconditional [VALIDATION_DISABLE_UNCONDITIONAL]]
                [--validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS]
                [--validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START]
                [--validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP]
                [--validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE]
                [--validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH]
                [--validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc,dpm++,perflow}]
                [--validation_num_video_frames VALIDATION_NUM_VIDEO_FRAMES]
                [--validation_resolution VALIDATION_RESOLUTION]
                [--validation_seed_source {cpu,gpu}]
                [--i_know_what_i_am_doing [I_KNOW_WHAT_I_AM_DOING]]
                [--flow_sigmoid_scale FLOW_SIGMOID_SCALE]
                [--flux_fast_schedule [FLUX_FAST_SCHEDULE]]
                [--flow_use_uniform_schedule [FLOW_USE_UNIFORM_SCHEDULE]]
                [--flow_use_beta_schedule [FLOW_USE_BETA_SCHEDULE]]
                [--flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA]
                [--flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA]
                [--flow_schedule_shift FLOW_SCHEDULE_SHIFT]
                [--flow_schedule_auto_shift [FLOW_SCHEDULE_AUTO_SHIFT]]
                [--flux_guidance_mode {constant,random-range}]
                [--flux_attention_masked_training [FLUX_ATTENTION_MASKED_TRAINING]]
                [--flux_guidance_value FLUX_GUIDANCE_VALUE]
                [--flux_guidance_min FLUX_GUIDANCE_MIN]
                [--flux_guidance_max FLUX_GUIDANCE_MAX]
                [--t5_padding {zero,unmodified}]
                [--sd3_clip_uncond_behaviour {empty_string,zero}]
                [--sd3_t5_uncond_behaviour {empty_string,zero}]
                [--soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA]
                [--mixed_precision {no,fp16,bf16,fp8}]
                [--attention_mechanism {diffusers,xformers,flash-attn,flash-attn-2,flash-attn-3,flash-attn-3-varlen,flex,cudnn,native-efficient,native-flash,native-math,native-npu,native-xla,sla,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}]
                [--sageattention_usage {training,inference,training+inference}]
                [--disable_tf32 [DISABLE_TF32]]
                [--set_grads_to_none [SET_GRADS_TO_NONE]]
                [--noise_offset NOISE_OFFSET]
                [--noise_offset_probability NOISE_OFFSET_PROBABILITY]
                [--input_perturbation INPUT_PERTURBATION]
                [--input_perturbation_steps INPUT_PERTURBATION_STEPS]
                [--lr_end LR_END] [--lr_scale [LR_SCALE]]
                [--lr_scale_sqrt [LR_SCALE_SQRT]]
                [--ignore_final_epochs [IGNORE_FINAL_EPOCHS]]
                [--freeze_encoder_before FREEZE_ENCODER_BEFORE]
                [--freeze_encoder_after FREEZE_ENCODER_AFTER]
                [--freeze_encoder_strategy {before,between,after}]
                [--layer_freeze_strategy {none,bitfit}]
                [--fully_unload_text_encoder [FULLY_UNLOAD_TEXT_ENCODER]]
                [--save_text_encoder [SAVE_TEXT_ENCODER]]
                [--text_encoder_limit TEXT_ENCODER_LIMIT]
                [--prepend_instance_prompt [PREPEND_INSTANCE_PROMPT]]
                [--only_instance_prompt [ONLY_INSTANCE_PROMPT]]
                [--data_aesthetic_score DATA_AESTHETIC_SCORE]
                [--delete_unwanted_images [DELETE_UNWANTED_IMAGES]]
                [--delete_problematic_images [DELETE_PROBLEMATIC_IMAGES]]
                [--disable_bucket_pruning [DISABLE_BUCKET_PRUNING]]
                [--disable_segmented_timestep_sampling [DISABLE_SEGMENTED_TIMESTEP_SAMPLING]]
                [--preserve_data_backend_cache [PRESERVE_DATA_BACKEND_CACHE]]
                [--override_dataset_config [OVERRIDE_DATASET_CONFIG]]
                [--cache_dir CACHE_DIR] [--cache_dir_text CACHE_DIR_TEXT]
                [--cache_dir_vae CACHE_DIR_VAE]
                [--compress_disk_cache [COMPRESS_DISK_CACHE]]
                [--aspect_bucket_disable_rebuild [ASPECT_BUCKET_DISABLE_REBUILD]]
                [--keep_vae_loaded [KEEP_VAE_LOADED]]
                [--skip_file_discovery SKIP_FILE_DISCOVERY]
                [--data_backend_sampling {uniform,auto-weighting}]
                [--image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE]
                [--write_batch_size WRITE_BATCH_SIZE]
                [--read_batch_size READ_BATCH_SIZE]
                [--enable_multiprocessing [ENABLE_MULTIPROCESSING]]
                [--max_workers MAX_WORKERS]
                [--aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS]
                [--torch_num_threads TORCH_NUM_THREADS]
                [--dataloader_prefetch [DATALOADER_PREFETCH]]
                [--dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN]
                [--aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT]
                [--aspect_bucket_alignment {8,16,24,32,64}]
                [--minimum_image_size MINIMUM_IMAGE_SIZE]
                [--maximum_image_size MAXIMUM_IMAGE_SIZE]
                [--target_downsample_size TARGET_DOWNSAMPLE_SIZE]
                [--max_upscale_threshold MAX_UPSCALE_THRESHOLD]
                [--metadata_update_interval METADATA_UPDATE_INTERVAL]
                [--debug_aspect_buckets [DEBUG_ASPECT_BUCKETS]]
                [--debug_dataset_loader [DEBUG_DATASET_LOADER]]
                [--print_filenames [PRINT_FILENAMES]]
                [--print_sampler_statistics [PRINT_SAMPLER_STATISTICS]]
                [--timestep_bias_strategy {earlier,later,range,none}]
                [--timestep_bias_begin TIMESTEP_BIAS_BEGIN]
                [--timestep_bias_end TIMESTEP_BIAS_END]
                [--timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER]
                [--timestep_bias_portion TIMESTEP_BIAS_PORTION]
                [--training_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--inference_scheduler_timestep_spacing {leading,linspace,trailing}]
                [--loss_type {l2,huber,smooth_l1}]
                [--huber_schedule {snr,exponential,constant}]
                [--huber_c HUBER_C] [--snr_gamma SNR_GAMMA]
                [--masked_loss_probability MASKED_LOSS_PROBABILITY]
                [--hidream_use_load_balancing_loss [HIDREAM_USE_LOAD_BALANCING_LOSS]]
                [--hidream_load_balancing_loss_weight HIDREAM_LOAD_BALANCING_LOSS_WEIGHT]
                [--adam_beta1 ADAM_BETA1] [--adam_beta2 ADAM_BETA2]
                [--optimizer_beta1 OPTIMIZER_BETA1]
                [--optimizer_beta2 OPTIMIZER_BETA2]
                [--optimizer_cpu_offload_method {none}]
                [--gradient_precision {unmodified,fp32}]
                [--adam_weight_decay ADAM_WEIGHT_DECAY]
                [--adam_epsilon ADAM_EPSILON] [--prodigy_steps PRODIGY_STEPS]
                [--max_grad_norm MAX_GRAD_NORM]
                [--grad_clip_method {value,norm}]
                [--optimizer_offload_gradients [OPTIMIZER_OFFLOAD_GRADIENTS]]
                [--fuse_optimizer [FUSE_OPTIMIZER]]
                [--optimizer_release_gradients [OPTIMIZER_RELEASE_GRADIENTS]]
                [--push_to_hub [PUSH_TO_HUB]]
                [--push_to_hub_background [PUSH_TO_HUB_BACKGROUND]]
                [--push_checkpoints_to_hub [PUSH_CHECKPOINTS_TO_HUB]]
                [--publishing_config PUBLISHING_CONFIG]
                [--hub_model_id HUB_MODEL_ID]
                [--model_card_private [MODEL_CARD_PRIVATE]]
                [--model_card_safe_for_work [MODEL_CARD_SAFE_FOR_WORK]]
                [--model_card_note MODEL_CARD_NOTE]
                [--modelspec_comment MODELSPEC_COMMENT]
                [--report_to {tensorboard,wandb,comet_ml,all,none}]
                [--checkpoint_step_interval CHECKPOINT_STEP_INTERVAL]
                [--checkpoint_epoch_interval CHECKPOINT_EPOCH_INTERVAL]
                [--checkpointing_rolling_steps CHECKPOINTING_ROLLING_STEPS]
                [--checkpointing_use_tempdir [CHECKPOINTING_USE_TEMPDIR]]
                [--checkpoints_rolling_total_limit CHECKPOINTS_ROLLING_TOTAL_LIMIT]
                [--tracker_run_name TRACKER_RUN_NAME]
                [--tracker_project_name TRACKER_PROJECT_NAME]
                [--tracker_image_layout {gallery,table}]
                [--enable_watermark [ENABLE_WATERMARK]]
                [--framerate FRAMERATE]
                [--seed_for_each_device [SEED_FOR_EACH_DEVICE]]
                [--snr_weight SNR_WEIGHT]
                [--rescale_betas_zero_snr [RESCALE_BETAS_ZERO_SNR]]
                [--webhook_config WEBHOOK_CONFIG]
                [--webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL]
                [--distillation_method {lcm,dcm,dmd,perflow}]
                [--distillation_config DISTILLATION_CONFIG]
                [--ema_validation {none,ema_only,comparison}]
                [--local_rank LOCAL_RANK] [--ltx_train_mode {t2v,i2v}]
                [--ltx_i2v_prob LTX_I2V_PROB]
                [--ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION]
                [--ltx_protect_first_frame [LTX_PROTECT_FIRST_FRAME]]
                [--offload_param_path OFFLOAD_PARAM_PATH]
                [--offset_noise [OFFSET_NOISE]]
                [--quantize_activations [QUANTIZE_ACTIVATIONS]]
                [--refiner_training [REFINER_TRAINING]]
                [--refiner_training_invert_schedule [REFINER_TRAINING_INVERT_SCHEDULE]]
                [--refiner_training_strength REFINER_TRAINING_STRENGTH]
                [--sdxl_refiner_uses_full_range [SDXL_REFINER_USES_FULL_RANGE]]
                [--sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION]

The following SimpleTuner command-line options are available:

options:
  -h, --help            show this help message and exit
  --model_family {kolors,auraflow,omnigen,flux,deepfloyd,cosmos2image,sana,qwen_image,pixart_sigma,sdxl,sd1x,sd2x,wan,hidream,sd3,lumina2,ltxvideo}
                        The base model architecture family to train
  --model_flavour MODEL_FLAVOUR
                        Specific variant of the selected model family
  --controlnet [CONTROLNET]
                        Train ControlNet (full or LoRA) branches alongside the
                        primary network.
  --pretrained_model_name_or_path PRETRAINED_MODEL_NAME_OR_PATH
                        Optional override of the model checkpoint. Leave blank
                        to use the default path for the selected model
                        flavour.
  --output_dir OUTPUT_DIR
                        Directory where model checkpoints and logs will be
                        saved
  --logging_dir LOGGING_DIR
                        Directory for TensorBoard logs
  --model_type {full,lora}
                        Choose between full model training or LoRA adapter
                        training
  --seed SEED           Seed used for deterministic training behaviour
  --resolution RESOLUTION
                        Resolution for training images
  --resume_from_checkpoint RESUME_FROM_CHECKPOINT
                        Select checkpoint to resume training from
  --prediction_type {epsilon,v_prediction,sample,flow_matching}
                        The parameterization type for the diffusion model
  --pretrained_vae_model_name_or_path PRETRAINED_VAE_MODEL_NAME_OR_PATH
                        Path to pretrained VAE model
  --vae_dtype {default,fp32,fp16,bf16}
                        Precision for VAE encoding/decoding. Lower precision
                        saves memory.
  --vae_cache_ondemand [VAE_CACHE_ONDEMAND]
                        Process VAE latents during training instead of
                        precomputing them
  --vae_cache_disable [VAE_CACHE_DISABLE]
                        Implicitly enables on-demand caching and disables
                        writing embeddings to disk.
  --accelerator_cache_clear_interval ACCELERATOR_CACHE_CLEAR_INTERVAL
                        Clear the cache from VRAM every X steps to prevent
                        memory leaks
  --aspect_bucket_rounding {1,2,3,4,5,6,7,8,9}
                        Number of decimal places to round aspect ratios to for
                        bucket creation
  --base_model_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for loading the base model. Lower precision
                        saves memory.
  --text_encoder_1_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_2_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_3_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --text_encoder_4_precision {no_change,int8-quanto,int4-quanto,int2-quanto,int8-torchao,nf4-bnb,int4-torchao,fp8-quanto,fp8uz-quanto,fp8-torchao}
                        Precision for text encoders. Lower precision saves
                        memory.
  --gradient_checkpointing_interval GRADIENT_CHECKPOINTING_INTERVAL
                        Checkpoint every N transformer blocks
  --offload_during_startup [OFFLOAD_DURING_STARTUP]
                        Offload text encoders to CPU during VAE caching
  --quantize_via {cpu,accelerator,pipeline}
                        Where to perform model quantization
  --quantization_config QUANTIZATION_CONFIG
                        JSON or file path describing Diffusers quantization
                        config for pipeline quantization
  --fuse_qkv_projections [FUSE_QKV_PROJECTIONS]
                        Enables Flash Attention 3 when supported; otherwise
                        falls back to PyTorch SDPA.
  --control [CONTROL]   Enable channel-wise control style training
  --controlnet_custom_config CONTROLNET_CUSTOM_CONFIG
                        Custom configuration for ControlNet models
  --controlnet_model_name_or_path CONTROLNET_MODEL_NAME_OR_PATH
                        Path to ControlNet model weights to preload
  --tread_config TREAD_CONFIG
                        Configuration for TREAD training method
  --pretrained_transformer_model_name_or_path PRETRAINED_TRANSFORMER_MODEL_NAME_OR_PATH
                        Path to pretrained transformer model
  --pretrained_transformer_subfolder PRETRAINED_TRANSFORMER_SUBFOLDER
                        Subfolder containing transformer model weights
  --pretrained_unet_model_name_or_path PRETRAINED_UNET_MODEL_NAME_OR_PATH
                        Path to pretrained UNet model
  --pretrained_unet_subfolder PRETRAINED_UNET_SUBFOLDER
                        Subfolder containing UNet model weights
  --pretrained_t5_model_name_or_path PRETRAINED_T5_MODEL_NAME_OR_PATH
                        Path to pretrained T5 model
  --pretrained_gemma_model_name_or_path PRETRAINED_GEMMA_MODEL_NAME_OR_PATH
                        Path to pretrained Gemma model
  --revision REVISION   Git branch/tag/commit for model version
  --variant VARIANT     Model variant (e.g., fp16, bf16)
  --base_model_default_dtype {bf16,fp32}
                        Default precision for quantized base model weights
  --unet_attention_slice [UNET_ATTENTION_SLICE]
                        Enable attention slicing for SDXL UNet
  --num_train_epochs NUM_TRAIN_EPOCHS
                        Number of times to iterate through the entire dataset
  --max_train_steps MAX_TRAIN_STEPS
                        Maximum number of training steps (0 = use epochs
                        instead)
  --train_batch_size TRAIN_BATCH_SIZE
                        Number of samples processed per forward/backward pass
                        (per device).
  --learning_rate LEARNING_RATE
                        Base learning rate for training
  --optimizer {adamw_bf16,ao-adamw8bit,ao-adamw4bit,ao-adamfp8,ao-adamwfp8,adamw_schedulefree,adamw_schedulefree+aggressive,adamw_schedulefree+no_kahan,optimi-stableadamw,optimi-adamw,optimi-lion,optimi-radam,optimi-ranger,optimi-adan,optimi-adam,optimi-sgd,soap,prodigy}
                        Optimization algorithm for training
  --optimizer_config OPTIMIZER_CONFIG
                        Comma-separated key=value pairs forwarded to the
                        selected optimizer
  --lr_scheduler {linear,sine,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}
                        How learning rate changes during training
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of steps to accumulate gradients
  --lr_warmup_steps LR_WARMUP_STEPS
                        Number of steps to gradually increase LR from 0
  --checkpoints_total_limit CHECKPOINTS_TOTAL_LIMIT
                        Maximum number of checkpoints to keep on disk
  --gradient_checkpointing [GRADIENT_CHECKPOINTING]
                        Trade compute for memory during training
  --train_text_encoder [TRAIN_TEXT_ENCODER]
                        Also train the text encoder (CLIP) model
  --text_encoder_lr TEXT_ENCODER_LR
                        Separate learning rate for text encoder
  --lr_num_cycles LR_NUM_CYCLES
                        Number of cosine annealing cycles
  --lr_power LR_POWER   Power for polynomial decay scheduler
  --use_soft_min_snr [USE_SOFT_MIN_SNR]
                        Use soft clamping instead of hard clamping for Min-SNR
  --use_ema [USE_EMA]   Maintain an exponential moving average copy of the
                        model during training.
  --ema_device {accelerator,cpu}
                        Where to keep the EMA weights in-between updates.
  --ema_cpu_only [EMA_CPU_ONLY]
                        Keep EMA weights exclusively on CPU even when
                        ema_device would normally move them.
  --ema_update_interval EMA_UPDATE_INTERVAL
                        Update EMA weights every N optimizer steps
  --ema_foreach_disable [EMA_FOREACH_DISABLE]
                        Fallback to standard tensor ops instead of
                        torch.foreach updates.
  --ema_decay EMA_DECAY
                        Smoothing factor for EMA updates (closer to 1.0 =
                        slower drift).
  --lora_rank LORA_RANK
                        Dimension of LoRA update matrices
  --lora_alpha LORA_ALPHA
                        Scaling factor for LoRA updates
  --lora_type {standard,lycoris}
                        LoRA implementation type
  --lora_dropout LORA_DROPOUT
                        LoRA dropout randomly ignores neurons during training.
                        This can help prevent overfitting.
  --lora_init_type {default,gaussian,loftq,olora,pissa}
                        The initialization type for the LoRA model
  --peft_lora_mode {standard,singlora}
                        PEFT LoRA training mode
  --peft_lora_target_modules PEFT_LORA_TARGET_MODULES
                        JSON array (or path to a JSON file) listing PEFT
                        LoRA target module names. Overrides preset targets.
  --singlora_ramp_up_steps SINGLORA_RAMP_UP_STEPS
                        Number of ramp-up steps for SingLoRA
  --slider_lora_target [SLIDER_LORA_TARGET]
                        Route LoRA training to slider-friendly targets
                        (self-attn + conv/time embeddings). Only affects
                        standard PEFT LoRA.
  --init_lora INIT_LORA
                        Specify an existing LoRA or LyCORIS safetensors file
                        to initialize the adapter
  --lycoris_config LYCORIS_CONFIG
                        Path to LyCORIS configuration JSON file
  --init_lokr_norm INIT_LOKR_NORM
                        Perturbed normal initialization for LyCORIS LoKr
                        layers
  --flux_lora_target {mmdit,context,context+ffs,all,all+ffs,ai-toolkit,tiny,nano,controlnet,all+ffs+embedder,all+ffs+embedder+controlnet}
                        Which layers to train in Flux models
  --use_dora [USE_DORA]
                        Enable DoRA (Weight-Decomposed LoRA)
  --resolution_type {pixel,area,pixel_area}
                        How to interpret the resolution value
  --data_backend_config DATA_BACKEND_CONFIG
                        Select a saved dataset configuration (managed in
                        Datasets & Environments tabs)
  --caption_strategy {filename,textfile,instance_prompt,parquet}
                        How to load captions for images
  --conditioning_multidataset_sampling {combined,random}
                        How to sample from multiple conditioning datasets
  --instance_prompt INSTANCE_PROMPT
                        Instance prompt for training
  --parquet_caption_column PARQUET_CAPTION_COLUMN
                        Column name containing captions in parquet files
  --parquet_filename_column PARQUET_FILENAME_COLUMN
                        Column name containing image paths in parquet files
  --ignore_missing_files [IGNORE_MISSING_FILES]
                        Continue training even if some files are missing
  --vae_cache_scan_behaviour {recreate,sync}
                        How to scan VAE cache for missing files
  --vae_enable_slicing [VAE_ENABLE_SLICING]
                        Enable VAE attention slicing for memory efficiency
  --vae_enable_tiling [VAE_ENABLE_TILING]
                        Enable VAE tiling for large images
  --vae_enable_patch_conv [VAE_ENABLE_PATCH_CONV]
                        Enable patch-based 3D conv for HunyuanVideo VAE to
                        reduce peak VRAM (slight slowdown)
  --vae_batch_size VAE_BATCH_SIZE
                        Batch size for VAE encoding during caching
  --caption_dropout_probability CAPTION_DROPOUT_PROBABILITY
                        Caption dropout will randomly drop captions and, for
                        SDXL, size conditioning inputs based on this
                        probability
  --tokenizer_max_length TOKENIZER_MAX_LENGTH
                        Override the tokenizer sequence length (advanced).
  --validation_step_interval VALIDATION_STEP_INTERVAL
                        Run validation every N training steps (deprecated alias: --validation_steps)
  --validation_epoch_interval VALIDATION_EPOCH_INTERVAL
                        Run validation every N training epochs
  --disable_benchmark [DISABLE_BENCHMARK]
                        Skip generating baseline comparison images before
                        training starts
  --validation_prompt VALIDATION_PROMPT
                        Prompt to use for validation images
  --num_validation_images NUM_VALIDATION_IMAGES
                        Number of images to generate per validation
  --num_eval_images NUM_EVAL_IMAGES
                        Number of images to generate for evaluation metrics
  --eval_steps_interval EVAL_STEPS_INTERVAL
                        Run evaluation every N training steps
  --eval_epoch_interval EVAL_EPOCH_INTERVAL
                        Run evaluation every N training epochs (decimals run
                        multiple times per epoch)
  --eval_timesteps EVAL_TIMESTEPS
                        Number of timesteps for evaluation
  --eval_dataset_pooling [EVAL_DATASET_POOLING]
                        Combine evaluation metrics from all datasets into a
                        single chart
  --evaluation_type {none,clip}
                        Type of evaluation metrics to compute
  --pretrained_evaluation_model_name_or_path PRETRAINED_EVALUATION_MODEL_NAME_OR_PATH
                        Path to pretrained model for evaluation metrics
  --validation_guidance VALIDATION_GUIDANCE
                        CFG guidance scale for validation images
  --validation_num_inference_steps VALIDATION_NUM_INFERENCE_STEPS
                        Number of diffusion steps for validation renders
  --validation_on_startup [VALIDATION_ON_STARTUP]
                        Run validation on the base model before training
                        starts
  --validation_using_datasets [VALIDATION_USING_DATASETS]
                        Use random images from training datasets for
                        validation
  --validation_torch_compile [VALIDATION_TORCH_COMPILE]
                        Use torch.compile() on validation pipeline for speed
  --validation_guidance_real VALIDATION_GUIDANCE_REAL
                        CFG value for distilled models (e.g., FLUX schnell)
  --validation_no_cfg_until_timestep VALIDATION_NO_CFG_UNTIL_TIMESTEP
                        Skip CFG for initial timesteps (Flux only)
  --validation_negative_prompt VALIDATION_NEGATIVE_PROMPT
                        Negative prompt for validation images
  --validation_randomize [VALIDATION_RANDOMIZE]
                        Use random seeds for each validation
  --validation_seed VALIDATION_SEED
                        Fixed seed for reproducible validation images
  --validation_disable [VALIDATION_DISABLE]
                        Completely disable validation image generation
  --validation_prompt_library [VALIDATION_PROMPT_LIBRARY]
                        Use SimpleTuner's built-in prompt library
  --user_prompt_library USER_PROMPT_LIBRARY
                        Path to custom JSON prompt library
  --eval_dataset_id EVAL_DATASET_ID
                        Specific dataset to use for evaluation metrics
  --validation_stitch_input_location {left,right}
                        Where to place input image in img2img validations
  --validation_guidance_rescale VALIDATION_GUIDANCE_RESCALE
                        CFG rescale value for validation
  --validation_disable_unconditional [VALIDATION_DISABLE_UNCONDITIONAL]
                        Disable unconditional image generation during
                        validation
  --validation_guidance_skip_layers VALIDATION_GUIDANCE_SKIP_LAYERS
                        JSON list of transformer layers to skip during
                        classifier-free guidance
  --validation_guidance_skip_layers_start VALIDATION_GUIDANCE_SKIP_LAYERS_START
                        Starting layer index to skip guidance
  --validation_guidance_skip_layers_stop VALIDATION_GUIDANCE_SKIP_LAYERS_STOP
                        Ending layer index to skip guidance
  --validation_guidance_skip_scale VALIDATION_GUIDANCE_SKIP_SCALE
                        Scale guidance strength when applying layer skipping
  --validation_lycoris_strength VALIDATION_LYCORIS_STRENGTH
                        Strength multiplier for LyCORIS validation
  --validation_noise_scheduler {ddim,ddpm,euler,euler-a,unipc,dpm++,perflow}
                        Noise scheduler for validation
  --validation_num_video_frames VALIDATION_NUM_VIDEO_FRAMES
                        Number of frames for video validation
  --validation_resolution VALIDATION_RESOLUTION
                        Override resolution for validation images (pixels or
                        megapixels)
  --validation_seed_source {cpu,gpu}
                        Source device used to generate validation seeds
  --i_know_what_i_am_doing [I_KNOW_WHAT_I_AM_DOING]
                        Unlock experimental overrides and bypass built-in
                        safety limits.
  --flow_sigmoid_scale FLOW_SIGMOID_SCALE
                        Scale factor for sigmoid timestep sampling for flow-
                        matching models.
  --flux_fast_schedule [FLUX_FAST_SCHEDULE]
                        Use experimental fast schedule for Flux training
  --flow_use_uniform_schedule [FLOW_USE_UNIFORM_SCHEDULE]
                        Use uniform schedule instead of sigmoid for flow-
                        matching
  --flow_use_beta_schedule [FLOW_USE_BETA_SCHEDULE]
                        Use beta schedule instead of sigmoid for flow-matching
  --flow_beta_schedule_alpha FLOW_BETA_SCHEDULE_ALPHA
                        Alpha value for beta schedule (default: 2.0)
  --flow_beta_schedule_beta FLOW_BETA_SCHEDULE_BETA
                        Beta value for beta schedule (default: 2.0)
  --flow_schedule_shift FLOW_SCHEDULE_SHIFT
                        Shift the noise schedule for flow-matching models
  --flow_schedule_auto_shift [FLOW_SCHEDULE_AUTO_SHIFT]
                        Auto-adjust schedule shift based on image resolution
  --flux_guidance_mode {constant,random-range}
                        Guidance mode for Flux training
  --flux_attention_masked_training [FLUX_ATTENTION_MASKED_TRAINING]
                        Enable attention masked training for Flux models
  --flux_guidance_value FLUX_GUIDANCE_VALUE
                        Guidance value for constant mode
  --flux_guidance_min FLUX_GUIDANCE_MIN
                        Minimum guidance value for random-range mode
  --flux_guidance_max FLUX_GUIDANCE_MAX
                        Maximum guidance value for random-range mode
  --t5_padding {zero,unmodified}
                        Padding behavior for T5 text encoder
  --sd3_clip_uncond_behaviour {empty_string,zero}
                        How SD3 handles unconditional prompts
  --sd3_t5_uncond_behaviour {empty_string,zero}
                        How SD3 T5 handles unconditional prompts
  --soft_min_snr_sigma_data SOFT_MIN_SNR_SIGMA_DATA
                        Sigma data for soft min SNR weighting
  --mixed_precision {no,fp16,bf16,fp8}
                        Precision for training computations
  --attention_mechanism {diffusers,xformers,flash-attn,flash-attn-2,flash-attn-3,flash-attn-3-varlen,flex,cudnn,native-efficient,native-flash,native-math,native-npu,native-xla,sla,sageattention,sageattention-int8-fp16-triton,sageattention-int8-fp16-cuda,sageattention-int8-fp8-cuda}
                        Attention computation backend
  --sageattention_usage {training,inference,training+inference}
                        When to use SageAttention
  --disable_tf32 [DISABLE_TF32]
                        Force IEEE FP32 precision (disables TF32) using
                        PyTorch's fp32_precision controls when available
  --set_grads_to_none [SET_GRADS_TO_NONE]
                        Set gradients to None instead of zero
  --noise_offset NOISE_OFFSET
                        Add noise offset to training
  --noise_offset_probability NOISE_OFFSET_PROBABILITY
                        Probability of applying noise offset
  --input_perturbation INPUT_PERTURBATION
                        Add additional noise only to the inputs fed to the
                        model during training
  --input_perturbation_steps INPUT_PERTURBATION_STEPS
                        Only apply input perturbation over the first N steps
                        with linear decay
  --lr_end LR_END       A polynomial learning rate will end up at this value
                        after the specified number of warmup steps
  --lr_scale [LR_SCALE]
                        Scale the learning rate by the number of GPUs,
                        gradient accumulation steps, and batch size
  --lr_scale_sqrt [LR_SCALE_SQRT]
                        If using --lr_scale, use the square root of (number of
                        GPUs * gradient accumulation steps * batch size)
  --ignore_final_epochs [IGNORE_FINAL_EPOCHS]
                        When provided, the max epoch counter will not
                        determine the end of the training run
  --freeze_encoder_before FREEZE_ENCODER_BEFORE
                        When using 'before' strategy, we will freeze layers
                        earlier than this
  --freeze_encoder_after FREEZE_ENCODER_AFTER
                        When using 'after' strategy, we will freeze layers
                        later than this
  --freeze_encoder_strategy {before,between,after}
                        When freezing the text encoder, we can use the
                        'before', 'between', or 'after' strategy
  --layer_freeze_strategy {none,bitfit}
                        When freezing parameters, we can use the 'none' or
                        'bitfit' strategy
  --fully_unload_text_encoder [FULLY_UNLOAD_TEXT_ENCODER]
                        If set, will fully unload the text_encoder from memory
                        when not in use
  --save_text_encoder [SAVE_TEXT_ENCODER]
                        If set, will save the text encoder after training
  --text_encoder_limit TEXT_ENCODER_LIMIT
                        When training the text encoder, we want to limit how
                        long it trains for to avoid catastrophic loss
  --prepend_instance_prompt [PREPEND_INSTANCE_PROMPT]
                        When determining the captions from the filename,
                        prepend the instance prompt as an enforced keyword
  --only_instance_prompt [ONLY_INSTANCE_PROMPT]
                        Use the instance prompt instead of the caption from
                        filename
  --data_aesthetic_score DATA_AESTHETIC_SCORE
                        Since currently we do not calculate aesthetic scores
                        for data, we will statically set it to one value. This
                        is only used by the SDXL Refiner
  --delete_unwanted_images [DELETE_UNWANTED_IMAGES]
                        If set, will delete images that are not of a minimum
                        size to save on disk space for large training runs
  --delete_problematic_images [DELETE_PROBLEMATIC_IMAGES]
                        If set, any images that error out during load will be
                        removed from the underlying storage medium
  --disable_bucket_pruning [DISABLE_BUCKET_PRUNING]
                        When training on very small datasets, you might not
                        care that the batch sizes will outpace your image
                        count. Setting this option will prevent SimpleTuner
                        from deleting your bucket lists that do not meet the
                        minimum image count requirements. Use at your own
                        risk, it may end up throwing off your statistics or
                        epoch tracking
  --disable_segmented_timestep_sampling [DISABLE_SEGMENTED_TIMESTEP_SAMPLING]
                        By default, the timestep schedule is divided into
                        roughly `train_batch_size` number of segments, and
                        then each of those are sampled from separately. This
                        improves the selection distribution, but may not be
                        desired in certain training scenarios, eg. when
                        limiting the timestep selection range
  --preserve_data_backend_cache [PRESERVE_DATA_BACKEND_CACHE]
                        For very large cloud storage buckets that will never
                        change, enabling this option will prevent the trainer
                        from scanning it at startup, by preserving the cache
                        files that we generate. Be careful when using this,
                        as, switching datasets can result in the preserved
                        cache being used, which would be problematic.
                        Currently, cache is not stored in the dataset itself
                        but rather, locally. This may change in a future
                        release
  --override_dataset_config [OVERRIDE_DATASET_CONFIG]
                        When provided, the dataset's config will not be
                        checked against the live backend config
  --cache_dir CACHE_DIR
                        The directory where the downloaded models and datasets
                        will be stored
  --cache_dir_text CACHE_DIR_TEXT
                        This is the path to a local directory that will
                        contain your text embed cache
  --cache_dir_vae CACHE_DIR_VAE
                        This is the path to a local directory that will
                        contain your VAE outputs
  --compress_disk_cache [COMPRESS_DISK_CACHE]
                        If set, will gzip-compress the disk cache for Pytorch
                        files. This will save substantial disk space, but may
                        slow down the training process
  --aspect_bucket_disable_rebuild [ASPECT_BUCKET_DISABLE_REBUILD]
                        When using a randomised aspect bucket list, the VAE
                        and aspect cache are rebuilt on each epoch. With a
                        large and diverse enough dataset, rebuilding the
                        aspect list may take a long time, and this may be
                        undesirable. This option will not override
                        vae_cache_clear_each_epoch. If both options are
                        provided, only the VAE cache will be rebuilt
  --keep_vae_loaded [KEEP_VAE_LOADED]
                        If set, will keep the VAE loaded in memory. This can
                        reduce disk churn, but consumes VRAM during the
                        forward pass
  --skip_file_discovery SKIP_FILE_DISCOVERY
                        Comma-separated values of which stages to skip
                        discovery for. Skipping any stage will speed up
                        resumption, but will increase the risk of errors, as
                        missing images or incorrectly bucketed images may not
                        be caught. Valid options: aspect, vae, text, metadata
  --data_backend_sampling {uniform,auto-weighting}
                        When using multiple data backends, the sampling
                        weighting can be set to 'uniform' or 'auto-weighting'
  --image_processing_batch_size IMAGE_PROCESSING_BATCH_SIZE
                        When resizing and cropping images, we do it in
                        parallel using processes or threads. This defines how
                        many images will be read into the queue before they
                        are processed
  --write_batch_size WRITE_BATCH_SIZE
                        When using certain storage backends, it is better to
                        batch smaller writes rather than continuous
                        dispatching. In SimpleTuner, write batching is
                        currently applied during VAE caching, when many small
                        objects are written. This mostly applies to S3, but
                        some shared server filesystems may benefit as well.
                        Default: 64
  --read_batch_size READ_BATCH_SIZE
                        Used by the VAE cache to prefetch image data. This is
                        the number of images to read ahead
  --enable_multiprocessing [ENABLE_MULTIPROCESSING]
                        If set, will use processes instead of threads during
                        metadata caching operations
  --max_workers MAX_WORKERS
                        How many active threads or processes to run during VAE
                        caching
  --aws_max_pool_connections AWS_MAX_POOL_CONNECTIONS
                        When using AWS backends, the maximum number of
                        connections to keep open to the S3 bucket at a single
                        time
  --torch_num_threads TORCH_NUM_THREADS
                        The number of threads to use for PyTorch operations.
                        This is not the same as the number of workers
  --dataloader_prefetch [DATALOADER_PREFETCH]
                        When provided, the dataloader will read-ahead and
                        attempt to retrieve latents, text embeds, and other
                        metadata ahead of the time when the batch is required,
                        so that it can be immediately available
  --dataloader_prefetch_qlen DATALOADER_PREFETCH_QLEN
                        Set the number of prefetched batches
  --aspect_bucket_worker_count ASPECT_BUCKET_WORKER_COUNT
                        The number of workers to use for aspect bucketing.
                        This is a CPU-bound task, so the number of workers
                        should be set to the number of CPU threads available.
                        If you use an I/O bound backend, an even higher value
                        may make sense. Default: 12
  --aspect_bucket_alignment {8,16,24,32,64}
                        When training diffusion models, the image sizes
                        generally must align to a 64 pixel interval
  --minimum_image_size MINIMUM_IMAGE_SIZE
                        The minimum resolution for both sides of input images
  --maximum_image_size MAXIMUM_IMAGE_SIZE
                        When cropping images that are excessively large, the
                        entire scene context may be lost, eg. the crop might
                        just end up being a portion of the background. To
                        avoid this, a maximum image size may be provided,
                        which will result in very-large images being
                        downsampled before cropping them. This value uses
                        --resolution_type to determine whether it is a pixel
                        edge or megapixel value
  --target_downsample_size TARGET_DOWNSAMPLE_SIZE
                        When using --maximum_image_size, very-large images
                        exceeding that value will be downsampled to this
                        target size before cropping
  --max_upscale_threshold MAX_UPSCALE_THRESHOLD
                        Limit upscaling of small images to prevent quality
                        degradation (opt-in). When set, filters out aspect
                        buckets requiring upscaling beyond this threshold.
                        For example, 0.2 allows up to 20% upscaling. Default
                        (None) allows unlimited upscaling. Must be between 0
                        and 1.
  --metadata_update_interval METADATA_UPDATE_INTERVAL
                        When generating the aspect bucket indicies, we want to
                        save it every X seconds
  --debug_aspect_buckets [DEBUG_ASPECT_BUCKETS]
                        If set, will print excessive debugging for aspect
                        bucket operations
  --debug_dataset_loader [DEBUG_DATASET_LOADER]
                        If set, will print excessive debugging for data loader
                        operations
  --print_filenames [PRINT_FILENAMES]
                        If any image files are stopping the process eg. due to
                        corruption or truncation, this will help identify
                        which is at fault
  --print_sampler_statistics [PRINT_SAMPLER_STATISTICS]
                        If provided, will print statistics about the dataset
                        sampler. This is useful for debugging
  --timestep_bias_strategy {earlier,later,range,none}
                        Strategy for biasing timestep sampling
  --timestep_bias_begin TIMESTEP_BIAS_BEGIN
                        Beginning of timestep bias range
  --timestep_bias_end TIMESTEP_BIAS_END
                        End of timestep bias range
  --timestep_bias_multiplier TIMESTEP_BIAS_MULTIPLIER
                        Multiplier for timestep bias probability
  --timestep_bias_portion TIMESTEP_BIAS_PORTION
                        Portion of training steps to apply timestep bias
  --training_scheduler_timestep_spacing {leading,linspace,trailing}
                        Timestep spacing for training scheduler
  --inference_scheduler_timestep_spacing {leading,linspace,trailing}
                        Timestep spacing for inference scheduler
  --loss_type {l2,huber,smooth_l1}
                        Loss function for training
  --huber_schedule {snr,exponential,constant}
                        Schedule for Huber loss transition threshold
  --huber_c HUBER_C     Transition point between L2 and L1 regions for Huber
                        loss
  --snr_gamma SNR_GAMMA
                        SNR weighting gamma value (0 = disabled)
  --masked_loss_probability MASKED_LOSS_PROBABILITY
                        Probability of applying masked loss weighting per
                        batch
  --hidream_use_load_balancing_loss [HIDREAM_USE_LOAD_BALANCING_LOSS]
                        Apply experimental load balancing loss when training
                        HiDream models.
  --hidream_load_balancing_loss_weight HIDREAM_LOAD_BALANCING_LOSS_WEIGHT
                        Strength multiplier for HiDream load balancing loss.
  --adam_beta1 ADAM_BETA1
                        First moment decay rate for Adam optimizers
  --adam_beta2 ADAM_BETA2
                        Second moment decay rate for Adam optimizers
  --optimizer_beta1 OPTIMIZER_BETA1
                        First moment decay rate for optimizers
  --optimizer_beta2 OPTIMIZER_BETA2
                        Second moment decay rate for optimizers
  --optimizer_cpu_offload_method {none}
                        Method for CPU offloading optimizer states
  --gradient_precision {unmodified,fp32}
                        Precision for gradient computation
  --adam_weight_decay ADAM_WEIGHT_DECAY
                        L2 regularisation strength for Adam-family optimizers.
  --adam_epsilon ADAM_EPSILON
                        Small constant added for numerical stability.
  --prodigy_steps PRODIGY_STEPS
                        Number of steps Prodigy should spend adapting its
                        learning rate.
  --max_grad_norm MAX_GRAD_NORM
                        Gradient clipping threshold to prevent exploding
                        gradients.
  --grad_clip_method {value,norm}
                        Strategy for applying max_grad_norm during clipping.
  --optimizer_offload_gradients [OPTIMIZER_OFFLOAD_GRADIENTS]
                        Move optimizer gradients to CPU to save GPU memory.
  --fuse_optimizer [FUSE_OPTIMIZER]
                        Enable fused kernels when offloading to reduce memory
                        overhead.
  --optimizer_release_gradients [OPTIMIZER_RELEASE_GRADIENTS]
                        Free gradient tensors immediately after optimizer step
                        when using Optimi optimizers.
  --push_to_hub [PUSH_TO_HUB]
                        Automatically upload the trained model to your Hugging
                        Face Hub repository.
  --push_to_hub_background [PUSH_TO_HUB_BACKGROUND]
                        Run Hub uploads in a background worker so training is
                        not blocked while pushing.
  --push_checkpoints_to_hub [PUSH_CHECKPOINTS_TO_HUB]
                        Upload intermediate checkpoints to the same Hugging
                        Face repository during training.
  --publishing_config PUBLISHING_CONFIG
                        Optional JSON/file path describing additional
                        publishing targets (S3/Backblaze B2/Azure Blob/Dropbox).
  --hub_model_id HUB_MODEL_ID
                        If left blank, SimpleTuner derives a name from the
                        project settings when pushing to Hub.
  --model_card_private [MODEL_CARD_PRIVATE]
                        Create the Hugging Face repository as private instead
                        of public.
  --model_card_safe_for_work [MODEL_CARD_SAFE_FOR_WORK]
                        Remove the default NSFW warning from the generated
                        model card on Hugging Face Hub.
  --model_card_note MODEL_CARD_NOTE
                        Optional note that appears at the top of the generated
                        model card.
  --modelspec_comment MODELSPEC_COMMENT
                        Text embedded in safetensors file metadata as
                        modelspec.comment, visible in external model viewers.
  --report_to {tensorboard,wandb,comet_ml,all,none}
                        Where to log training metrics
  --checkpoint_step_interval CHECKPOINT_STEP_INTERVAL
                        Save model checkpoint every N steps (deprecated alias: --checkpointing_steps)
  --checkpoint_epoch_interval CHECKPOINT_EPOCH_INTERVAL
                        Save model checkpoint every N epochs
  --checkpointing_rolling_steps CHECKPOINTING_ROLLING_STEPS
                        Rolling checkpoint window size for continuous
                        checkpointing
  --checkpointing_use_tempdir [CHECKPOINTING_USE_TEMPDIR]
                        Use temporary directory for checkpoint files before
                        final save
  --checkpoints_rolling_total_limit CHECKPOINTS_ROLLING_TOTAL_LIMIT
                        Maximum number of rolling checkpoints to keep
  --tracker_run_name TRACKER_RUN_NAME
                        Name for this training run in tracking platforms
  --tracker_project_name TRACKER_PROJECT_NAME
                        Project name in tracking platforms
  --tracker_image_layout {gallery,table}
                        How validation images are displayed in trackers
  --enable_watermark [ENABLE_WATERMARK]
                        Add invisible watermark to generated images
  --framerate FRAMERATE
                        Framerate for video model training
  --seed_for_each_device [SEED_FOR_EACH_DEVICE]
                        Use a unique deterministic seed per GPU instead of
                        sharing one seed across devices.
  --snr_weight SNR_WEIGHT
                        Weight factor for SNR-based loss scaling
  --rescale_betas_zero_snr [RESCALE_BETAS_ZERO_SNR]
                        Rescale betas for zero terminal SNR
  --webhook_config WEBHOOK_CONFIG
                        Path to webhook configuration file
  --webhook_reporting_interval WEBHOOK_REPORTING_INTERVAL
                        Interval for webhook reports (seconds)
  --distillation_method {lcm,dcm,dmd,perflow}
                        Method for model distillation
  --distillation_config DISTILLATION_CONFIG
                        Path to distillation configuration file
  --ema_validation {none,ema_only,comparison}
                        Control how EMA weights are used during validation
                        runs.
  --local_rank LOCAL_RANK
                        Local rank for distributed training
  --ltx_train_mode {t2v,i2v}
                        Training mode for LTX models
  --ltx_i2v_prob LTX_I2V_PROB
                        Probability of using image-to-video training for LTX
  --ltx_partial_noise_fraction LTX_PARTIAL_NOISE_FRACTION
                        Fraction of noise to add for LTX partial training
  --ltx_protect_first_frame [LTX_PROTECT_FIRST_FRAME]
                        Protect the first frame from noise in LTX training
  --offload_param_path OFFLOAD_PARAM_PATH
                        Path to offloaded parameter files
  --offset_noise [OFFSET_NOISE]
                        Enable offset-noise training
  --quantize_activations [QUANTIZE_ACTIVATIONS]
                        Quantize model activations during training
  --refiner_training [REFINER_TRAINING]
                        Enable refiner model training mode
  --refiner_training_invert_schedule [REFINER_TRAINING_INVERT_SCHEDULE]
                        Invert the noise schedule for refiner training
  --refiner_training_strength REFINER_TRAINING_STRENGTH
                        Strength of refiner training
  --sdxl_refiner_uses_full_range [SDXL_REFINER_USES_FULL_RANGE]
                        Use full timestep range for SDXL refiner
  --sana_complex_human_instruction SANA_COMPLEX_HUMAN_INSTRUCTION
                        Complex human instruction for Sana model training
```
