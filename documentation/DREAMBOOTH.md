# Dreambooth (single-subject training)

## Background

The term Dreambooth refers to a technique developed by Google to inject subjects by finetuning them into a model using a small set of high quality images ([paper](https://dreambooth.github.io))

In the context of fine-tuning, Dreambooth adds new techniques to help prevent model collapse due to eg. overfitting or artifacts.

### Regularisation images

Regularisation images are typically generated by the model you are training, using a token that resembles your class.

They do not **have** to be synthetic images generated by the model, but this possibly has better performance than using real data (eg. photographs of real persons).

Example: If you are training in images of a male subject, your regularisation data would be photographs or synthetic generated samples of random male subjects.

> üü¢ Regularisation images can be configured as a separate dataset, allowing them to mix evenly with your training data.

### Rare token training

A concept of dubious value from the original paper was to do a reverse search through the model's tokenizer vocabulary to find a "rare" string that had very little training associated to it.

Since that time, the idea has evolved and debated, with an opposing camp deciding to train against a celebrity's name that looks similar enough, as this requires less compute.

> üü° Rare token training is supported in SimpleTuner, but there's no tool available to help you find one.

### Prior preservation loss

The model contains something called a "prior" which could, in theory, be preserved during Dreambooth training. In experiments with Stable Diffusion however, it didn't seem to help - the model just overfits on its own knowledge.

> üî¥ Prior preservation loss is not supported in SimpleTuner, all regularisation data is treated as if it were usual training data.

## Setup

Following the [tutorial](/TUTORIAL.md) is required before you can continue into Dreambooth-specific configuration.

For DeepFloyd tuning, it's recommended to visit [this page](/documentation/DEEPFLOYD.md) for specific tips related to that model's setup.

For Stable Diffusion 1.x/2.x/XL, here are recommended configuration values.

Located in `sdxl-env.sh` or `sd2x-env.sh`:
```bash
TRAIN_BATCH_SIZE=1

LEARNING_RATE=4e-6
LEARNING_RATE_END=4e-7
LR_SCHEDULE=cosine
LR_WARMUP_STEPS=100

OPTIMIZER=adamw_bf16

MAX_NUM_STEPS=1000
NUM_EPOCHS=0

VALIDATION_STEPS=100
VALIDATION_PROMPT="a photograph of subjectname"

DATALOADER_CONFIG="multidatabackend-dreambooth.json"
```

Inside our dataloader config `multidatabackend-dreambooth.json`, it will look something like this:

```json
[
    {
        "id": "subjectname-data",
        "type": "local",
        "instance_data_dir": "/training/datasets/subjectname",
        "caption_strategy": "instanceprompt",
        "instance_prompt": "subjectname",
        "cache_dir_vae": "/training/vae_cache/subjectname",
        "repeats": 1,
        "crop": false,
        "resolution": 0.5,
        "resolution_type": "area",
        "minimum_image_size": 0.25
    },
    {
        "id": "regularisation-data",
        "type": "local",
        "instance_data_dir": "/training/datasets/regularisation",
        "caption_strategy": "instanceprompt",
        "instance_prompt": "a picture of a man",
        "cache_dir_vae": "/training/vae_cache/regularisation",
        "repeats": 10,
        "ignore_epochs": true,
        "resolution": 0.5,
        "resolution_type": "area",
        "minimum_image_size": 0.5
    },
    {
        "id": "textembeds",
        "type": "local",
        "dataset_type": "text_embeds",
        "default": true,
        "cache_dir": "/training/text_cache/sdxl_base"
    }
]
```

Some key values have been tweaked to make training a single subject easier:

- We now have two datasets configured. Regularisation data is optional, and training may work better without it. You can remove that dataset from the list if desired.
- Resolution is set to `0.5` which will be approximately 512x512 training, which goes faster for SDXL models, and is the native resolution for 1.5 models.
- Minimum image size is set to `0.25` which will allow us to upsample some smaller images, which might be needed for datasets with a few important but low resolution images.
- `caption_strategy` is now `instanceprompt`, which means we will use `instance_prompt` value for every image in the dataset as its caption.
  - **Note:** Using the instance prompt is the traditional method of Dreambooth training, but short captions may work better. If you find the model fails to generalise, it may be worth attempting to use captions.

For a regularisation dataset:

- Set `ignore_epochs=true`, which will ensure this dataset does not count toward a "finished epoch"
- Set `repeats` high enough that this dataset will never stop being sampled
- `minimum_image_size` has been increased to ensure we don't introduce too many low-quality artifacts
- Similarly, using more descriptive captions may help avoid forgetting. Switching from `instanceprompt` to `textfile` or other strategies will require creating `.txt` files for each image.

## Selecting an instance prompt

As mentioned earlier, the original focus of Dreambooth was the selection of rare tokens to train on.

Alternatively, one might use the real name of their subject, or a 'similar enough' celebrity.

After a number of training experiments, it seems as though a 'similar enough' celebrity is the best choice, especially if prompting the model for the person's real name ends up looking dissimilar.

# Refiner tuning

If you're a fan of the SDXL refiner, you may find that it causes your generations to "ruin" the results of your Dreamboothed model.

SimpleTuner supports training the SDXL refiner using LoRA and full rank.

This requires a couple considerations:
- The images should be purely high-quality
- The text embeds cannot be shared with the base model's
- The VAE embeds **can** be shared with the base model

You'll need to update `cache_dir` in your dataloader configuration, `multidatabackend.json`:

```json
[
    {
        "id": "textembeds",
        "type": "local",
        "dataset_type": "text_embeds",
        "default": true,
        "cache_dir": "/training/text_cache/sdxl_refiner"
    }
]
```

If you wish to target a specific aesthetic score with your data, you can add this to `sdxl-env.sh`:

```bash
export TRAINER_EXTRA_ARGS="${TRAINER_EXTRA_ARGS} --data_aesthetic_score=5.6"
```

Update **5.6** to the score you would like to target. The default is **7.0**.

> ‚ö†Ô∏è Currently, the validations images generate a complete image using only the refiner model. This is undesirable, but future work plans on extending evaluations for SDXL Refiner models to include a refining process from a ground truth image set.