# SCFM vs Glance: A Comprehensive Comparison

This document provides a detailed comparison between SCFM (Shortcutting Pre-trained Flow Matching Diffusion Models) and Glance implementation in SimpleTuner.

## Executive Summary

**SCFM and Glance are fundamentally different approaches to accelerating flow-matching diffusion models:**

- **Glance** is a split-schedule inference technique using two separate LoRAs
- **SCFM** is a distillation method that creates a single LoRA capable of aggressive trajectory skipping

## Key Questions Answered

### 1. Does it require inference time modifications?

#### Glance:
**YES - Significant inference modifications required**
- Requires two separate inference passes with different LoRAs
- First pass uses "Slow" LoRA for early timesteps (e.g., 1000, 979.19, 957.52, 934.92, 911.34)
- Second pass uses "Fast" LoRA for late timesteps (e.g., 886.71, 745.07, 562.95, 320.08, 20.0)
- Must manually switch LoRAs mid-generation
- Must pass latents from first phase to second phase
- Requires custom sigma schedules matching training timesteps
- Total of ~10 steps split into two phases

#### SCFM:
**NO - Standard inference with LoRA**
- Uses a single LoRA at inference time
- Standard diffusion pipeline, just loaded with the distilled LoRA
- Works with 3-8 inference steps
- No mid-generation switching required
- Compatible with standard diffusers inference patterns
- Can adjust LoRA scale and timestep shifting without additional complexity

**Key Difference:** Glance requires orchestrating two separate models/passes, while SCFM is a drop-in LoRA replacement.

---

### 2. Is it using pre-cached ODE endpoints?

#### Glance:
**NO - No caching involved**
- Trains on fresh noise samples each time
- Each LoRA learns to denoise at specific timestep ranges
- No teacher model outputs are cached
- Training is straightforward: sample noise, add it at custom timesteps, train to denoise
- Does not require dataset of images pre-generated by teacher

#### SCFM:
**YES - Uses pre-generated teacher outputs as training data**
- Requires generating a dataset using the teacher model first
- Example from their script:
  ```shell
  python inference/sample_dataset.py \
  --output_dir flux_dataset_1024 \
  --resolution 1024 \
  --num_inference_steps 32 \
  --n_sample_per_bucket 8
  ```
- This pre-generation step creates the training dataset
- Training then distills from these cached samples
- Significantly increases storage requirements (need full dataset of generated images)

**Key Difference:** SCFM requires a pre-generation phase and storage of teacher outputs, while Glance trains directly on raw data.

---

### 3. Is it running the teacher model?

#### Glance:
**NO - No teacher model at all**
- Trains two student LoRAs independently
- Both LoRAs train against the same base model's denoising objective
- No distillation from a separate teacher
- Simply partitions the timestep range:
  - Slow LoRA: learns early steps (high noise → medium noise)
  - Fast LoRA: learns late steps (medium noise → clean image)

#### SCFM:
**YES - Complex dual-teacher system during training**
- Uses teacher model for trajectory guidance during training
- Implements velocity field self-distillation
- From their code analysis:
  - Has "teacher_path" option for external teacher
  - Has "teacher_min_timesteps" and "teacher_max_timesteps" parameters
  - Implements dual EMA (Exponential Moving Average) systems:
    1. `tea_lora_parameters_ema` - Teacher LoRA EMA
    2. `slow_stu_lora_parameters_ema` - Slow student LoRA EMA  
    3. `fast_stu_lora_parameters_ema` - Fast student LoRA EMA
- Training involves:
  ```python
  # First jump: Teacher teaching (if teacher_indices exist)
  # Switches to teacher LoRA parameters
  # Runs t_skip // 2 steps with teacher
  
  # First jump: Self teaching (for self_indices)
  # Uses fast student EMA parameters
  # Runs t_skip // 2 steps
  
  # Second jump: Always uses slow student EMA
  # Completes remaining t_skip // 2 steps
  ```
- The `t_skip` parameter (default=2) defines how many teacher steps to skip
- `teacher_ratio` (default=0.4) determines batch split between teacher/self guidance
- `non_shortcut_ratio` (default=0.0 in their config) controls non-shortcut loss weight

**Key Difference:** SCFM has an active teacher model providing velocity field guidance during training, while Glance has no teacher concept.

---

## What Really Differs? Detailed Analysis

### Training Methodology

#### Glance:
1. **Split Schedule Training**
   - Train LoRA #1 on timesteps [1000, 979.19, 957.52, 934.92, 911.34]
   - Train LoRA #2 on timesteps [886.71, 745.07, 562.95, 320.08, 20.0]
   - Each LoRA learns to denoise in its specific range
   - Simple MSE loss against base model predictions
   - No inter-LoRA communication during training
   - Can train on a single image/caption pair

2. **Use Case:** Personalization/single-concept learning
   - Designed for training on 1 image
   - Creates a "quick sketch" of a specific subject
   - Optimized for reproducing one particular concept

#### SCFM:
1. **Velocity Field Self-Distillation**
   - Generates dataset with 32-step teacher model first
   - Uses batch splitting strategy:
     - Portion of batch for teacher-guided learning
     - Portion for self-guided learning (bootstrapping)
   - Implements trajectory skipping via `t_skip` parameter
   - Three-EMA system (teacher, slow student, fast student)
   - Trains a single LoRA that learns to approximate multiple teacher steps in one
   - Requires larger diverse dataset (e.g., 10+ text-image pairs minimum)

2. **Use Case:** General-purpose acceleration
   - Designed for broad distribution coverage
   - Creates fast sampler for general use
   - Maintains quality across diverse prompts

### Architecture Differences

#### Glance:
- **Two separate LoRAs:**
  - Each typically rank 32
  - No special embedding requirements
  - Standard LoRA architecture
  - Total parameters: 2 × (rank × dimension count)

#### SCFM:
- **Single LoRA with higher capacity:**
  - Typical rank 64 (from their config)
  - Applied to extensive layer list including:
    - Time and text embedders
    - Context and input embedders
    - Attention layers (Q, K, V, output)
    - Feed-forward networks
    - Normalization layers
  - Does NOT require step-size embedding (key innovation)
  - Total parameters: 1 × (64 × dimension count) across many layers

### Loss Function

#### Glance:
```python
# Simplified conceptual loss (pseudocode)
# MSE represents mean squared error loss function
loss = MSE(model_pred, target_velocity_at_custom_timestep)
```
- Standard flow matching loss
- No distillation component
- Trains against base model's predictions

#### SCFM:
```python
# Dual-path loss (simplified pseudocode)
# MSE represents mean squared error loss function
# weighted_combination represents the batch-weighted loss aggregation

# Teacher-guided samples:
loss_teacher = MSE(student_pred, teacher_trajectory_shortcut)

# Self-guided samples (bootstrap):
loss_self = MSE(student_pred, fast_ema_trajectory_shortcut)

# Combined with non_shortcut_ratio weighting
total_loss = weighted_combination(loss_teacher, loss_self, loss_non_shortcut)
```
- Complex distillation loss
- Learns to compress multiple teacher steps
- Self-bootstrapping component

### Computational Efficiency

#### Glance Training:
- **Fast to train:** ~60 steps per LoRA (120 total)
- **Minimal compute:** Can train on modest hardware
- **No dataset prep:** Train directly on raw image
- **Storage:** Negligible (just 1 image)
- **Time:** Minutes to train both LoRAs

#### SCFM Training:
- **Dataset generation:** Must run 32-step teacher on dataset first
- **Example:** 8 samples per bucket × N buckets × 32 steps = significant compute
- **Training:** "less than one A100 day" for 3-step Flux
- **Storage:** Full generated dataset (images + latents potentially cached)
- **Total time:** Dataset generation + training time

### Inference Comparison

#### Glance Inference Pattern:
```python
# Load base model with Slow LoRA
pipe_slow = FluxPipeline.from_pretrained(base_model).to(device)
pipe_slow.load_lora_weights("output/glance-slow")

# First phase: 5 steps
# Note: output_type="latent" returns latent tensors via .images attribute
latents = pipe_slow(
    prompt=prompt,
    num_inference_steps=5,
    sigmas=[1.0, 0.979, 0.957, 0.935, 0.911],
    output_type="latent"
).images

# Unload slow, load Fast LoRA
pipe_fast = FluxPipeline.from_pretrained(base_model).to(device)
pipe_fast.load_lora_weights("output/glance-fast")

# Second phase: 5 steps from latents
image = pipe_fast(
    prompt=prompt,
    num_inference_steps=5,
    sigmas=[0.887, 0.745, 0.563, 0.320, 0.020],
    latents=latents
).images[0]
```
**Total steps:** 10 (5 + 5)
**Complexity:** High - manual LoRA swapping, latent passing

#### SCFM Inference Pattern:
```python
# Load base model with SCFM LoRA
pipe = FluxPipeline.from_pretrained(base_model).to(device)
pipe.load_lora_weights("output/flux-scfm")

# Single pass inference
image = pipe(
    prompt=prompt,
    num_inference_steps=3,  # or 4, 8, etc.
    lora_scale=1.5  # optional adjustment
).images[0]
```
**Total steps:** 3-8
**Complexity:** Low - standard inference

### Quality vs Speed Trade-offs

#### Glance:
- **Quality:** Good for single-concept reproduction
- **Generalization:** Limited - trained on 1 image
- **Speed:** 10 steps total (moderate speedup)
- **Flexibility:** Can adjust split point between slow/fast
- **Best for:** Personalizing a specific subject quickly

#### SCFM:
- **Quality:** High across diverse prompts
- **Generalization:** Excellent - trained on diverse dataset
- **Speed:** 3-8 steps (aggressive speedup)
- **Flexibility:** Adjustable LoRA scale, timestep shifting
- **Best for:** General-purpose fast generation

## Fundamental Philosophical Differences

### Glance Philosophy:
"Split the denoising schedule into phases and specialize a LoRA for each phase"
- Simple, interpretable approach
- No distillation complexity
- Think of it as "dividing the work" between two specialists
- Each LoRA is an expert at its timestep range

### SCFM Philosophy:
"Teach a single model to skip multiple steps by learning from a teacher's multi-step trajectory"
- Complex distillation approach
- Velocity field compression
- Think of it as "learning shortcuts" through the ODE trajectory
- One LoRA learns to approximate what normally takes many steps

## When to Use Each

### Use Glance When:
- ✅ Training on a single image or very small dataset
- ✅ Want quick personalization/concept learning
- ✅ Don't mind two-phase inference
- ✅ Have limited compute for training
- ✅ Don't want to generate training dataset
- ✅ Want interpretable, simple approach

### Use SCFM When:
- ✅ Need general-purpose acceleration
- ✅ Have diverse training dataset available
- ✅ Can afford dataset pre-generation
- ✅ Want aggressive step reduction (3-4 steps)
- ✅ Prefer single-LoRA inference simplicity
- ✅ Have significant compute budget (A100-day scale)
- ✅ Need high quality across diverse prompts

## Could SCFM Features Be Added to Glance?

**Short answer: They're fundamentally different approaches**

However, potential hybrid ideas:

1. **Multi-phase SCFM:** Apply SCFM's distillation separately to early/late phases
   - Could reduce each Glance LoRA's step count further
   - Would still require two-phase inference

2. **Glance-style SCFM:** Train SCFM with phase-specific emphasis
   - Use SCFM's distillation on split schedules
   - Might combine benefits of both

3. **Self-distilled Glance:** Add SCFM's self-teaching to Glance
   - Use one Glance LoRA to guide the other during training
   - Could improve inter-phase consistency

## Technical Implementation Status in SimpleTuner

### Glance:
✅ **Fully implemented and documented**
- Uses `--flow_custom_timesteps` parameter
- Works with any flow-matching model (Flux, SD3, etc.)
- Documented in `/documentation/distillation/GLANCE.md`
- Simple config-based training

### SCFM:
❌ **Not currently implemented**
- Would require:
  - Dataset pre-generation pipeline
  - Dual/triple EMA system
  - Velocity field distillation loss
  - Trajectory skipping logic
  - Teacher model orchestration
- Significant implementation effort

## Conclusion

**SCFM and Glance solve different problems:**

- **Glance** = Simple split-schedule training for few-shot/single-shot concept learning with easy setup
- **SCFM** = Complex distillation system for general-purpose acceleration with significant infrastructure requirements

**There is NOT "0 difference" - they are distinct approaches:**
1. Different training methodologies (split schedule vs distillation)
2. Different use cases (personalization vs general acceleration)
3. Different inference patterns (two-phase vs single-phase)
4. Different data requirements (single image vs diverse dataset)
5. Different compute requirements (minutes vs A100-day)

Both are valuable techniques for different scenarios. SCFM achieves more aggressive speedup (3-4 vs 10 steps) but requires substantially more resources and complexity. Glance provides a much simpler path to acceleration for specific use cases.
