{
    "base_model_precision": "int8-quanto",
    "checkpoint_step_interval": 10,
    "controlnet": false,
    "data_backend_config": "config/examples/multidatabackend-small-dreambooth-512px.json",
    "disable_bucket_pruning": true,
    "flow_schedule_shift": 3,
    "fuse_qkv_projections": false,
    "grad_clip_method": "value",
    "gradient_checkpointing": true,
    "hub_model_id": "simpletuner-example-flux2-peft-lora",
    "learning_rate": 1e-4,
    "lora_alpha": 16,
    "lora_rank": 16,
    "lora_type": "standard",
    "lr_scheduler": "constant",
    "max_train_steps": 100,
    "mixed_precision": "bf16",
    "model_family": "flux2",
    "model_type": "lora",
    "num_eval_images": 25,
    "num_train_epochs": 0,
    "optimizer": "adamw_bf16",
    "output_dir": "output/examples/flux2.peft-lora+TREAD",
    "push_checkpoints_to_hub": false,
    "push_to_hub": false,
    "quantize_via": "cpu",
    "report_to": "none",
    "seed": 42,
    "tracker_project_name": "lora-training",
    "tracker_run_name": "example-training-run",
    "train_batch_size": 1,
    "tread_config": {
        "routes": [
            {
                "end_layer_idx": -2,
                "selection_ratio": 0.5,
                "start_layer_idx": 2
            }
        ]
    },
    "use_ema": false,
    "vae_batch_size": 1,
    "validation_disable_unconditional": true,
    "validation_guidance": 4.0,
    "validation_guidance_rescale": 0.0,
    "validation_negative_prompt": "ugly, cropped, blurry, low-quality, mediocre average",
    "validation_num_inference_steps": 16,
    "validation_prompt": "ðŸŸ« is holding a sign that says hello world from flux2",
    "validation_prompt_library": false,
    "validation_resolution": "1024x1024",
    "validation_seed": 42,
    "validation_steps": 50,
}
